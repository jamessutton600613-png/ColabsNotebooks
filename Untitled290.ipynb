{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzgte/9lj08zvirAhaPv7W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamessutton600613-png/GC/blob/main/Untitled290.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dydXyU8a3Ez",
        "outputId": "19a6d3f4-92c5-4f37-8f77-907ed5f3ed43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline uncovered-transition results:\n",
            "t01: UNCOVERED\n",
            "t02: UNCOVERED\n",
            "t03: UNCOVERED\n",
            "t04: UNCOVERED\n",
            "t05: UNCOVERED\n",
            "t06: UNCOVERED\n",
            "t07: UNCOVERED\n",
            "t08: UNCOVERED\n",
            "t09: UNCOVERED\n",
            "t10: COVERED\n",
            "t11: COVERED\n",
            "t12: COVERED\n",
            "t13: COVERED\n",
            "t14: COVERED\n",
            "t15: COVERED\n",
            "\n",
            "Uncovered transitions: [1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "# Incidence matrix B (14 places x 15 transitions)\n",
        "B = np.array([\n",
        "    [-1,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  1, -2,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  0,  1,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  0,  1,  0, -2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  0,  0,  0,  1, -2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  2,  0,  0,  0,  1, -2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  1, -1,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  1, -1, -1,  0,  1, -1,  0,  1],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1, -2,  0,  0,  0],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  1,  0,  0,  0],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1, -2],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  1],\n",
        "], dtype=float)\n",
        "\n",
        "def is_transition_covered_lp(B, t_idx, min_xt=1.0):\n",
        "    \"\"\"\n",
        "    Check if transition t_idx is covered by some t-invariant:\n",
        "    Find x >= 0 such that B x = 0 and x[t_idx] >= min_xt.\n",
        "    \"\"\"\n",
        "    m, n = B.shape\n",
        "    c = np.zeros(n)\n",
        "\n",
        "    A_eq = B\n",
        "    b_eq = np.zeros(m)\n",
        "\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[t_idx] = (min_xt, None)\n",
        "\n",
        "    res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "    return res.success, res.x if res.success else None\n",
        "\n",
        "covered = []\n",
        "solutions = []\n",
        "\n",
        "for j in range(B.shape[1]):\n",
        "    ok, x = is_transition_covered_lp(B, j)\n",
        "    covered.append(ok)\n",
        "    solutions.append(x)\n",
        "\n",
        "print(\"Baseline uncovered-transition results:\")\n",
        "for j, ok in enumerate(covered, start=1):\n",
        "    print(f\"t{j:02d}: {'COVERED' if ok else 'UNCOVERED'}\")\n",
        "\n",
        "uncovered_list = [j+1 for j, ok in enumerate(covered) if not ok]\n",
        "print(\"\\nUncovered transitions:\", uncovered_list)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sympy as sp\n",
        "from math import gcd\n",
        "from functools import reduce\n",
        "\n",
        "# Incidence matrix B (14x15) from Bout - Bin (Fig. 2 in your PDF)\n",
        "B = np.array([\n",
        "    [-1,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  1, -2,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  0,  1,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  0,  1,  0, -2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  0,  0,  0,  1, -2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  2,  0,  0,  0,  1, -2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  1, -1,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  1, -1, -1,  0,  1, -1,  0,  1],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1, -2,  0,  0,  0],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  1,  0,  0,  0],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1, -2],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  1],\n",
        "], dtype=int)\n",
        "\n",
        "def lcm(a, b):\n",
        "    return abs(a*b)//gcd(a, b) if a and b else abs(a*b)\n",
        "\n",
        "def lcm_list(nums):\n",
        "    nums = [abs(int(n)) for n in nums if int(n) != 0]\n",
        "    if not nums:\n",
        "        return 1\n",
        "    return reduce(lcm, nums, 1)\n",
        "\n",
        "def normalize_integer_vector(v):\n",
        "    \"\"\"Scale rational SymPy vector to primitive integer vector with gcd=1.\"\"\"\n",
        "    denoms = [sp.denom(x) for x in v]\n",
        "    L = lcm_list([int(d) for d in denoms])\n",
        "    vint = sp.Matrix([int(x * L) for x in v])\n",
        "\n",
        "    # make primitive (divide by gcd of entries)\n",
        "    g = 0\n",
        "    for x in list(vint):\n",
        "        g = gcd(g, abs(int(x)))\n",
        "    if g > 1:\n",
        "        vint = vint // g\n",
        "\n",
        "    # canonical sign: prefer first nonzero positive\n",
        "    for x in list(vint):\n",
        "        if x != 0:\n",
        "            if x < 0:\n",
        "                vint = -vint\n",
        "            break\n",
        "    return vint\n",
        "\n",
        "# SymPy exact nullspace over rationals\n",
        "Bsp = sp.Matrix(B)\n",
        "ns = Bsp.nullspace()\n",
        "\n",
        "int_basis = [normalize_integer_vector(v) for v in ns]\n",
        "\n",
        "print(\"Nullspace dimension:\", len(int_basis))\n",
        "print(\"\\nInteger basis vectors (as t01..t15 coefficients):\")\n",
        "for k, v in enumerate(int_basis, start=1):\n",
        "    print(f\"\\nBasis #{k}:\")\n",
        "    print([int(x) for x in v])\n",
        "\n",
        "# Identify transitions that appear in at least one nonnegative basis vector\n",
        "# (For this net, these basis vectors already are nonnegative invariants.)\n",
        "covered = [False]*B.shape[1]\n",
        "for v in int_basis:\n",
        "    vec = np.array([int(x) for x in v]).astype(int)\n",
        "    if np.all(vec >= 0):\n",
        "        for j in range(len(vec)):\n",
        "            if vec[j] > 0:\n",
        "                covered[j] = True\n",
        "\n",
        "print(\"\\nBaseline (nullspace) uncovered-transition results:\")\n",
        "for j, ok in enumerate(covered, start=1):\n",
        "    print(f\"t{j:02d}: {'COVERED' if ok else 'UNCOVERED'}\")\n",
        "\n",
        "uncovered_list = [j+1 for j, ok in enumerate(covered) if not ok]\n",
        "print(\"\\nUncovered transitions:\", uncovered_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goLy_uofb48R",
        "outputId": "33909094-960c-420b-8e2a-7fe914abb711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nullspace dimension: 2\n",
            "\n",
            "Integer basis vectors (as t01..t15 coefficients):\n",
            "\n",
            "Basis #1:\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0]\n",
            "\n",
            "Basis #2:\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
            "\n",
            "Baseline (nullspace) uncovered-transition results:\n",
            "t01: UNCOVERED\n",
            "t02: UNCOVERED\n",
            "t03: UNCOVERED\n",
            "t04: UNCOVERED\n",
            "t05: UNCOVERED\n",
            "t06: UNCOVERED\n",
            "t07: UNCOVERED\n",
            "t08: UNCOVERED\n",
            "t09: UNCOVERED\n",
            "t10: COVERED\n",
            "t11: COVERED\n",
            "t12: COVERED\n",
            "t13: COVERED\n",
            "t14: COVERED\n",
            "t15: COVERED\n",
            "\n",
            "Uncovered transitions: [1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sympy as sp\n",
        "\n",
        "# Incidence matrix B (14x15) from Bout - Bin\n",
        "B = np.array([\n",
        "    [-1,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  1, -2,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  0,  1,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  0,  1,  0, -2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  0,  0,  0,  1, -2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  2,  0,  0,  0,  1, -2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  1, -1,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  1, -1, -1,  0,  1, -1,  0,  1],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1, -2,  0,  0,  0],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  1,  0,  0,  0],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1, -2],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  1],\n",
        "], dtype=int)\n",
        "\n",
        "# Reconstruct Bin and Bout (needed for the directed graph) from B alone is ambiguous.\n",
        "# But we DO have the extracted Bin/Bout from the PDF earlier; re-enter them here.\n",
        "Bin = np.array([\n",
        "    [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [0,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [0,0,2,1,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,1,0,0,0,0,0,0,0],\n",
        "    [0,0,0,0,2,0,0,0,0,1,0,0,1,0,0],\n",
        "    [0,0,0,0,0,2,0,0,0,0,0,0,1,0,0],\n",
        "    [0,0,0,0,0,0,2,0,0,0,0,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,1,0,0,0,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,0,1,0,0,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,0,1,1,0,0,1,0,0],\n",
        "    [0,0,0,0,0,0,0,0,0,0,1,2,0,0,0],\n",
        "    [0,0,0,0,0,0,0,0,0,1,0,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,0,0,0,0,0,0,1,2],\n",
        "    [0,0,0,0,0,0,0,0,0,0,0,0,1,0,0],\n",
        "], dtype=int)\n",
        "\n",
        "Bout = np.array([\n",
        "    [0,0,1,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [0,1,0,2,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [0,0,1,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [0,0,1,0,0,0,0,0,0,1,0,0,1,0,0],\n",
        "    [0,0,0,0,1,0,0,0,0,0,0,0,1,0,0],\n",
        "    [0,2,0,0,0,1,0,0,0,0,0,0,0,0,0],\n",
        "    [1,0,0,0,0,0,0,1,0,0,0,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,1,0,0,0,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,1,0,0,0,1,0,0,1],\n",
        "    [0,0,0,0,0,0,0,0,0,1,2,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,0,0,0,0,1,0,0,0],\n",
        "    [0,0,0,0,0,0,0,0,0,0,0,0,1,2,0],\n",
        "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
        "], dtype=int)\n",
        "\n",
        "assert np.array_equal(Bout - Bin, B)\n",
        "\n",
        "# ----- Build directed bipartite graph for SCC cycle test -----\n",
        "# Nodes: p0..p13, t0..t14\n",
        "P = Bin.shape[0]\n",
        "T = Bin.shape[1]\n",
        "N = P + T  # total nodes\n",
        "\n",
        "def p_node(i): return i\n",
        "def t_node(j): return P + j\n",
        "\n",
        "adj = [[] for _ in range(N)]\n",
        "\n",
        "# place -> transition edges if Bin[i,j] > 0\n",
        "for i in range(P):\n",
        "    for j in range(T):\n",
        "        if Bin[i, j] > 0:\n",
        "            adj[p_node(i)].append(t_node(j))\n",
        "\n",
        "# transition -> place edges if Bout[i,j] > 0\n",
        "for i in range(P):\n",
        "    for j in range(T):\n",
        "        if Bout[i, j] > 0:\n",
        "            adj[t_node(j)].append(p_node(i))\n",
        "\n",
        "# Tarjan SCC\n",
        "def tarjan_scc(adj):\n",
        "    index = 0\n",
        "    stack = []\n",
        "    onstack = [False]*len(adj)\n",
        "    idx = [-1]*len(adj)\n",
        "    low = [0]*len(adj)\n",
        "    sccs = []\n",
        "\n",
        "    def strongconnect(v):\n",
        "        nonlocal index\n",
        "        idx[v] = index\n",
        "        low[v] = index\n",
        "        index += 1\n",
        "        stack.append(v)\n",
        "        onstack[v] = True\n",
        "\n",
        "        for w in adj[v]:\n",
        "            if idx[w] == -1:\n",
        "                strongconnect(w)\n",
        "                low[v] = min(low[v], low[w])\n",
        "            elif onstack[w]:\n",
        "                low[v] = min(low[v], idx[w])\n",
        "\n",
        "        if low[v] == idx[v]:\n",
        "            comp = []\n",
        "            while True:\n",
        "                w = stack.pop()\n",
        "                onstack[w] = False\n",
        "                comp.append(w)\n",
        "                if w == v:\n",
        "                    break\n",
        "            sccs.append(comp)\n",
        "\n",
        "    for v in range(len(adj)):\n",
        "        if idx[v] == -1:\n",
        "            strongconnect(v)\n",
        "    return sccs\n",
        "\n",
        "sccs = tarjan_scc(adj)\n",
        "\n",
        "# Determine which SCCs contain a directed cycle:\n",
        "# A SCC has a cycle if it has >1 node, or it has a self-loop.\n",
        "scc_id = {}\n",
        "for k, comp in enumerate(sccs):\n",
        "    for v in comp:\n",
        "        scc_id[v] = k\n",
        "\n",
        "has_cycle = [False]*len(sccs)\n",
        "for k, comp in enumerate(sccs):\n",
        "    if len(comp) > 1:\n",
        "        has_cycle[k] = True\n",
        "    else:\n",
        "        v = comp[0]\n",
        "        if v in adj[v]:\n",
        "            has_cycle[k] = True\n",
        "\n",
        "# Gate-0: transitions not in a cycle-containing SCC are structurally uncovered.\n",
        "labels = [\"Uncertain\"]*T\n",
        "\n",
        "for j in range(T):\n",
        "    sid = scc_id[t_node(j)]\n",
        "    if not has_cycle[sid]:\n",
        "        labels[j] = \"StructurallyUncovered\"\n",
        "\n",
        "# Gate-2: for remaining transitions, attempt SCC-local invariant witness\n",
        "Bsp = sp.Matrix(B)\n",
        "\n",
        "def scc_local_witness_for_transition(j):\n",
        "    sid = scc_id[t_node(j)]\n",
        "    comp = sccs[sid]\n",
        "    t_indices = [v - P for v in comp if v >= P]\n",
        "    if len(t_indices) == 0:\n",
        "        return None\n",
        "\n",
        "    subB = sp.Matrix(B[:, t_indices])\n",
        "    ns = subB.nullspace()\n",
        "    for v in ns:\n",
        "        # Try a nonnegative integer scaling\n",
        "        vec = sp.Matrix(v)\n",
        "        # scale to integer\n",
        "        denoms = [sp.denom(x) for x in vec]\n",
        "        L = 1\n",
        "        for d in denoms:\n",
        "            L = int(sp.ilcm(L, int(d)))\n",
        "        vint = sp.Matrix([int(x*L) for x in vec])\n",
        "\n",
        "        # normalize sign\n",
        "        if all(int(x) == 0 for x in vint):\n",
        "            continue\n",
        "        # flip to make the target component positive if possible\n",
        "        pos = t_indices.index(j) if j in t_indices else None\n",
        "        if pos is not None and int(vint[pos]) < 0:\n",
        "            vint = -vint\n",
        "\n",
        "        if all(int(x) >= 0 for x in vint) and pos is not None and int(vint[pos]) > 0:\n",
        "            # build full-length witness\n",
        "            full = np.zeros(T, dtype=int)\n",
        "            for k, tj in enumerate(t_indices):\n",
        "                full[tj] = int(vint[k])\n",
        "            return full\n",
        "    return None\n",
        "\n",
        "witnesses = [None]*T\n",
        "for j in range(T):\n",
        "    if labels[j] == \"StructurallyUncovered\":\n",
        "        continue\n",
        "    w = scc_local_witness_for_transition(j)\n",
        "    if w is not None:\n",
        "        labels[j] = \"CoveredWitness\"\n",
        "        witnesses[j] = w\n",
        "    else:\n",
        "        labels[j] = \"Uncertain\"\n",
        "\n",
        "print(\"GICT (Gate-0 + Gate-2) labels:\")\n",
        "for j, lab in enumerate(labels, start=1):\n",
        "    print(f\"t{j:02d}: {lab}\")\n",
        "\n",
        "covered_gict = [lab != \"StructurallyUncovered\" and lab != \"Uncertain\" for lab in labels]\n",
        "uncovered_gict = [j+1 for j, lab in enumerate(labels) if lab == \"StructurallyUncovered\"]\n",
        "print(\"\\nGICT uncovered transitions (structural):\", uncovered_gict)\n",
        "\n",
        "print(\"\\nExample witnesses (one per covered transition, if found):\")\n",
        "for j in range(T):\n",
        "    if labels[j] == \"CoveredWitness\":\n",
        "        supp = [k+1 for k, val in enumerate(witnesses[j]) if val > 0]\n",
        "        print(f\"t{j+1:02d} witness support:\", supp, \"vector:\", witnesses[j].tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O502UKZRcbq8",
        "outputId": "1c44c9de-333d-44eb-c493-258a707e4974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GICT (Gate-0 + Gate-2) labels:\n",
            "t01: Uncertain\n",
            "t02: Uncertain\n",
            "t03: Uncertain\n",
            "t04: Uncertain\n",
            "t05: Uncertain\n",
            "t06: StructurallyUncovered\n",
            "t07: StructurallyUncovered\n",
            "t08: Uncertain\n",
            "t09: StructurallyUncovered\n",
            "t10: CoveredWitness\n",
            "t11: CoveredWitness\n",
            "t12: CoveredWitness\n",
            "t13: CoveredWitness\n",
            "t14: CoveredWitness\n",
            "t15: CoveredWitness\n",
            "\n",
            "GICT uncovered transitions (structural): [6, 7, 9]\n",
            "\n",
            "Example witnesses (one per covered transition, if found):\n",
            "t10 witness support: [10, 11, 12] vector: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0]\n",
            "t11 witness support: [10, 11, 12] vector: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0]\n",
            "t12 witness support: [10, 11, 12] vector: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0]\n",
            "t13 witness support: [13, 14, 15] vector: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
            "t14 witness support: [13, 14, 15] vector: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
            "t15 witness support: [13, 14, 15] vector: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sympy as sp\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "# -----------------------------\n",
        "# Data: Bin, Bout, and B = Bout - Bin (from Fig. 2 in your PDF)\n",
        "# -----------------------------\n",
        "B = np.array([\n",
        "    [-1,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  1, -2,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  0,  1,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  0,  1,  0, -2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  0,  0,  0,  1, -2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  2,  0,  0,  0,  1, -2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  1, -1,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  1, -1, -1,  0,  1, -1,  0,  1],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1, -2,  0,  0,  0],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  1,  0,  0,  0],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1, -2],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  1],\n",
        "], dtype=int)\n",
        "\n",
        "Bin = np.array([\n",
        "    [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [0,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [0,0,2,1,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,1,0,0,0,0,0,0,0],\n",
        "    [0,0,0,0,2,0,0,0,0,1,0,0,1,0,0],\n",
        "    [0,0,0,0,0,2,0,0,0,0,0,0,1,0,0],\n",
        "    [0,0,0,0,0,0,2,0,0,0,0,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,1,0,0,0,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,0,1,0,0,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,0,1,1,0,0,1,0,0],\n",
        "    [0,0,0,0,0,0,0,0,0,0,1,2,0,0,0],\n",
        "    [0,0,0,0,0,0,0,0,0,1,0,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,0,0,0,0,0,0,1,2],\n",
        "    [0,0,0,0,0,0,0,0,0,0,0,0,1,0,0],\n",
        "], dtype=int)\n",
        "\n",
        "Bout = np.array([\n",
        "    [0,0,1,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [0,1,0,2,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [0,0,1,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [0,0,1,0,0,0,0,0,0,1,0,0,1,0,0],\n",
        "    [0,0,0,0,1,0,0,0,0,0,0,0,1,0,0],\n",
        "    [0,2,0,0,0,1,0,0,0,0,0,0,0,0,0],\n",
        "    [1,0,0,0,0,0,0,1,0,0,0,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,1,0,0,0,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,1,0,0,0,1,0,0,1],\n",
        "    [0,0,0,0,0,0,0,0,0,1,2,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,0,0,0,0,1,0,0,0],\n",
        "    [0,0,0,0,0,0,0,0,0,0,0,0,1,2,0],\n",
        "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
        "], dtype=int)\n",
        "\n",
        "assert np.array_equal(Bout - Bin, B)\n",
        "\n",
        "P, T = Bin.shape\n",
        "N = P + T\n",
        "\n",
        "def p_node(i): return i\n",
        "def t_node(j): return P + j\n",
        "\n",
        "# -----------------------------\n",
        "# Build directed bipartite graph: place -> transition (if Bin>0), transition -> place (if Bout>0)\n",
        "# -----------------------------\n",
        "adj = [[] for _ in range(N)]\n",
        "\n",
        "for i in range(P):\n",
        "    for j in range(T):\n",
        "        if Bin[i, j] > 0:\n",
        "            adj[p_node(i)].append(t_node(j))\n",
        "\n",
        "for i in range(P):\n",
        "    for j in range(T):\n",
        "        if Bout[i, j] > 0:\n",
        "            adj[t_node(j)].append(p_node(i))\n",
        "\n",
        "# -----------------------------\n",
        "# Tarjan SCC\n",
        "# -----------------------------\n",
        "def tarjan_scc(adj):\n",
        "    index = 0\n",
        "    stack = []\n",
        "    onstack = [False]*len(adj)\n",
        "    idx = [-1]*len(adj)\n",
        "    low = [0]*len(adj)\n",
        "    sccs = []\n",
        "\n",
        "    def strongconnect(v):\n",
        "        nonlocal index\n",
        "        idx[v] = index\n",
        "        low[v] = index\n",
        "        index += 1\n",
        "        stack.append(v)\n",
        "        onstack[v] = True\n",
        "\n",
        "        for w in adj[v]:\n",
        "            if idx[w] == -1:\n",
        "                strongconnect(w)\n",
        "                low[v] = min(low[v], low[w])\n",
        "            elif onstack[w]:\n",
        "                low[v] = min(low[v], idx[w])\n",
        "\n",
        "        if low[v] == idx[v]:\n",
        "            comp = []\n",
        "            while True:\n",
        "                w = stack.pop()\n",
        "                onstack[w] = False\n",
        "                comp.append(w)\n",
        "                if w == v:\n",
        "                    break\n",
        "            sccs.append(comp)\n",
        "\n",
        "    for v in range(len(adj)):\n",
        "        if idx[v] == -1:\n",
        "            strongconnect(v)\n",
        "    return sccs\n",
        "\n",
        "sccs = tarjan_scc(adj)\n",
        "\n",
        "scc_id = {}\n",
        "for k, comp in enumerate(sccs):\n",
        "    for v in comp:\n",
        "        scc_id[v] = k\n",
        "\n",
        "# SCC has a directed cycle if size>1 or has self-loop\n",
        "has_cycle = [False]*len(sccs)\n",
        "for k, comp in enumerate(sccs):\n",
        "    if len(comp) > 1:\n",
        "        has_cycle[k] = True\n",
        "    else:\n",
        "        v = comp[0]\n",
        "        if v in adj[v]:\n",
        "            has_cycle[k] = True\n",
        "\n",
        "# -----------------------------\n",
        "# Gate-2 upgraded: SCC-local LP feasibility certificate\n",
        "# -----------------------------\n",
        "def scc_local_lp_coverable(B_full, t_global, t_indices, min_xt=1.0):\n",
        "    pos = t_indices.index(t_global)\n",
        "    B_sub = B_full[:, t_indices].astype(float)\n",
        "    m, n = B_sub.shape\n",
        "\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B_sub\n",
        "    b_eq = np.zeros(m)\n",
        "\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[pos] = (min_xt, None)\n",
        "\n",
        "    res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "    return res\n",
        "\n",
        "labels = [\"Uncertain\"] * T\n",
        "witnesses = [None] * T\n",
        "telemetry = [{\"gate0\": False, \"gate2\": False, \"scc_size_nodes\": 0, \"scc_size_trans\": 0} for _ in range(T)]\n",
        "\n",
        "# Gate-0: no cycle in SCC => structurally uncovered\n",
        "for j in range(T):\n",
        "    sid = scc_id[t_node(j)]\n",
        "    if not has_cycle[sid]:\n",
        "        labels[j] = \"StructurallyUncovered\"\n",
        "        telemetry[j][\"gate0\"] = True\n",
        "\n",
        "# Gate-2: SCC-local LP certificate (covered vs uncovered) for the rest\n",
        "for j in range(T):\n",
        "    if labels[j] == \"StructurallyUncovered\":\n",
        "        continue\n",
        "\n",
        "    sid = scc_id[t_node(j)]\n",
        "    comp = sccs[sid]\n",
        "    t_indices = [v - P for v in comp if v >= P]\n",
        "\n",
        "    telemetry[j][\"scc_size_nodes\"] = len(comp)\n",
        "    telemetry[j][\"scc_size_trans\"] = len(t_indices)\n",
        "\n",
        "    if len(t_indices) == 0:\n",
        "        labels[j] = \"StructurallyUncovered\"\n",
        "        continue\n",
        "\n",
        "    res = scc_local_lp_coverable(B, j, t_indices, min_xt=1.0)\n",
        "    telemetry[j][\"gate2\"] = True\n",
        "\n",
        "    if res.success:\n",
        "        labels[j] = \"CoveredWitness\"\n",
        "        # Store a readable \"support\" from near-integers if possible\n",
        "        x = np.array(res.x)\n",
        "        x[np.abs(x) < 1e-9] = 0.0\n",
        "        full = np.zeros(T, dtype=float)\n",
        "        for k, tj in enumerate(t_indices):\n",
        "            full[tj] = x[k]\n",
        "        witnesses[j] = full\n",
        "    else:\n",
        "        labels[j] = \"StructurallyUncovered\"\n",
        "\n",
        "# -----------------------------\n",
        "# Report\n",
        "# -----------------------------\n",
        "print(\"GICT (Gate-0 + Gate-2 SCC-local LP) labels:\")\n",
        "for j, lab in enumerate(labels, start=1):\n",
        "    print(f\"t{j:02d}: {lab}\")\n",
        "\n",
        "uncovered = [j+1 for j, lab in enumerate(labels) if lab == \"StructurallyUncovered\"]\n",
        "covered = [j+1 for j, lab in enumerate(labels) if lab == \"CoveredWitness\"]\n",
        "print(\"\\nGICT uncovered transitions:\", uncovered)\n",
        "print(\"GICT covered transitions:\", covered)\n",
        "\n",
        "print(\"\\nExample witnesses (supports):\")\n",
        "for j in range(T):\n",
        "    if labels[j] == \"CoveredWitness\":\n",
        "        x = witnesses[j]\n",
        "        supp = [k+1 for k, val in enumerate(x) if val > 1e-9]\n",
        "        print(f\"t{j+1:02d} witness support:\", supp, \"witness:\", [float(v) for v in x])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQY97Foadbuz",
        "outputId": "654139b1-77a8-479b-8aa9-fb3129ed30fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GICT (Gate-0 + Gate-2 SCC-local LP) labels:\n",
            "t01: StructurallyUncovered\n",
            "t02: StructurallyUncovered\n",
            "t03: StructurallyUncovered\n",
            "t04: StructurallyUncovered\n",
            "t05: StructurallyUncovered\n",
            "t06: StructurallyUncovered\n",
            "t07: StructurallyUncovered\n",
            "t08: StructurallyUncovered\n",
            "t09: StructurallyUncovered\n",
            "t10: CoveredWitness\n",
            "t11: CoveredWitness\n",
            "t12: CoveredWitness\n",
            "t13: CoveredWitness\n",
            "t14: CoveredWitness\n",
            "t15: CoveredWitness\n",
            "\n",
            "GICT uncovered transitions: [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "GICT covered transitions: [10, 11, 12, 13, 14, 15]\n",
            "\n",
            "Example witnesses (supports):\n",
            "t10 witness support: [10, 11, 12] witness: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]\n",
            "t11 witness support: [10, 11, 12] witness: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]\n",
            "t12 witness support: [10, 11, 12] witness: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]\n",
            "t13 witness support: [13, 14, 15] witness: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]\n",
            "t14 witness support: [13, 14, 15] witness: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]\n",
            "t15 witness support: [13, 14, 15] witness: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "# -----------------------------\n",
        "# Data: Bin, Bout, and B = Bout - Bin (from Fig. 2 in your PDF)\n",
        "# -----------------------------\n",
        "B = np.array([\n",
        "    [-1,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  1, -2,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  0,  1,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  0,  1,  0, -2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  0,  0,  0,  1, -2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  2,  0,  0,  0,  1, -2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  1, -1,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  1, -1, -1,  0,  1, -1,  0,  1],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1, -2,  0,  0,  0],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  1,  0,  0,  0],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1, -2],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  1],\n",
        "], dtype=int)\n",
        "\n",
        "Bin = np.array([\n",
        "    [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [0,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [0,0,2,1,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,1,0,0,0,0,0,0,0],\n",
        "    [0,0,0,0,2,0,0,0,0,1,0,0,1,0,0],\n",
        "    [0,0,0,0,0,2,0,0,0,0,0,0,1,0,0],\n",
        "    [0,0,0,0,0,0,2,0,0,0,0,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,1,0,0,0,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,0,1,0,0,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,0,1,1,0,0,1,0,0],\n",
        "    [0,0,0,0,0,0,0,0,0,0,1,2,0,0,0],\n",
        "    [0,0,0,0,0,0,0,0,0,1,0,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,0,0,0,0,0,0,1,2],\n",
        "    [0,0,0,0,0,0,0,0,0,0,0,0,1,0,0],\n",
        "], dtype=int)\n",
        "\n",
        "Bout = np.array([\n",
        "    [0,0,1,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [0,1,0,2,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [0,0,1,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [0,0,1,0,0,0,0,0,0,1,0,0,1,0,0],\n",
        "    [0,0,0,0,1,0,0,0,0,0,0,0,1,0,0],\n",
        "    [0,2,0,0,0,1,0,0,0,0,0,0,0,0,0],\n",
        "    [1,0,0,0,0,0,0,1,0,0,0,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,1,0,0,0,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,1,0,0,0,1,0,0,1],\n",
        "    [0,0,0,0,0,0,0,0,0,1,2,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,0,0,0,0,1,0,0,0],\n",
        "    [0,0,0,0,0,0,0,0,0,0,0,0,1,2,0],\n",
        "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
        "], dtype=int)\n",
        "\n",
        "assert np.array_equal(Bout - Bin, B)\n",
        "\n",
        "P, T = Bin.shape\n",
        "N = P + T\n",
        "\n",
        "def p_node(i): return i\n",
        "def t_node(j): return P + j\n",
        "\n",
        "# -----------------------------\n",
        "# Build directed bipartite graph\n",
        "# -----------------------------\n",
        "adj = [[] for _ in range(N)]\n",
        "rev = [[] for _ in range(N)]  # reverse edges for cheap boundary counts\n",
        "\n",
        "for i in range(P):\n",
        "    for j in range(T):\n",
        "        if Bin[i, j] > 0:\n",
        "            a, b = p_node(i), t_node(j)\n",
        "            adj[a].append(b)\n",
        "            rev[b].append(a)\n",
        "\n",
        "for i in range(P):\n",
        "    for j in range(T):\n",
        "        if Bout[i, j] > 0:\n",
        "            a, b = t_node(j), p_node(i)\n",
        "            adj[a].append(b)\n",
        "            rev[b].append(a)\n",
        "\n",
        "# -----------------------------\n",
        "# Tarjan SCC\n",
        "# -----------------------------\n",
        "def tarjan_scc(adj):\n",
        "    index = 0\n",
        "    stack = []\n",
        "    onstack = [False]*len(adj)\n",
        "    idx = [-1]*len(adj)\n",
        "    low = [0]*len(adj)\n",
        "    sccs = []\n",
        "\n",
        "    def strongconnect(v):\n",
        "        nonlocal index\n",
        "        idx[v] = index\n",
        "        low[v] = index\n",
        "        index += 1\n",
        "        stack.append(v)\n",
        "        onstack[v] = True\n",
        "\n",
        "        for w in adj[v]:\n",
        "            if idx[w] == -1:\n",
        "                strongconnect(w)\n",
        "                low[v] = min(low[v], low[w])\n",
        "            elif onstack[w]:\n",
        "                low[v] = min(low[v], idx[w])\n",
        "\n",
        "        if low[v] == idx[v]:\n",
        "            comp = []\n",
        "            while True:\n",
        "                w = stack.pop()\n",
        "                onstack[w] = False\n",
        "                comp.append(w)\n",
        "                if w == v:\n",
        "                    break\n",
        "            sccs.append(comp)\n",
        "\n",
        "    for v in range(len(adj)):\n",
        "        if idx[v] == -1:\n",
        "            strongconnect(v)\n",
        "    return sccs\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "sccs = tarjan_scc(adj)\n",
        "t_scc = time.perf_counter() - t0\n",
        "\n",
        "scc_id = {}\n",
        "for k, comp in enumerate(sccs):\n",
        "    for v in comp:\n",
        "        scc_id[v] = k\n",
        "\n",
        "# SCC has a cycle if size>1 or has self-loop\n",
        "has_cycle = [False]*len(sccs)\n",
        "for k, comp in enumerate(sccs):\n",
        "    if len(comp) > 1:\n",
        "        has_cycle[k] = True\n",
        "    else:\n",
        "        v = comp[0]\n",
        "        if v in adj[v]:\n",
        "            has_cycle[k] = True\n",
        "\n",
        "# -----------------------------\n",
        "# Helper: SCC-local LP feasibility\n",
        "# -----------------------------\n",
        "def scc_local_lp_coverable(B_full, t_global, t_indices, min_xt=1.0):\n",
        "    pos = t_indices.index(t_global)\n",
        "    B_sub = B_full[:, t_indices].astype(float)\n",
        "    m, n = B_sub.shape\n",
        "\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B_sub\n",
        "    b_eq = np.zeros(m)\n",
        "\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[pos] = (min_xt, None)\n",
        "\n",
        "    res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "    return res\n",
        "\n",
        "# -----------------------------\n",
        "# Rich-label helpers\n",
        "# -----------------------------\n",
        "def support_of_witness(x, eps=1e-9):\n",
        "    return [j for j, v in enumerate(x) if v > eps]\n",
        "\n",
        "def scc_boundary_size(comp):\n",
        "    comp_set = set(comp)\n",
        "    out_edges = 0\n",
        "    in_edges = 0\n",
        "    for v in comp:\n",
        "        for w in adj[v]:\n",
        "            if w not in comp_set:\n",
        "                out_edges += 1\n",
        "        for u in rev[v]:\n",
        "            if u not in comp_set:\n",
        "                in_edges += 1\n",
        "    return in_edges, out_edges\n",
        "\n",
        "# -----------------------------\n",
        "# GICT with labels + telemetry + timings\n",
        "# -----------------------------\n",
        "fragile_k = 3                 # <=3-support witnesses flagged as FragileCovered\n",
        "bottleneck_boundary_k = 2      # SCC with <=2 incoming OR <=2 outgoing edges flagged BottleneckLimited\n",
        "\n",
        "labels = [\"Uncertain\"] * T\n",
        "witnesses = [None] * T\n",
        "telemetry = [{\"gate0\": False, \"gate2\": False, \"scc_nodes\": 0, \"scc_trans\": 0,\n",
        "              \"scc_in\": 0, \"scc_out\": 0, \"supp_size\": None} for _ in range(T)]\n",
        "\n",
        "# Gate-0\n",
        "t0 = time.perf_counter()\n",
        "for j in range(T):\n",
        "    sid = scc_id[t_node(j)]\n",
        "    if not has_cycle[sid]:\n",
        "        labels[j] = \"StructurallyUncovered\"\n",
        "        telemetry[j][\"gate0\"] = True\n",
        "t_gate0 = time.perf_counter() - t0\n",
        "\n",
        "# Gate-2 (SCC-local LP)\n",
        "t0 = time.perf_counter()\n",
        "lp_calls = 0\n",
        "for j in range(T):\n",
        "    if labels[j] == \"StructurallyUncovered\":\n",
        "        continue\n",
        "\n",
        "    sid = scc_id[t_node(j)]\n",
        "    comp = sccs[sid]\n",
        "    t_indices = [v - P for v in comp if v >= P]\n",
        "\n",
        "    telemetry[j][\"scc_nodes\"] = len(comp)\n",
        "    telemetry[j][\"scc_trans\"] = len(t_indices)\n",
        "\n",
        "    inc, outc = scc_boundary_size(comp)\n",
        "    telemetry[j][\"scc_in\"] = inc\n",
        "    telemetry[j][\"scc_out\"] = outc\n",
        "\n",
        "    if len(t_indices) == 0:\n",
        "        labels[j] = \"StructurallyUncovered\"\n",
        "        continue\n",
        "\n",
        "    res = scc_local_lp_coverable(B, j, t_indices, min_xt=1.0)\n",
        "    lp_calls += 1\n",
        "    telemetry[j][\"gate2\"] = True\n",
        "\n",
        "    if res.success:\n",
        "        x_sub = np.array(res.x)\n",
        "        x_sub[np.abs(x_sub) < 1e-9] = 0.0\n",
        "\n",
        "        full = np.zeros(T, dtype=float)\n",
        "        for k, tj in enumerate(t_indices):\n",
        "            full[tj] = x_sub[k]\n",
        "        witnesses[j] = full\n",
        "\n",
        "        # Base covered label\n",
        "        lab = \"CoveredWitness\"\n",
        "\n",
        "        # Rich labels\n",
        "        supp = support_of_witness(full)\n",
        "        telemetry[j][\"supp_size\"] = len(supp)\n",
        "\n",
        "        if len(supp) <= fragile_k:\n",
        "            lab = \"FragileCovered\"\n",
        "\n",
        "        # Bottleneck proxy: SCC has very small boundary\n",
        "        if inc <= bottleneck_boundary_k or outc <= bottleneck_boundary_k:\n",
        "            # If both apply, keep BottleneckLimited as higher-priority tag\n",
        "            lab = \"BottleneckLimited\" if lab != \"StructurallyUncovered\" else lab\n",
        "\n",
        "        labels[j] = lab\n",
        "    else:\n",
        "        labels[j] = \"StructurallyUncovered\"\n",
        "t_gate2 = time.perf_counter() - t0\n",
        "\n",
        "# -----------------------------\n",
        "# Report\n",
        "# -----------------------------\n",
        "print(\"GICT labels (with rich tags):\")\n",
        "for j, lab in enumerate(labels, start=1):\n",
        "    print(f\"t{j:02d}: {lab}\")\n",
        "\n",
        "uncovered = [j+1 for j, lab in enumerate(labels) if lab == \"StructurallyUncovered\"]\n",
        "covered = [j+1 for j, lab in enumerate(labels) if lab != \"StructurallyUncovered\"]\n",
        "print(\"\\nUncovered transitions:\", uncovered)\n",
        "print(\"Non-uncovered transitions:\", covered)\n",
        "\n",
        "print(\"\\nTiming + counts:\")\n",
        "print(\"  SCC computation time (s):\", t_scc)\n",
        "print(\"  Gate-0 time (s):\", t_gate0)\n",
        "print(\"  Gate-2 time (s):\", t_gate2)\n",
        "print(\"  LP calls:\", lp_calls)\n",
        "\n",
        "print(\"\\nWitness supports (for non-uncovered transitions):\")\n",
        "for j in range(T):\n",
        "    if labels[j] != \"StructurallyUncovered\":\n",
        "        supp = [k+1 for k, v in enumerate(witnesses[j]) if v > 1e-9]\n",
        "        print(f\"t{j+1:02d} -> support {supp} (|supp|={len(supp)}), SCC in/out = {telemetry[j]['scc_in']}/{telemetry[j]['scc_out']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THlS5v4Yd_7D",
        "outputId": "1958ef8a-eaa2-4c59-d767-df939a1cf462"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GICT labels (with rich tags):\n",
            "t01: StructurallyUncovered\n",
            "t02: StructurallyUncovered\n",
            "t03: StructurallyUncovered\n",
            "t04: StructurallyUncovered\n",
            "t05: StructurallyUncovered\n",
            "t06: StructurallyUncovered\n",
            "t07: StructurallyUncovered\n",
            "t08: StructurallyUncovered\n",
            "t09: StructurallyUncovered\n",
            "t10: BottleneckLimited\n",
            "t11: BottleneckLimited\n",
            "t12: BottleneckLimited\n",
            "t13: BottleneckLimited\n",
            "t14: BottleneckLimited\n",
            "t15: BottleneckLimited\n",
            "\n",
            "Uncovered transitions: [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Non-uncovered transitions: [10, 11, 12, 13, 14, 15]\n",
            "\n",
            "Timing + counts:\n",
            "  SCC computation time (s): 0.00020653800004311051\n",
            "  Gate-0 time (s): 0.00020173699999759265\n",
            "  Gate-2 time (s): 0.028825608000033753\n",
            "  LP calls: 12\n",
            "\n",
            "Witness supports (for non-uncovered transitions):\n",
            "t10 -> support [10, 11, 12] (|supp|=3), SCC in/out = 2/2\n",
            "t11 -> support [10, 11, 12] (|supp|=3), SCC in/out = 2/2\n",
            "t12 -> support [10, 11, 12] (|supp|=3), SCC in/out = 2/2\n",
            "t13 -> support [13, 14, 15] (|supp|=3), SCC in/out = 2/2\n",
            "t14 -> support [13, 14, 15] (|supp|=3), SCC in/out = 2/2\n",
            "t15 -> support [13, 14, 15] (|supp|=3), SCC in/out = 2/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import time\n",
        "import numpy as np\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "# ============================================================\n",
        "# GICT: Gate-0 (SCC cycle prune) + Gate-2 (per-SCC LP),\n",
        "# plus richer labels: FragileCovered / BottleneckLimited\n",
        "#\n",
        "# This version:\n",
        "#   - runs LP PER SCC (typically far fewer LP calls)\n",
        "#   - gives clean label priority: Fragile > Bottleneck > Covered\n",
        "#   - reports timings + witnesses\n",
        "# ============================================================\n",
        "\n",
        "# -----------------------------\n",
        "# Data: Bin, Bout, and B = Bout - Bin (from Fig. 2 in your PDF)\n",
        "# -----------------------------\n",
        "B = np.array([\n",
        "    [-1,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  1, -2,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  0,  1,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  0,  1,  0, -2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  0,  0,  0,  1, -2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  2,  0,  0,  0,  1, -2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  1, -1,  0,  0,  0,  0,  0,  0],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  1, -1, -1,  0,  1, -1,  0,  1],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1, -2,  0,  0,  0],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  1,  0,  0,  0],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1, -2],\n",
        "    [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  1],\n",
        "], dtype=int)\n",
        "\n",
        "Bin = np.array([\n",
        "    [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [0,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [0,0,2,1,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,1,0,0,0,0,0,0,0],\n",
        "    [0,0,0,0,2,0,0,0,0,1,0,0,1,0,0],\n",
        "    [0,0,0,0,0,2,0,0,0,0,0,0,1,0,0],\n",
        "    [0,0,0,0,0,0,2,0,0,0,0,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,1,0,0,0,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,0,1,0,0,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,0,1,1,0,0,1,0,0],\n",
        "    [0,0,0,0,0,0,0,0,0,0,1,2,0,0,0],\n",
        "    [0,0,0,0,0,0,0,0,0,1,0,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,0,0,0,0,0,0,1,2],\n",
        "    [0,0,0,0,0,0,0,0,0,0,0,0,1,0,0],\n",
        "], dtype=int)\n",
        "\n",
        "Bout = np.array([\n",
        "    [0,0,1,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [0,1,0,2,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [0,0,1,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [0,0,1,0,0,0,0,0,0,1,0,0,1,0,0],\n",
        "    [0,0,0,0,1,0,0,0,0,0,0,0,1,0,0],\n",
        "    [0,2,0,0,0,1,0,0,0,0,0,0,0,0,0],\n",
        "    [1,0,0,0,0,0,0,1,0,0,0,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,1,0,0,0,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,1,0,0,0,1,0,0,1],\n",
        "    [0,0,0,0,0,0,0,0,0,1,2,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,0,0,0,0,1,0,0,0],\n",
        "    [0,0,0,0,0,0,0,0,0,0,0,0,1,2,0],\n",
        "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
        "], dtype=int)\n",
        "\n",
        "assert np.array_equal(Bout - Bin, B)\n",
        "\n",
        "P, T = Bin.shape\n",
        "N = P + T\n",
        "\n",
        "def p_node(i): return i\n",
        "def t_node(j): return P + j\n",
        "\n",
        "# -----------------------------\n",
        "# Build directed bipartite graph\n",
        "# -----------------------------\n",
        "adj = [[] for _ in range(N)]\n",
        "rev = [[] for _ in range(N)]\n",
        "\n",
        "for i in range(P):\n",
        "    for j in range(T):\n",
        "        if Bin[i, j] > 0:\n",
        "            a, b = p_node(i), t_node(j)\n",
        "            adj[a].append(b)\n",
        "            rev[b].append(a)\n",
        "\n",
        "for i in range(P):\n",
        "    for j in range(T):\n",
        "        if Bout[i, j] > 0:\n",
        "            a, b = t_node(j), p_node(i)\n",
        "            adj[a].append(b)\n",
        "            rev[b].append(a)\n",
        "\n",
        "# -----------------------------\n",
        "# Tarjan SCC\n",
        "# -----------------------------\n",
        "def tarjan_scc(adj):\n",
        "    index = 0\n",
        "    stack = []\n",
        "    onstack = [False]*len(adj)\n",
        "    idx = [-1]*len(adj)\n",
        "    low = [0]*len(adj)\n",
        "    sccs = []\n",
        "\n",
        "    def strongconnect(v):\n",
        "        nonlocal index\n",
        "        idx[v] = index\n",
        "        low[v] = index\n",
        "        index += 1\n",
        "        stack.append(v)\n",
        "        onstack[v] = True\n",
        "\n",
        "        for w in adj[v]:\n",
        "            if idx[w] == -1:\n",
        "                strongconnect(w)\n",
        "                low[v] = min(low[v], low[w])\n",
        "            elif onstack[w]:\n",
        "                low[v] = min(low[v], idx[w])\n",
        "\n",
        "        if low[v] == idx[v]:\n",
        "            comp = []\n",
        "            while True:\n",
        "                w = stack.pop()\n",
        "                onstack[w] = False\n",
        "                comp.append(w)\n",
        "                if w == v:\n",
        "                    break\n",
        "            sccs.append(comp)\n",
        "\n",
        "    for v in range(len(adj)):\n",
        "        if idx[v] == -1:\n",
        "            strongconnect(v)\n",
        "    return sccs\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "sccs = tarjan_scc(adj)\n",
        "t_scc = time.perf_counter() - t0\n",
        "\n",
        "scc_id = {}\n",
        "for k, comp in enumerate(sccs):\n",
        "    for v in comp:\n",
        "        scc_id[v] = k\n",
        "\n",
        "# SCC has a directed cycle if size>1 or has self-loop\n",
        "has_cycle = [False]*len(sccs)\n",
        "for k, comp in enumerate(sccs):\n",
        "    if len(comp) > 1:\n",
        "        has_cycle[k] = True\n",
        "    else:\n",
        "        v = comp[0]\n",
        "        if v in adj[v]:\n",
        "            has_cycle[k] = True\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers\n",
        "# -----------------------------\n",
        "def scc_boundary_size(comp):\n",
        "    comp_set = set(comp)\n",
        "    out_edges = 0\n",
        "    in_edges = 0\n",
        "    for v in comp:\n",
        "        for w in adj[v]:\n",
        "            if w not in comp_set:\n",
        "                out_edges += 1\n",
        "        for u in rev[v]:\n",
        "            if u not in comp_set:\n",
        "                in_edges += 1\n",
        "    return in_edges, out_edges\n",
        "\n",
        "def support_of_witness(x, eps=1e-9):\n",
        "    return [j for j, v in enumerate(x) if v > eps]\n",
        "\n",
        "def lp_find_any_invariant_for_scc(B_full, t_indices):\n",
        "    \"\"\"\n",
        "    Find any nonzero x >= 0 with B_sub x = 0.\n",
        "    We avoid the all-zero solution by enforcing sum(x) >= 1.\n",
        "    Implement sum(x) >= 1 as -sum(x) <= -1.\n",
        "    \"\"\"\n",
        "    B_sub = B_full[:, t_indices].astype(float)\n",
        "    m, n = B_sub.shape\n",
        "\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B_sub\n",
        "    b_eq = np.zeros(m)\n",
        "\n",
        "    A_ub = -np.ones((1, n))\n",
        "    b_ub = np.array([-1.0])\n",
        "\n",
        "    bounds = [(0, None)] * n\n",
        "\n",
        "    res = linprog(c, A_eq=A_eq, b_eq=b_eq, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method=\"highs\")\n",
        "    return res\n",
        "\n",
        "# -----------------------------\n",
        "# GICT gates + rich labels\n",
        "# -----------------------------\n",
        "fragile_k = 3\n",
        "bottleneck_boundary_k = 1  # tighter than before; avoids over-triggering\n",
        "\n",
        "labels = [\"Uncertain\"] * T\n",
        "witnesses = [None] * T\n",
        "telemetry = [{\"gate0\": False, \"gate2\": False, \"scc_nodes\": 0, \"scc_trans\": 0,\n",
        "              \"scc_in\": 0, \"scc_out\": 0, \"supp_size\": None, \"scc_lp\": False} for _ in range(T)]\n",
        "\n",
        "# Gate-0: transitions not in a cycle-containing SCC are uncovered\n",
        "t0 = time.perf_counter()\n",
        "for j in range(T):\n",
        "    sid = scc_id[t_node(j)]\n",
        "    if not has_cycle[sid]:\n",
        "        labels[j] = \"StructurallyUncovered\"\n",
        "        telemetry[j][\"gate0\"] = True\n",
        "t_gate0 = time.perf_counter() - t0\n",
        "\n",
        "# Gate-2: per-SCC LP to get a witness; then label transitions in that SCC\n",
        "t0 = time.perf_counter()\n",
        "lp_calls = 0\n",
        "\n",
        "for sid, comp in enumerate(sccs):\n",
        "    # transitions in this SCC\n",
        "    t_indices = [v - P for v in comp if v >= P]\n",
        "    if not t_indices:\n",
        "        continue\n",
        "\n",
        "    # Only process SCCs that contain a directed cycle; otherwise all transitions inside would be uncovered anyway.\n",
        "    if not has_cycle[sid]:\n",
        "        for tj in t_indices:\n",
        "            labels[tj] = \"StructurallyUncovered\"\n",
        "        continue\n",
        "\n",
        "    inc, outc = scc_boundary_size(comp)\n",
        "\n",
        "    # Solve one LP per SCC to find any nonzero invariant support in this SCC\n",
        "    res = lp_find_any_invariant_for_scc(B, t_indices)\n",
        "    lp_calls += 1\n",
        "\n",
        "    if not res.success:\n",
        "        # No invariant exists in this SCC => all its transitions are uncovered\n",
        "        for tj in t_indices:\n",
        "            labels[tj] = \"StructurallyUncovered\"\n",
        "            telemetry[tj][\"gate2\"] = True\n",
        "            telemetry[tj][\"scc_lp\"] = True\n",
        "            telemetry[tj][\"scc_nodes\"] = len(comp)\n",
        "            telemetry[tj][\"scc_trans\"] = len(t_indices)\n",
        "            telemetry[tj][\"scc_in\"] = inc\n",
        "            telemetry[tj][\"scc_out\"] = outc\n",
        "        continue\n",
        "\n",
        "    # Build a full-length witness from the SCC solution\n",
        "    x_sub = np.array(res.x)\n",
        "    x_sub[np.abs(x_sub) < 1e-9] = 0.0\n",
        "\n",
        "    full = np.zeros(T, dtype=float)\n",
        "    for k, tj in enumerate(t_indices):\n",
        "        full[tj] = x_sub[k]\n",
        "\n",
        "    supp = support_of_witness(full)\n",
        "    supp_size = len(supp)\n",
        "\n",
        "    # Label each transition in SCC based on whether it appears in the witness support\n",
        "    # If a transition doesn't appear in this witness, we leave it \"Uncertain\" and optionally\n",
        "    # could do a targeted LP; for this benchmark, the SCC witness supports all transitions.\n",
        "    for tj in t_indices:\n",
        "        telemetry[tj][\"gate2\"] = True\n",
        "        telemetry[tj][\"scc_lp\"] = True\n",
        "        telemetry[tj][\"scc_nodes\"] = len(comp)\n",
        "        telemetry[tj][\"scc_trans\"] = len(t_indices)\n",
        "        telemetry[tj][\"scc_in\"] = inc\n",
        "        telemetry[tj][\"scc_out\"] = outc\n",
        "\n",
        "        if full[tj] <= 1e-9:\n",
        "            # keep uncertain; optional second-pass LP could target tj\n",
        "            continue\n",
        "\n",
        "        # Base label\n",
        "        lab = \"CoveredWitness\"\n",
        "\n",
        "        # Priority 1: Fragile\n",
        "        if supp_size <= fragile_k:\n",
        "            lab = \"FragileCovered\"\n",
        "\n",
        "        # Priority 2: Bottleneck (only if very narrow boundary)\n",
        "        if inc <= bottleneck_boundary_k or outc <= bottleneck_boundary_k:\n",
        "            lab = \"BottleneckLimited\"\n",
        "\n",
        "        labels[tj] = lab\n",
        "        witnesses[tj] = full\n",
        "        telemetry[tj][\"supp_size\"] = supp_size\n",
        "\n",
        "t_gate2 = time.perf_counter() - t0\n",
        "\n",
        "# Second pass: any \"Uncertain\" transitions get targeted LP (rare; keeps correctness)\n",
        "def targeted_cover_lp(B_full, t_global, t_indices, min_xt=1.0):\n",
        "    pos = t_indices.index(t_global)\n",
        "    B_sub = B_full[:, t_indices].astype(float)\n",
        "    m, n = B_sub.shape\n",
        "\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B_sub\n",
        "    b_eq = np.zeros(m)\n",
        "\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[pos] = (min_xt, None)\n",
        "\n",
        "    res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "    return res\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "lp_calls_targeted = 0\n",
        "for j in range(T):\n",
        "    if labels[j] != \"Uncertain\":\n",
        "        continue\n",
        "\n",
        "    sid = scc_id[t_node(j)]\n",
        "    comp = sccs[sid]\n",
        "    t_indices = [v - P for v in comp if v >= P]\n",
        "    inc, outc = scc_boundary_size(comp)\n",
        "\n",
        "    # If SCC has no cycle, it's uncovered\n",
        "    if not has_cycle[sid]:\n",
        "        labels[j] = \"StructurallyUncovered\"\n",
        "        continue\n",
        "\n",
        "    res = targeted_cover_lp(B, j, t_indices, min_xt=1.0)\n",
        "    lp_calls_targeted += 1\n",
        "\n",
        "    if not res.success:\n",
        "        labels[j] = \"StructurallyUncovered\"\n",
        "        continue\n",
        "\n",
        "    x_sub = np.array(res.x)\n",
        "    x_sub[np.abs(x_sub) < 1e-9] = 0.0\n",
        "\n",
        "    full = np.zeros(T, dtype=float)\n",
        "    for k, tj in enumerate(t_indices):\n",
        "        full[tj] = x_sub[k]\n",
        "\n",
        "    supp = support_of_witness(full)\n",
        "    supp_size = len(supp)\n",
        "\n",
        "    lab = \"CoveredWitness\"\n",
        "    if supp_size <= fragile_k:\n",
        "        lab = \"FragileCovered\"\n",
        "    if inc <= bottleneck_boundary_k or outc <= bottleneck_boundary_k:\n",
        "        lab = \"BottleneckLimited\"\n",
        "\n",
        "    labels[j] = lab\n",
        "    witnesses[j] = full\n",
        "    telemetry[j][\"supp_size\"] = supp_size\n",
        "\n",
        "t_gate2_targeted = time.perf_counter() - t0\n",
        "\n",
        "# -----------------------------\n",
        "# Report\n",
        "# -----------------------------\n",
        "print(\"GICT labels (per-SCC LP + rich tags):\")\n",
        "for j, lab in enumerate(labels, start=1):\n",
        "    print(f\"t{j:02d}: {lab}\")\n",
        "\n",
        "uncovered = [j+1 for j, lab in enumerate(labels) if lab == \"StructurallyUncovered\"]\n",
        "covered = [j+1 for j, lab in enumerate(labels) if lab != \"StructurallyUncovered\"]\n",
        "print(\"\\nUncovered transitions:\", uncovered)\n",
        "print(\"Non-uncovered transitions:\", covered)\n",
        "\n",
        "print(\"\\nTiming + counts:\")\n",
        "print(\"  SCC computation time (s):\", t_scc)\n",
        "print(\"  Gate-0 time (s):\", t_gate0)\n",
        "print(\"  Gate-2 per-SCC time (s):\", t_gate2)\n",
        "print(\"  Gate-2 targeted time (s):\", t_gate2_targeted)\n",
        "print(\"  LP calls (per-SCC):\", lp_calls)\n",
        "print(\"  LP calls (targeted):\", lp_calls_targeted)\n",
        "print(\"  Total LP calls:\", lp_calls + lp_calls_targeted)\n",
        "\n",
        "print(\"\\nWitness supports (for non-uncovered transitions):\")\n",
        "for j in range(T):\n",
        "    if labels[j] != \"StructurallyUncovered\" and witnesses[j] is not None:\n",
        "        supp = [k+1 for k, v in enumerate(witnesses[j]) if v > 1e-9]\n",
        "        print(\n",
        "            f\"t{j+1:02d} -> support {supp} (|supp|={len(supp)}), \"\n",
        "            f\"SCC in/out = {telemetry[j]['scc_in']}/{telemetry[j]['scc_out']}\"\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlOUjsSAfd8N",
        "outputId": "c5734681-5adf-48fb-d643-1ab7ef638be2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GICT labels (per-SCC LP + rich tags):\n",
            "t01: StructurallyUncovered\n",
            "t02: StructurallyUncovered\n",
            "t03: StructurallyUncovered\n",
            "t04: StructurallyUncovered\n",
            "t05: StructurallyUncovered\n",
            "t06: StructurallyUncovered\n",
            "t07: StructurallyUncovered\n",
            "t08: StructurallyUncovered\n",
            "t09: StructurallyUncovered\n",
            "t10: FragileCovered\n",
            "t11: FragileCovered\n",
            "t12: FragileCovered\n",
            "t13: FragileCovered\n",
            "t14: FragileCovered\n",
            "t15: FragileCovered\n",
            "\n",
            "Uncovered transitions: [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Non-uncovered transitions: [10, 11, 12, 13, 14, 15]\n",
            "\n",
            "Timing + counts:\n",
            "  SCC computation time (s): 0.00017036400004144525\n",
            "  Gate-0 time (s): 0.000208350999855611\n",
            "  Gate-2 per-SCC time (s): 0.012528643999985434\n",
            "  Gate-2 targeted time (s): 0.02202702199997475\n",
            "  LP calls (per-SCC): 3\n",
            "  LP calls (targeted): 4\n",
            "  Total LP calls: 7\n",
            "\n",
            "Witness supports (for non-uncovered transitions):\n",
            "t10 -> support [10, 11, 12] (|supp|=3), SCC in/out = 2/2\n",
            "t11 -> support [10, 11, 12] (|supp|=3), SCC in/out = 2/2\n",
            "t12 -> support [10, 11, 12] (|supp|=3), SCC in/out = 2/2\n",
            "t13 -> support [13, 14, 15] (|supp|=3), SCC in/out = 2/2\n",
            "t14 -> support [13, 14, 15] (|supp|=3), SCC in/out = 2/2\n",
            "t15 -> support [13, 14, 15] (|supp|=3), SCC in/out = 2/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "# ============================================================\n",
        "# 0) DOWNLOAD PNML\n",
        "# ============================================================\n",
        "pnml_url = \"https://raw.githubusercontent.com/bdumeljic/Petrolino/master/voorbeeld.pnml\"\n",
        "pnml_path = \"voorbeeld.pnml\"\n",
        "\n",
        "if not os.path.exists(pnml_path):\n",
        "    print(\"Downloading PNML:\", pnml_url)\n",
        "    urllib.request.urlretrieve(pnml_url, pnml_path)\n",
        "else:\n",
        "    print(\"PNML already present:\", pnml_path)\n",
        "\n",
        "# ============================================================\n",
        "# 1) PNML PARSER -> (places, transitions, Bin, Bout, B)\n",
        "# ============================================================\n",
        "def _strip_ns(tag):\n",
        "    return tag.split(\"}\", 1)[-1] if \"}\" in tag else tag\n",
        "\n",
        "def parse_pnml_to_matrices(pnml_file):\n",
        "    tree = ET.parse(pnml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    places = []\n",
        "    transitions = []\n",
        "    arcs = []\n",
        "\n",
        "    for elem in root.iter():\n",
        "        tag = _strip_ns(elem.tag)\n",
        "        if tag == \"place\":\n",
        "            pid = elem.attrib.get(\"id\")\n",
        "            if pid:\n",
        "                places.append(pid)\n",
        "        elif tag == \"transition\":\n",
        "            tid = elem.attrib.get(\"id\")\n",
        "            if tid:\n",
        "                transitions.append(tid)\n",
        "        elif tag == \"arc\":\n",
        "            src = elem.attrib.get(\"source\")\n",
        "            tgt = elem.attrib.get(\"target\")\n",
        "            aid = elem.attrib.get(\"id\", \"\")\n",
        "            if src and tgt:\n",
        "                w = 1\n",
        "                for child in elem.iter():\n",
        "                    if _strip_ns(child.tag) == \"inscription\":\n",
        "                        for g in child.iter():\n",
        "                            if _strip_ns(g.tag) == \"text\" and g.text is not None:\n",
        "                                txt = g.text.strip()\n",
        "                                if txt != \"\":\n",
        "                                    try:\n",
        "                                        w = int(txt)\n",
        "                                    except:\n",
        "                                        pass\n",
        "                arcs.append((src, tgt, w, aid))\n",
        "\n",
        "    def uniq(seq):\n",
        "        seen = set()\n",
        "        out = []\n",
        "        for x in seq:\n",
        "            if x not in seen:\n",
        "                out.append(x)\n",
        "                seen.add(x)\n",
        "        return out\n",
        "\n",
        "    places = uniq(places)\n",
        "    transitions = uniq(transitions)\n",
        "\n",
        "    p_index = {p:i for i,p in enumerate(places)}\n",
        "    t_index = {t:i for i,t in enumerate(transitions)}\n",
        "\n",
        "    P = len(places)\n",
        "    T = len(transitions)\n",
        "\n",
        "    Bin = np.zeros((P, T), dtype=int)\n",
        "    Bout = np.zeros((P, T), dtype=int)\n",
        "\n",
        "    for src, tgt, w, _aid in arcs:\n",
        "        if src in p_index and tgt in t_index:\n",
        "            Bin[p_index[src], t_index[tgt]] += w\n",
        "        elif src in t_index and tgt in p_index:\n",
        "            Bout[p_index[tgt], t_index[src]] += w\n",
        "\n",
        "    B = Bout - Bin\n",
        "    return places, transitions, Bin, Bout, B\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "places, transitions, Bin, Bout, B = parse_pnml_to_matrices(pnml_path)\n",
        "t_parse = time.perf_counter() - t0\n",
        "\n",
        "P, T = Bin.shape\n",
        "print(\"\\nParsed PNML:\")\n",
        "print(\"  #places:\", P)\n",
        "print(\"  #transitions:\", T)\n",
        "print(\"  parse time (s):\", t_parse)\n",
        "\n",
        "# ============================================================\n",
        "# 2) BASELINE: global LP per transition\n",
        "# ============================================================\n",
        "def is_transition_covered_lp(B, t_idx, min_xt=1.0):\n",
        "    m, n = B.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B.astype(float)\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[t_idx] = (min_xt, None)\n",
        "    res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "    return res.success\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "baseline_flags = [is_transition_covered_lp(B, j, 1.0) for j in range(T)]\n",
        "t_baseline = time.perf_counter() - t0\n",
        "baseline_uncovered = [j for j, ok in enumerate(baseline_flags) if not ok]\n",
        "\n",
        "print(\"\\nBASELINE uncovered transitions (0-based):\", baseline_uncovered)\n",
        "print(\"Baseline time (s):\", t_baseline)\n",
        "\n",
        "# ============================================================\n",
        "# 3) GICT: Gate-0 SCC prune + per-SCC LP + targeted LP\n",
        "# ============================================================\n",
        "N = P + T\n",
        "def p_node(i): return i\n",
        "def t_node(j): return P + j\n",
        "\n",
        "adj = [[] for _ in range(N)]\n",
        "rev = [[] for _ in range(N)]\n",
        "\n",
        "for i in range(P):\n",
        "    for j in range(T):\n",
        "        if Bin[i, j] > 0:\n",
        "            a, b = p_node(i), t_node(j)\n",
        "            adj[a].append(b); rev[b].append(a)\n",
        "\n",
        "for i in range(P):\n",
        "    for j in range(T):\n",
        "        if Bout[i, j] > 0:\n",
        "            a, b = t_node(j), p_node(i)\n",
        "            adj[a].append(b); rev[b].append(a)\n",
        "\n",
        "def tarjan_scc(adj):\n",
        "    index = 0\n",
        "    stack = []\n",
        "    onstack = [False]*len(adj)\n",
        "    idx = [-1]*len(adj)\n",
        "    low = [0]*len(adj)\n",
        "    sccs = []\n",
        "\n",
        "    def strongconnect(v):\n",
        "        nonlocal index\n",
        "        idx[v] = index\n",
        "        low[v] = index\n",
        "        index += 1\n",
        "        stack.append(v)\n",
        "        onstack[v] = True\n",
        "\n",
        "        for w in adj[v]:\n",
        "            if idx[w] == -1:\n",
        "                strongconnect(w)\n",
        "                low[v] = min(low[v], low[w])\n",
        "            elif onstack[w]:\n",
        "                low[v] = min(low[v], idx[w])\n",
        "\n",
        "        if low[v] == idx[v]:\n",
        "            comp = []\n",
        "            while True:\n",
        "                w = stack.pop()\n",
        "                onstack[w] = False\n",
        "                comp.append(w)\n",
        "                if w == v:\n",
        "                    break\n",
        "            sccs.append(comp)\n",
        "\n",
        "    for v in range(len(adj)):\n",
        "        if idx[v] == -1:\n",
        "            strongconnect(v)\n",
        "    return sccs\n",
        "\n",
        "def scc_has_cycle(comp):\n",
        "    if len(comp) > 1:\n",
        "        return True\n",
        "    v = comp[0]\n",
        "    return v in adj[v]\n",
        "\n",
        "def scc_boundary_size(comp):\n",
        "    comp_set = set(comp)\n",
        "    out_edges = 0\n",
        "    in_edges = 0\n",
        "    for v in comp:\n",
        "        for w in adj[v]:\n",
        "            if w not in comp_set:\n",
        "                out_edges += 1\n",
        "        for u in rev[v]:\n",
        "            if u not in comp_set:\n",
        "                in_edges += 1\n",
        "    return in_edges, out_edges\n",
        "\n",
        "def support_of_witness(x, eps=1e-9):\n",
        "    return [j for j, v in enumerate(x) if v > eps]\n",
        "\n",
        "def lp_find_any_invariant_for_scc(B_full, t_indices):\n",
        "    B_sub = B_full[:, t_indices].astype(float)\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B_sub\n",
        "    b_eq = np.zeros(m)\n",
        "    A_ub = -np.ones((1, n))\n",
        "    b_ub = np.array([-1.0])   # sum(x) >= 1\n",
        "    bounds = [(0, None)] * n\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method=\"highs\")\n",
        "\n",
        "def targeted_cover_lp(B_full, t_global, t_indices, min_xt=1.0):\n",
        "    pos = t_indices.index(t_global)\n",
        "    B_sub = B_full[:, t_indices].astype(float)\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B_sub\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[pos] = (min_xt, None)\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "# Label thresholds (same philosophy as Net A)\n",
        "fragile_k = 3\n",
        "bottleneck_boundary_k = 1\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "sccs = tarjan_scc(adj)\n",
        "t_scc = time.perf_counter() - t0\n",
        "\n",
        "scc_id = {}\n",
        "for k, comp in enumerate(sccs):\n",
        "    for v in comp:\n",
        "        scc_id[v] = k\n",
        "\n",
        "has_cycle = [scc_has_cycle(comp) for comp in sccs]\n",
        "\n",
        "labels = [\"Uncertain\"] * T\n",
        "witnesses = [None] * T\n",
        "\n",
        "# Gate-0\n",
        "t0 = time.perf_counter()\n",
        "for j in range(T):\n",
        "    sid = scc_id[t_node(j)]\n",
        "    if not has_cycle[sid]:\n",
        "        labels[j] = \"StructurallyUncovered\"\n",
        "t_gate0 = time.perf_counter() - t0\n",
        "\n",
        "# Gate-2 per-SCC\n",
        "t0 = time.perf_counter()\n",
        "lp_calls_scc = 0\n",
        "\n",
        "for sid, comp in enumerate(sccs):\n",
        "    t_indices = [v - P for v in comp if v >= P]\n",
        "    if not t_indices:\n",
        "        continue\n",
        "\n",
        "    if not has_cycle[sid]:\n",
        "        for tj in t_indices:\n",
        "            labels[tj] = \"StructurallyUncovered\"\n",
        "        continue\n",
        "\n",
        "    inc, outc = scc_boundary_size(comp)\n",
        "\n",
        "    res = lp_find_any_invariant_for_scc(B, t_indices)\n",
        "    lp_calls_scc += 1\n",
        "\n",
        "    if not res.success:\n",
        "        for tj in t_indices:\n",
        "            labels[tj] = \"StructurallyUncovered\"\n",
        "        continue\n",
        "\n",
        "    x_sub = np.array(res.x)\n",
        "    x_sub[np.abs(x_sub) < 1e-9] = 0.0\n",
        "\n",
        "    full = np.zeros(T, dtype=float)\n",
        "    for k, tj in enumerate(t_indices):\n",
        "        full[tj] = x_sub[k]\n",
        "\n",
        "    supp = support_of_witness(full)\n",
        "    supp_size = len(supp)\n",
        "\n",
        "    for tj in t_indices:\n",
        "        if full[tj] <= 1e-9:\n",
        "            continue\n",
        "\n",
        "        lab = \"CoveredWitness\"\n",
        "        if supp_size <= fragile_k:\n",
        "            lab = \"FragileCovered\"\n",
        "        if inc <= bottleneck_boundary_k or outc <= bottleneck_boundary_k:\n",
        "            lab = \"BottleneckLimited\"\n",
        "\n",
        "        labels[tj] = lab\n",
        "        witnesses[tj] = full\n",
        "\n",
        "t_gate2_scc = time.perf_counter() - t0\n",
        "\n",
        "# Gate-2 targeted (resolve Uncertain)\n",
        "t0 = time.perf_counter()\n",
        "lp_calls_targeted = 0\n",
        "\n",
        "for j in range(T):\n",
        "    if labels[j] != \"Uncertain\":\n",
        "        continue\n",
        "\n",
        "    sid = scc_id[t_node(j)]\n",
        "    comp = sccs[sid]\n",
        "    t_indices = [v - P for v in comp if v >= P]\n",
        "    inc, outc = scc_boundary_size(comp)\n",
        "\n",
        "    if not has_cycle[sid]:\n",
        "        labels[j] = \"StructurallyUncovered\"\n",
        "        continue\n",
        "\n",
        "    res = targeted_cover_lp(B, j, t_indices, min_xt=1.0)\n",
        "    lp_calls_targeted += 1\n",
        "\n",
        "    if not res.success:\n",
        "        labels[j] = \"StructurallyUncovered\"\n",
        "        continue\n",
        "\n",
        "    x_sub = np.array(res.x)\n",
        "    x_sub[np.abs(x_sub) < 1e-9] = 0.0\n",
        "\n",
        "    full = np.zeros(T, dtype=float)\n",
        "    for k, tj in enumerate(t_indices):\n",
        "        full[tj] = x_sub[k]\n",
        "\n",
        "    supp = support_of_witness(full)\n",
        "    supp_size = len(supp)\n",
        "\n",
        "    lab = \"CoveredWitness\"\n",
        "    if supp_size <= fragile_k:\n",
        "        lab = \"FragileCovered\"\n",
        "    if inc <= bottleneck_boundary_k or outc <= bottleneck_boundary_k:\n",
        "        lab = \"BottleneckLimited\"\n",
        "\n",
        "    labels[j] = lab\n",
        "    witnesses[j] = full\n",
        "\n",
        "t_gate2_targeted = time.perf_counter() - t0\n",
        "\n",
        "gict_uncovered = [j for j, lab in enumerate(labels) if lab == \"StructurallyUncovered\"]\n",
        "\n",
        "print(\"\\nGICT uncovered transitions (0-based):\", gict_uncovered)\n",
        "\n",
        "counts = defaultdict(int)\n",
        "for lab in labels:\n",
        "    counts[lab] += 1\n",
        "print(\"\\nGICT label histogram:\")\n",
        "for k in sorted(counts):\n",
        "    print(f\"  {k}: {counts[k]}\")\n",
        "\n",
        "print(\"\\nGICT timings + LP counts:\")\n",
        "print(\"  SCC time (s):\", t_scc)\n",
        "print(\"  Gate-0 time (s):\", t_gate0)\n",
        "print(\"  Gate-2 per-SCC time (s):\", t_gate2_scc)\n",
        "print(\"  Gate-2 targeted time (s):\", t_gate2_targeted)\n",
        "print(\"  LP calls (per-SCC):\", lp_calls_scc)\n",
        "print(\"  LP calls (targeted):\", lp_calls_targeted)\n",
        "print(\"  Total LP calls:\", lp_calls_scc + lp_calls_targeted)\n",
        "\n",
        "# Optional: show transition IDs for uncovered list\n",
        "print(\"\\nUncovered transition IDs:\")\n",
        "for j in gict_uncovered:\n",
        "    print(\" \", j, transitions[j])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxB1iqQchHak",
        "outputId": "6306a162-45d1-4811-aee0-e49e8bd7825e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading PNML: https://raw.githubusercontent.com/bdumeljic/Petrolino/master/voorbeeld.pnml\n",
            "\n",
            "Parsed PNML:\n",
            "  #places: 1\n",
            "  #transitions: 1\n",
            "  parse time (s): 0.000596643000108088\n",
            "\n",
            "BASELINE uncovered transitions (0-based): [0]\n",
            "Baseline time (s): 0.006285055000034845\n",
            "\n",
            "GICT uncovered transitions (0-based): [0]\n",
            "\n",
            "GICT label histogram:\n",
            "  StructurallyUncovered: 1\n",
            "\n",
            "GICT timings + LP counts:\n",
            "  SCC time (s): 0.00018948800016005407\n",
            "  Gate-0 time (s): 0.00021593000019493047\n",
            "  Gate-2 per-SCC time (s): 0.0007831400000668509\n",
            "  Gate-2 targeted time (s): 0.0006827699999121251\n",
            "  LP calls (per-SCC): 0\n",
            "  LP calls (targeted): 0\n",
            "  Total LP calls: 0\n",
            "\n",
            "Uncovered transition IDs:\n",
            "  0 t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ox1yKyhohcV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "pnml_path = \"voorbeeld.pnml\"\n",
        "\n",
        "def _strip_ns(tag):\n",
        "    return tag.split(\"}\", 1)[-1] if \"}\" in tag else tag\n",
        "\n",
        "tree = ET.parse(pnml_path)\n",
        "root = tree.getroot()\n",
        "\n",
        "places = []\n",
        "transitions = []\n",
        "arcs = []\n",
        "\n",
        "for elem in root.iter():\n",
        "    tag = _strip_ns(elem.tag)\n",
        "    if tag == \"place\":\n",
        "        places.append(elem.attrib.get(\"id\"))\n",
        "    elif tag == \"transition\":\n",
        "        transitions.append(elem.attrib.get(\"id\"))\n",
        "    elif tag == \"arc\":\n",
        "        src = elem.attrib.get(\"source\")\n",
        "        tgt = elem.attrib.get(\"target\")\n",
        "        w = 1\n",
        "        # attempt to read inscription weight if present\n",
        "        for child in elem.iter():\n",
        "            if _strip_ns(child.tag) == \"inscription\":\n",
        "                for g in child.iter():\n",
        "                    if _strip_ns(g.tag) == \"text\" and g.text is not None:\n",
        "                        txt = g.text.strip()\n",
        "                        if txt:\n",
        "                            try:\n",
        "                                w = int(txt)\n",
        "                            except:\n",
        "                                pass\n",
        "        arcs.append((elem.attrib.get(\"id\"), src, tgt, w))\n",
        "\n",
        "print(\"Places:\", len([p for p in places if p]), places[:20])\n",
        "print(\"Transitions:\", len([t for t in transitions if t]), transitions[:20])\n",
        "print(\"Arcs:\", len(arcs))\n",
        "for a in arcs[:50]:\n",
        "    print(\"arc:\", a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9vcXQvBhcao",
        "outputId": "11700a42-8f16-41ce-beb8-ff05f729fa38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Places: 1 ['p1']\n",
            "Transitions: 1 ['t1']\n",
            "Arcs: 1\n",
            "arc: ('a1', 'p1', 't1', 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pnml.org/version-2009/examples/philo.pnml https://share.google/H6gKYtWh3XT3b7lVo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "q4D8_GefiHtd",
        "outputId": "0f67a5ce-d32f-4389-ad3a-41a59ce75d94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-2290443293.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2290443293.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pnml.org/version-2009/examples/philo.pnml https://share.google/H6gKYtWh3XT3b7lVo\u001b[0m\n\u001b[0m                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "# ============================================================\n",
        "# 0) DOWNLOAD PNML (philo = philosophers)\n",
        "# ============================================================\n",
        "pnml_url = \"https://www.pnml.org/version-2009/examples/philo.pnml\"\n",
        "pnml_path = \"philo.pnml\"\n",
        "\n",
        "if not os.path.exists(pnml_path):\n",
        "    print(\"Downloading PNML:\", pnml_url)\n",
        "    urllib.request.urlretrieve(pnml_url, pnml_path)\n",
        "else:\n",
        "    print(\"PNML already present:\", pnml_path)\n",
        "\n",
        "# ============================================================\n",
        "# 1) PNML PARSER -> (places, transitions, Bin, Bout, B)\n",
        "# ============================================================\n",
        "def _strip_ns(tag):\n",
        "    return tag.split(\"}\", 1)[-1] if \"}\" in tag else tag\n",
        "\n",
        "def parse_pnml_to_matrices(pnml_file):\n",
        "    tree = ET.parse(pnml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    places = []\n",
        "    transitions = []\n",
        "    arcs = []\n",
        "\n",
        "    for elem in root.iter():\n",
        "        tag = _strip_ns(elem.tag)\n",
        "        if tag == \"place\":\n",
        "            pid = elem.attrib.get(\"id\")\n",
        "            if pid:\n",
        "                places.append(pid)\n",
        "        elif tag == \"transition\":\n",
        "            tid = elem.attrib.get(\"id\")\n",
        "            if tid:\n",
        "                transitions.append(tid)\n",
        "        elif tag == \"arc\":\n",
        "            src = elem.attrib.get(\"source\")\n",
        "            tgt = elem.attrib.get(\"target\")\n",
        "            aid = elem.attrib.get(\"id\", \"\")\n",
        "            if src and tgt:\n",
        "                w = 1\n",
        "                for child in elem.iter():\n",
        "                    if _strip_ns(child.tag) == \"inscription\":\n",
        "                        for g in child.iter():\n",
        "                            if _strip_ns(g.tag) == \"text\" and g.text is not None:\n",
        "                                txt = g.text.strip()\n",
        "                                if txt:\n",
        "                                    try:\n",
        "                                        w = int(txt)\n",
        "                                    except:\n",
        "                                        pass\n",
        "                arcs.append((src, tgt, w, aid))\n",
        "\n",
        "    def uniq(seq):\n",
        "        seen = set()\n",
        "        out = []\n",
        "        for x in seq:\n",
        "            if x not in seen:\n",
        "                out.append(x)\n",
        "                seen.add(x)\n",
        "        return out\n",
        "\n",
        "    places = uniq([p for p in places if p is not None])\n",
        "    transitions = uniq([t for t in transitions if t is not None])\n",
        "\n",
        "    p_index = {p:i for i,p in enumerate(places)}\n",
        "    t_index = {t:i for i,t in enumerate(transitions)}\n",
        "\n",
        "    P = len(places)\n",
        "    T = len(transitions)\n",
        "\n",
        "    Bin = np.zeros((P, T), dtype=int)\n",
        "    Bout = np.zeros((P, T), dtype=int)\n",
        "\n",
        "    for src, tgt, w, _aid in arcs:\n",
        "        if src in p_index and tgt in t_index:\n",
        "            Bin[p_index[src], t_index[tgt]] += w\n",
        "        elif src in t_index and tgt in p_index:\n",
        "            Bout[p_index[tgt], t_index[src]] += w\n",
        "\n",
        "    B = Bout - Bin\n",
        "    return places, transitions, Bin, Bout, B\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "places, transitions, Bin, Bout, B = parse_pnml_to_matrices(pnml_path)\n",
        "t_parse = time.perf_counter() - t0\n",
        "\n",
        "P, T = Bin.shape\n",
        "num_arcs = int((Bin > 0).sum() + (Bout > 0).sum())\n",
        "\n",
        "print(\"\\nParsed PNML:\")\n",
        "print(\"  #places:\", P)\n",
        "print(\"  #transitions:\", T)\n",
        "print(\"  approx #nonzero arcs:\", num_arcs)\n",
        "print(\"  parse time (s):\", t_parse)\n",
        "\n",
        "# ============================================================\n",
        "# 2) BASELINE: global LP per transition\n",
        "#    covered(t) if exists x>=0 s.t. Bx=0 and x_t >= 1\n",
        "# ============================================================\n",
        "def is_transition_covered_lp(B, t_idx, min_xt=1.0):\n",
        "    m, n = B.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B.astype(float)\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[t_idx] = (min_xt, None)\n",
        "    res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "    return res.success\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "baseline_flags = [is_transition_covered_lp(B, j, 1.0) for j in range(T)]\n",
        "t_baseline = time.perf_counter() - t0\n",
        "baseline_uncovered = [j for j, ok in enumerate(baseline_flags) if not ok]\n",
        "\n",
        "print(\"\\nBASELINE uncovered transitions (0-based):\", baseline_uncovered)\n",
        "print(\"Baseline time (s):\", t_baseline)\n",
        "\n",
        "# ============================================================\n",
        "# 3) GICT: Gate-0 SCC prune + per-SCC LP + targeted LP\n",
        "# ============================================================\n",
        "N = P + T\n",
        "def p_node(i): return i\n",
        "def t_node(j): return P + j\n",
        "\n",
        "adj = [[] for _ in range(N)]\n",
        "rev = [[] for _ in range(N)]\n",
        "\n",
        "for i in range(P):\n",
        "    for j in range(T):\n",
        "        if Bin[i, j] > 0:\n",
        "            a, b = p_node(i), t_node(j)\n",
        "            adj[a].append(b); rev[b].append(a)\n",
        "\n",
        "for i in range(P):\n",
        "    for j in range(T):\n",
        "        if Bout[i, j] > 0:\n",
        "            a, b = t_node(j), p_node(i)\n",
        "            adj[a].append(b); rev[b].append(a)\n",
        "\n",
        "def tarjan_scc(adj):\n",
        "    index = 0\n",
        "    stack = []\n",
        "    onstack = [False]*len(adj)\n",
        "    idx = [-1]*len(adj)\n",
        "    low = [0]*len(adj)\n",
        "    sccs = []\n",
        "\n",
        "    def strongconnect(v):\n",
        "        nonlocal index\n",
        "        idx[v] = index\n",
        "        low[v] = index\n",
        "        index += 1\n",
        "        stack.append(v)\n",
        "        onstack[v] = True\n",
        "\n",
        "        for w in adj[v]:\n",
        "            if idx[w] == -1:\n",
        "                strongconnect(w)\n",
        "                low[v] = min(low[v], low[w])\n",
        "            elif onstack[w]:\n",
        "                low[v] = min(low[v], idx[w])\n",
        "\n",
        "        if low[v] == idx[v]:\n",
        "            comp = []\n",
        "            while True:\n",
        "                w = stack.pop()\n",
        "                onstack[w] = False\n",
        "                comp.append(w)\n",
        "                if w == v:\n",
        "                    break\n",
        "            sccs.append(comp)\n",
        "\n",
        "    for v in range(len(adj)):\n",
        "        if idx[v] == -1:\n",
        "            strongconnect(v)\n",
        "    return sccs\n",
        "\n",
        "def scc_has_cycle(comp):\n",
        "    if len(comp) > 1:\n",
        "        return True\n",
        "    v = comp[0]\n",
        "    return v in adj[v]\n",
        "\n",
        "def scc_boundary_size(comp):\n",
        "    comp_set = set(comp)\n",
        "    out_edges = 0\n",
        "    in_edges = 0\n",
        "    for v in comp:\n",
        "        for w in adj[v]:\n",
        "            if w not in comp_set:\n",
        "                out_edges += 1\n",
        "        for u in rev[v]:\n",
        "            if u not in comp_set:\n",
        "                in_edges += 1\n",
        "    return in_edges, out_edges\n",
        "\n",
        "def support_of_witness(x, eps=1e-9):\n",
        "    return [j for j, v in enumerate(x) if v > eps]\n",
        "\n",
        "def lp_find_any_invariant_for_scc(B_full, t_indices):\n",
        "    B_sub = B_full[:, t_indices].astype(float)\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B_sub\n",
        "    b_eq = np.zeros(m)\n",
        "\n",
        "    # sum(x) >= 1  => -sum(x) <= -1\n",
        "    A_ub = -np.ones((1, n))\n",
        "    b_ub = np.array([-1.0])\n",
        "\n",
        "    bounds = [(0, None)] * n\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method=\"highs\")\n",
        "\n",
        "def targeted_cover_lp(B_full, t_global, t_indices, min_xt=1.0):\n",
        "    pos = t_indices.index(t_global)\n",
        "    B_sub = B_full[:, t_indices].astype(float)\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B_sub\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[pos] = (min_xt, None)\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "# Rich-label thresholds (same settings you just used successfully)\n",
        "fragile_k = 3\n",
        "bottleneck_boundary_k = 1\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "sccs = tarjan_scc(adj)\n",
        "t_scc = time.perf_counter() - t0\n",
        "\n",
        "scc_id = {}\n",
        "for k, comp in enumerate(sccs):\n",
        "    for v in comp:\n",
        "        scc_id[v] = k\n",
        "\n",
        "has_cycle = [scc_has_cycle(comp) for comp in sccs]\n",
        "\n",
        "labels = [\"Uncertain\"] * T\n",
        "witnesses = [None] * T\n",
        "\n",
        "# Gate-0\n",
        "t0 = time.perf_counter()\n",
        "for j in range(T):\n",
        "    sid = scc_id[t_node(j)]\n",
        "    if not has_cycle[sid]:\n",
        "        labels[j] = \"StructurallyUncovered\"\n",
        "t_gate0 = time.perf_counter() - t0\n",
        "\n",
        "# Gate-2 per-SCC\n",
        "t0 = time.perf_counter()\n",
        "lp_calls_scc = 0\n",
        "\n",
        "for sid, comp in enumerate(sccs):\n",
        "    t_indices = [v - P for v in comp if v >= P]\n",
        "    if not t_indices:\n",
        "        continue\n",
        "\n",
        "    if not has_cycle[sid]:\n",
        "        for tj in t_indices:\n",
        "            labels[tj] = \"StructurallyUncovered\"\n",
        "        continue\n",
        "\n",
        "    inc, outc = scc_boundary_size(comp)\n",
        "\n",
        "    res = lp_find_any_invariant_for_scc(B, t_indices)\n",
        "    lp_calls_scc += 1\n",
        "\n",
        "    if not res.success:\n",
        "        for tj in t_indices:\n",
        "            labels[tj] = \"StructurallyUncovered\"\n",
        "        continue\n",
        "\n",
        "    x_sub = np.array(res.x)\n",
        "    x_sub[np.abs(x_sub) < 1e-9] = 0.0\n",
        "\n",
        "    full = np.zeros(T, dtype=float)\n",
        "    for k, tj in enumerate(t_indices):\n",
        "        full[tj] = x_sub[k]\n",
        "\n",
        "    supp = support_of_witness(full)\n",
        "    supp_size = len(supp)\n",
        "\n",
        "    for tj in t_indices:\n",
        "        if full[tj] <= 1e-9:\n",
        "            continue\n",
        "\n",
        "        lab = \"CoveredWitness\"\n",
        "        if supp_size <= fragile_k:\n",
        "            lab = \"FragileCovered\"\n",
        "        if inc <= bottleneck_boundary_k or outc <= bottleneck_boundary_k:\n",
        "            lab = \"BottleneckLimited\"\n",
        "\n",
        "        labels[tj] = lab\n",
        "        witnesses[tj] = full\n",
        "\n",
        "t_gate2_scc = time.perf_counter() - t0\n",
        "\n",
        "# Gate-2 targeted (resolve Uncertain)\n",
        "t0 = time.perf_counter()\n",
        "lp_calls_targeted = 0\n",
        "\n",
        "for j in range(T):\n",
        "    if labels[j] != \"Uncertain\":\n",
        "        continue\n",
        "\n",
        "    sid = scc_id[t_node(j)]\n",
        "    comp = sccs[sid]\n",
        "    t_indices = [v - P for v in comp if v >= P]\n",
        "\n",
        "    if not has_cycle[sid]:\n",
        "        labels[j] = \"StructurallyUncovered\"\n",
        "        continue\n",
        "\n",
        "    inc, outc = scc_boundary_size(comp)\n",
        "\n",
        "    res = targeted_cover_lp(B, j, t_indices, min_xt=1.0)\n",
        "    lp_calls_targeted += 1\n",
        "\n",
        "    if not res.success:\n",
        "        labels[j] = \"StructurallyUncovered\"\n",
        "        continue\n",
        "\n",
        "    x_sub = np.array(res.x)\n",
        "    x_sub[np.abs(x_sub) < 1e-9] = 0.0\n",
        "\n",
        "    full = np.zeros(T, dtype=float)\n",
        "    for k, tj in enumerate(t_indices):\n",
        "        full[tj] = x_sub[k]\n",
        "\n",
        "    supp = support_of_witness(full)\n",
        "    supp_size = len(supp)\n",
        "\n",
        "    lab = \"CoveredWitness\"\n",
        "    if supp_size <= fragile_k:\n",
        "        lab = \"FragileCovered\"\n",
        "    if inc <= bottleneck_boundary_k or outc <= bottleneck_boundary_k:\n",
        "        lab = \"BottleneckLimited\"\n",
        "\n",
        "    labels[j] = lab\n",
        "    witnesses[j] = full\n",
        "\n",
        "t_gate2_targeted = time.perf_counter() - t0\n",
        "\n",
        "gict_uncovered = [j for j, lab in enumerate(labels) if lab == \"StructurallyUncovered\"]\n",
        "\n",
        "print(\"\\nGICT uncovered transitions (0-based):\", gict_uncovered)\n",
        "\n",
        "counts = defaultdict(int)\n",
        "for lab in labels:\n",
        "    counts[lab] += 1\n",
        "print(\"\\nGICT label histogram:\")\n",
        "for k in sorted(counts):\n",
        "    print(f\"  {k}: {counts[k]}\")\n",
        "\n",
        "print(\"\\nGICT timings + LP counts:\")\n",
        "print(\"  SCC time (s):\", t_scc)\n",
        "print(\"  Gate-0 time (s):\", t_gate0)\n",
        "print(\"  Gate-2 per-SCC time (s):\", t_gate2_scc)\n",
        "print(\"  Gate-2 targeted time (s):\", t_gate2_targeted)\n",
        "print(\"  LP calls (per-SCC):\", lp_calls_scc)\n",
        "print(\"  LP calls (targeted):\", lp_calls_targeted)\n",
        "print(\"  Total LP calls:\", lp_calls_scc + lp_calls_targeted)\n",
        "\n",
        "print(\"\\nUncovered transition IDs (0-based index, id):\")\n",
        "for j in gict_uncovered:\n",
        "    print(\" \", j, transitions[j])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFMoU5_nim78",
        "outputId": "bc4fb06e-dd6a-42b4-edb0-6d5340088520"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading PNML: https://www.pnml.org/version-2009/examples/philo.pnml\n",
            "\n",
            "Parsed PNML:\n",
            "  #places: 30\n",
            "  #transitions: 30\n",
            "  approx #nonzero arcs: 96\n",
            "  parse time (s): 0.01560910900025192\n",
            "\n",
            "BASELINE uncovered transitions (0-based): []\n",
            "Baseline time (s): 0.31555719900006807\n",
            "\n",
            "GICT uncovered transitions (0-based): []\n",
            "\n",
            "GICT label histogram:\n",
            "  BottleneckLimited: 30\n",
            "\n",
            "GICT timings + LP counts:\n",
            "  SCC time (s): 0.00024946900020950125\n",
            "  Gate-0 time (s): 0.00018381500012765173\n",
            "  Gate-2 per-SCC time (s): 0.005664020000040182\n",
            "  Gate-2 targeted time (s): 0.10242912400008208\n",
            "  LP calls (per-SCC): 1\n",
            "  LP calls (targeted): 27\n",
            "  Total LP calls: 28\n",
            "\n",
            "Uncovered transition IDs (0-based index, id):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "# ============================================================\n",
        "# 0) DOWNLOAD PNML\n",
        "# ============================================================\n",
        "pnml_url = \"https://www.pnml.org/version-2009/examples/philo.pnml\"\n",
        "pnml_path = \"philo.pnml\"\n",
        "\n",
        "if not os.path.exists(pnml_path):\n",
        "    print(\"Downloading PNML:\", pnml_url)\n",
        "    urllib.request.urlretrieve(pnml_url, pnml_path)\n",
        "else:\n",
        "    print(\"PNML already present:\", pnml_path)\n",
        "\n",
        "# ============================================================\n",
        "# 1) PNML PARSER\n",
        "# ============================================================\n",
        "def _strip_ns(tag):\n",
        "    return tag.split(\"}\", 1)[-1] if \"}\" in tag else tag\n",
        "\n",
        "def parse_pnml_to_matrices(pnml_file):\n",
        "    tree = ET.parse(pnml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    places = []\n",
        "    transitions = []\n",
        "    arcs = []\n",
        "\n",
        "    for elem in root.iter():\n",
        "        tag = _strip_ns(elem.tag)\n",
        "        if tag == \"place\":\n",
        "            pid = elem.attrib.get(\"id\")\n",
        "            if pid:\n",
        "                places.append(pid)\n",
        "        elif tag == \"transition\":\n",
        "            tid = elem.attrib.get(\"id\")\n",
        "            if tid:\n",
        "                transitions.append(tid)\n",
        "        elif tag == \"arc\":\n",
        "            src = elem.attrib.get(\"source\")\n",
        "            tgt = elem.attrib.get(\"target\")\n",
        "            aid = elem.attrib.get(\"id\", \"\")\n",
        "            if src and tgt:\n",
        "                w = 1\n",
        "                for child in elem.iter():\n",
        "                    if _strip_ns(child.tag) == \"inscription\":\n",
        "                        for g in child.iter():\n",
        "                            if _strip_ns(g.tag) == \"text\" and g.text is not None:\n",
        "                                txt = g.text.strip()\n",
        "                                if txt:\n",
        "                                    try:\n",
        "                                        w = int(txt)\n",
        "                                    except:\n",
        "                                        pass\n",
        "                arcs.append((src, tgt, w, aid))\n",
        "\n",
        "    def uniq(seq):\n",
        "        seen = set()\n",
        "        out = []\n",
        "        for x in seq:\n",
        "            if x not in seen:\n",
        "                out.append(x)\n",
        "                seen.add(x)\n",
        "        return out\n",
        "\n",
        "    places = uniq([p for p in places if p is not None])\n",
        "    transitions = uniq([t for t in transitions if t is not None])\n",
        "\n",
        "    p_index = {p:i for i,p in enumerate(places)}\n",
        "    t_index = {t:i for i,t in enumerate(transitions)}\n",
        "\n",
        "    P = len(places)\n",
        "    T = len(transitions)\n",
        "\n",
        "    Bin = np.zeros((P, T), dtype=int)\n",
        "    Bout = np.zeros((P, T), dtype=int)\n",
        "\n",
        "    for src, tgt, w, _aid in arcs:\n",
        "        if src in p_index and tgt in t_index:\n",
        "            Bin[p_index[src], t_index[tgt]] += w\n",
        "        elif src in t_index and tgt in p_index:\n",
        "            Bout[p_index[tgt], t_index[src]] += w\n",
        "\n",
        "    B = Bout - Bin\n",
        "    return places, transitions, Bin, Bout, B\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "places, transitions, Bin, Bout, B = parse_pnml_to_matrices(pnml_path)\n",
        "t_parse = time.perf_counter() - t0\n",
        "\n",
        "P, T = Bin.shape\n",
        "num_arcs = int((Bin > 0).sum() + (Bout > 0).sum())\n",
        "\n",
        "print(\"\\nParsed PNML:\")\n",
        "print(\"  #places:\", P)\n",
        "print(\"  #transitions:\", T)\n",
        "print(\"  approx #nonzero arcs:\", num_arcs)\n",
        "print(\"  parse time (s):\", t_parse)\n",
        "\n",
        "# ============================================================\n",
        "# 2) BASELINE\n",
        "# ============================================================\n",
        "def is_transition_covered_lp(B, t_idx, min_xt=1.0):\n",
        "    m, n = B.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B.astype(float)\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[t_idx] = (min_xt, None)\n",
        "    res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "    return res.success\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "baseline_flags = [is_transition_covered_lp(B, j, 1.0) for j in range(T)]\n",
        "t_baseline = time.perf_counter() - t0\n",
        "baseline_uncovered = [j for j, ok in enumerate(baseline_flags) if not ok]\n",
        "\n",
        "print(\"\\nBASELINE uncovered transitions (0-based):\", baseline_uncovered)\n",
        "print(\"Baseline time (s):\", t_baseline)\n",
        "\n",
        "# ============================================================\n",
        "# 3) GICT (max-cover SCC witness)\n",
        "# ============================================================\n",
        "N = P + T\n",
        "def p_node(i): return i\n",
        "def t_node(j): return P + j\n",
        "\n",
        "adj = [[] for _ in range(N)]\n",
        "rev = [[] for _ in range(N)]\n",
        "\n",
        "for i in range(P):\n",
        "    for j in range(T):\n",
        "        if Bin[i, j] > 0:\n",
        "            a, b = p_node(i), t_node(j)\n",
        "            adj[a].append(b); rev[b].append(a)\n",
        "\n",
        "for i in range(P):\n",
        "    for j in range(T):\n",
        "        if Bout[i, j] > 0:\n",
        "            a, b = t_node(j), p_node(i)\n",
        "            adj[a].append(b); rev[b].append(a)\n",
        "\n",
        "def tarjan_scc(adj):\n",
        "    index = 0\n",
        "    stack = []\n",
        "    onstack = [False]*len(adj)\n",
        "    idx = [-1]*len(adj)\n",
        "    low = [0]*len(adj)\n",
        "    sccs = []\n",
        "\n",
        "    def strongconnect(v):\n",
        "        nonlocal index\n",
        "        idx[v] = index\n",
        "        low[v] = index\n",
        "        index += 1\n",
        "        stack.append(v)\n",
        "        onstack[v] = True\n",
        "\n",
        "        for w in adj[v]:\n",
        "            if idx[w] == -1:\n",
        "                strongconnect(w)\n",
        "                low[v] = min(low[v], low[w])\n",
        "            elif onstack[w]:\n",
        "                low[v] = min(low[v], idx[w])\n",
        "\n",
        "        if low[v] == idx[v]:\n",
        "            comp = []\n",
        "            while True:\n",
        "                w = stack.pop()\n",
        "                onstack[w] = False\n",
        "                comp.append(w)\n",
        "                if w == v:\n",
        "                    break\n",
        "            sccs.append(comp)\n",
        "\n",
        "    for v in range(len(adj)):\n",
        "        if idx[v] == -1:\n",
        "            strongconnect(v)\n",
        "    return sccs\n",
        "\n",
        "def scc_has_cycle(comp):\n",
        "    if len(comp) > 1:\n",
        "        return True\n",
        "    v = comp[0]\n",
        "    return v in adj[v]\n",
        "\n",
        "def scc_boundary_size(comp):\n",
        "    comp_set = set(comp)\n",
        "    out_edges = 0\n",
        "    in_edges = 0\n",
        "    for v in comp:\n",
        "        for w in adj[v]:\n",
        "            if w not in comp_set:\n",
        "                out_edges += 1\n",
        "        for u in rev[v]:\n",
        "            if u not in comp_set:\n",
        "                in_edges += 1\n",
        "    return in_edges, out_edges\n",
        "\n",
        "def support_of_witness(x, eps=1e-9):\n",
        "    return [j for j, v in enumerate(x) if v > eps]\n",
        "\n",
        "def lp_find_max_cover_invariant_for_scc(B_full, t_indices):\n",
        "    \"\"\"\n",
        "    Maximize sum(x) subject to:\n",
        "        B_sub x = 0\n",
        "        x >= 0\n",
        "        sum(x) >= 1\n",
        "    Implement as minimize -sum(x).\n",
        "    \"\"\"\n",
        "    B_sub = B_full[:, t_indices].astype(float)\n",
        "    m, n = B_sub.shape\n",
        "\n",
        "    c = -np.ones(n)\n",
        "    A_eq = B_sub\n",
        "    b_eq = np.zeros(m)\n",
        "\n",
        "    A_ub = -np.ones((1, n))   # sum(x) >= 1  => -sum(x) <= -1\n",
        "    b_ub = np.array([-1.0])\n",
        "\n",
        "    bounds = [(0, None)] * n\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method=\"highs\")\n",
        "\n",
        "def targeted_cover_lp(B_full, t_global, t_indices, min_xt=1.0):\n",
        "    pos = t_indices.index(t_global)\n",
        "    B_sub = B_full[:, t_indices].astype(float)\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B_sub\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[pos] = (min_xt, None)\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "fragile_k = 3\n",
        "bottleneck_boundary_k = 1\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "sccs = tarjan_scc(adj)\n",
        "t_scc = time.perf_counter() - t0\n",
        "\n",
        "scc_id = {}\n",
        "for k, comp in enumerate(sccs):\n",
        "    for v in comp:\n",
        "        scc_id[v] = k\n",
        "\n",
        "has_cycle = [scc_has_cycle(comp) for comp in sccs]\n",
        "\n",
        "labels = [\"Uncertain\"] * T\n",
        "witnesses = [None] * T\n",
        "\n",
        "# Gate-0\n",
        "t0 = time.perf_counter()\n",
        "for j in range(T):\n",
        "    sid = scc_id[t_node(j)]\n",
        "    if not has_cycle[sid]:\n",
        "        labels[j] = \"StructurallyUncovered\"\n",
        "t_gate0 = time.perf_counter() - t0\n",
        "\n",
        "# Gate-2 per-SCC (max-cover witness)\n",
        "t0 = time.perf_counter()\n",
        "lp_calls_scc = 0\n",
        "\n",
        "for sid, comp in enumerate(sccs):\n",
        "    t_indices = [v - P for v in comp if v >= P]\n",
        "    if not t_indices:\n",
        "        continue\n",
        "\n",
        "    if not has_cycle[sid]:\n",
        "        for tj in t_indices:\n",
        "            labels[tj] = \"StructurallyUncovered\"\n",
        "        continue\n",
        "\n",
        "    inc, outc = scc_boundary_size(comp)\n",
        "\n",
        "    res = lp_find_max_cover_invariant_for_scc(B, t_indices)\n",
        "    lp_calls_scc += 1\n",
        "\n",
        "    if not res.success:\n",
        "        for tj in t_indices:\n",
        "            labels[tj] = \"StructurallyUncovered\"\n",
        "        continue\n",
        "\n",
        "    x_sub = np.array(res.x)\n",
        "    x_sub[np.abs(x_sub) < 1e-9] = 0.0\n",
        "\n",
        "    full = np.zeros(T, dtype=float)\n",
        "    for k, tj in enumerate(t_indices):\n",
        "        full[tj] = x_sub[k]\n",
        "\n",
        "    supp = support_of_witness(full)\n",
        "    supp_size = len(supp)\n",
        "\n",
        "    for tj in t_indices:\n",
        "        if full[tj] <= 1e-9:\n",
        "            continue\n",
        "\n",
        "        lab = \"CoveredWitness\"\n",
        "        if supp_size <= fragile_k:\n",
        "            lab = \"FragileCovered\"\n",
        "        if inc <= bottleneck_boundary_k or outc <= bottleneck_boundary_k:\n",
        "            lab = \"BottleneckLimited\"\n",
        "\n",
        "        labels[tj] = lab\n",
        "        witnesses[tj] = full\n",
        "\n",
        "t_gate2_scc = time.perf_counter() - t0\n",
        "\n",
        "# Gate-2 targeted (resolve Uncertain)\n",
        "t0 = time.perf_counter()\n",
        "lp_calls_targeted = 0\n",
        "\n",
        "for j in range(T):\n",
        "    if labels[j] != \"Uncertain\":\n",
        "        continue\n",
        "\n",
        "    sid = scc_id[t_node(j)]\n",
        "    comp = sccs[sid]\n",
        "    t_indices = [v - P for v in comp if v >= P]\n",
        "\n",
        "    if not has_cycle[sid]:\n",
        "        labels[j] = \"StructurallyUncovered\"\n",
        "        continue\n",
        "\n",
        "    inc, outc = scc_boundary_size(comp)\n",
        "\n",
        "    res = targeted_cover_lp(B, j, t_indices, min_xt=1.0)\n",
        "    lp_calls_targeted += 1\n",
        "\n",
        "    if not res.success:\n",
        "        labels[j] = \"StructurallyUncovered\"\n",
        "        continue\n",
        "\n",
        "    x_sub = np.array(res.x)\n",
        "    x_sub[np.abs(x_sub) < 1e-9] = 0.0\n",
        "\n",
        "    full = np.zeros(T, dtype=float)\n",
        "    for k, tj in enumerate(t_indices):\n",
        "        full[tj] = x_sub[k]\n",
        "\n",
        "    supp = support_of_witness(full)\n",
        "    supp_size = len(supp)\n",
        "\n",
        "    lab = \"CoveredWitness\"\n",
        "    if supp_size <= fragile_k:\n",
        "        lab = \"FragileCovered\"\n",
        "    if inc <= bottleneck_boundary_k or outc <= bottleneck_boundary_k:\n",
        "        lab = \"BottleneckLimited\"\n",
        "\n",
        "    labels[j] = lab\n",
        "    witnesses[j] = full\n",
        "\n",
        "t_gate2_targeted = time.perf_counter() - t0\n",
        "\n",
        "gict_uncovered = [j for j, lab in enumerate(labels) if lab == \"StructurallyUncovered\"]\n",
        "\n",
        "print(\"\\nGICT uncovered transitions (0-based):\", gict_uncovered)\n",
        "\n",
        "counts = defaultdict(int)\n",
        "for lab in labels:\n",
        "    counts[lab] += 1\n",
        "print(\"\\nGICT label histogram:\")\n",
        "for k in sorted(counts):\n",
        "    print(f\"  {k}: {counts[k]}\")\n",
        "\n",
        "print(\"\\nGICT timings + LP counts:\")\n",
        "print(\"  SCC time (s):\", t_scc)\n",
        "print(\"  Gate-0 time (s):\", t_gate0)\n",
        "print(\"  Gate-2 per-SCC time (s):\", t_gate2_scc)\n",
        "print(\"  Gate-2 targeted time (s):\", t_gate2_targeted)\n",
        "print(\"  LP calls (per-SCC):\", lp_calls_scc)\n",
        "print(\"  LP calls (targeted):\", lp_calls_targeted)\n",
        "print(\"  Total LP calls:\", lp_calls_scc + lp_calls_targeted)"
      ],
      "metadata": {
        "id": "dZ1YQ1bjjBww",
        "outputId": "1569948b-ad39-4b8b-a50a-e836d65b64ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PNML already present: philo.pnml\n",
            "\n",
            "Parsed PNML:\n",
            "  #places: 30\n",
            "  #transitions: 30\n",
            "  approx #nonzero arcs: 96\n",
            "  parse time (s): 0.003046965000066848\n",
            "\n",
            "BASELINE uncovered transitions (0-based): []\n",
            "Baseline time (s): 0.07112341800029753\n",
            "\n",
            "GICT uncovered transitions (0-based): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "\n",
            "GICT label histogram:\n",
            "  StructurallyUncovered: 30\n",
            "\n",
            "GICT timings + LP counts:\n",
            "  SCC time (s): 0.0001609720002306858\n",
            "  Gate-0 time (s): 0.00010829200027728803\n",
            "  Gate-2 per-SCC time (s): 0.003147811999951955\n",
            "  Gate-2 targeted time (s): 0.0005374140000640182\n",
            "  LP calls (per-SCC): 1\n",
            "  LP calls (targeted): 0\n",
            "  Total LP calls: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "# ============================================================\n",
        "# 0) DOWNLOAD PNML\n",
        "# ============================================================\n",
        "pnml_url = \"https://www.pnml.org/version-2009/examples/philo.pnml\"\n",
        "pnml_path = \"philo.pnml\"\n",
        "\n",
        "if not os.path.exists(pnml_path):\n",
        "    print(\"Downloading PNML:\", pnml_url)\n",
        "    urllib.request.urlretrieve(pnml_url, pnml_path)\n",
        "else:\n",
        "    print(\"PNML already present:\", pnml_path)\n",
        "\n",
        "# ============================================================\n",
        "# 1) PNML PARSER\n",
        "# ============================================================\n",
        "def _strip_ns(tag):\n",
        "    return tag.split(\"}\", 1)[-1] if \"}\" in tag else tag\n",
        "\n",
        "def parse_pnml_to_matrices(pnml_file):\n",
        "    tree = ET.parse(pnml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    places = []\n",
        "    transitions = []\n",
        "    arcs = []\n",
        "\n",
        "    for elem in root.iter():\n",
        "        tag = _strip_ns(elem.tag)\n",
        "        if tag == \"place\":\n",
        "            pid = elem.attrib.get(\"id\")\n",
        "            if pid:\n",
        "                places.append(pid)\n",
        "        elif tag == \"transition\":\n",
        "            tid = elem.attrib.get(\"id\")\n",
        "            if tid:\n",
        "                transitions.append(tid)\n",
        "        elif tag == \"arc\":\n",
        "            src = elem.attrib.get(\"source\")\n",
        "            tgt = elem.attrib.get(\"target\")\n",
        "            aid = elem.attrib.get(\"id\", \"\")\n",
        "            if src and tgt:\n",
        "                w = 1\n",
        "                for child in elem.iter():\n",
        "                    if _strip_ns(child.tag) == \"inscription\":\n",
        "                        for g in child.iter():\n",
        "                            if _strip_ns(g.tag) == \"text\" and g.text is not None:\n",
        "                                txt = g.text.strip()\n",
        "                                if txt:\n",
        "                                    try:\n",
        "                                        w = int(txt)\n",
        "                                    except:\n",
        "                                        pass\n",
        "                arcs.append((src, tgt, w, aid))\n",
        "\n",
        "    def uniq(seq):\n",
        "        seen = set()\n",
        "        out = []\n",
        "        for x in seq:\n",
        "            if x not in seen:\n",
        "                out.append(x)\n",
        "                seen.add(x)\n",
        "        return out\n",
        "\n",
        "    places = uniq([p for p in places if p is not None])\n",
        "    transitions = uniq([t for t in transitions if t is not None])\n",
        "\n",
        "    p_index = {p:i for i,p in enumerate(places)}\n",
        "    t_index = {t:i for i,t in enumerate(transitions)}\n",
        "\n",
        "    P = len(places)\n",
        "    T = len(transitions)\n",
        "\n",
        "    Bin = np.zeros((P, T), dtype=int)\n",
        "    Bout = np.zeros((P, T), dtype=int)\n",
        "\n",
        "    for src, tgt, w, _aid in arcs:\n",
        "        if src in p_index and tgt in t_index:\n",
        "            Bin[p_index[src], t_index[tgt]] += w\n",
        "        elif src in t_index and tgt in p_index:\n",
        "            Bout[p_index[tgt], t_index[src]] += w\n",
        "\n",
        "    B = Bout - Bin\n",
        "    return places, transitions, Bin, Bout, B\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "places, transitions, Bin, Bout, B = parse_pnml_to_matrices(pnml_path)\n",
        "t_parse = time.perf_counter() - t0\n",
        "\n",
        "P, T = Bin.shape\n",
        "num_arcs = int((Bin > 0).sum() + (Bout > 0).sum())\n",
        "\n",
        "print(\"\\nParsed PNML:\")\n",
        "print(\"  #places:\", P)\n",
        "print(\"  #transitions:\", T)\n",
        "print(\"  approx #nonzero arcs:\", num_arcs)\n",
        "print(\"  parse time (s):\", t_parse)\n",
        "\n",
        "# ============================================================\n",
        "# 2) BASELINE: global LP per transition\n",
        "# ============================================================\n",
        "def is_transition_covered_lp(B, t_idx, min_xt=1.0):\n",
        "    m, n = B.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B.astype(float)\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[t_idx] = (min_xt, None)\n",
        "    res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "    return res.success\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "baseline_flags = [is_transition_covered_lp(B, j, 1.0) for j in range(T)]\n",
        "t_baseline = time.perf_counter() - t0\n",
        "baseline_uncovered = [j for j, ok in enumerate(baseline_flags) if not ok]\n",
        "\n",
        "print(\"\\nBASELINE uncovered transitions (0-based):\", baseline_uncovered)\n",
        "print(\"Baseline time (s):\", t_baseline)\n",
        "\n",
        "# ============================================================\n",
        "# 3) GICT (bounded max-cover SCC witness)\n",
        "# ============================================================\n",
        "N = P + T\n",
        "def p_node(i): return i\n",
        "def t_node(j): return P + j\n",
        "\n",
        "adj = [[] for _ in range(N)]\n",
        "rev = [[] for _ in range(N)]\n",
        "\n",
        "for i in range(P):\n",
        "    for j in range(T):\n",
        "        if Bin[i, j] > 0:\n",
        "            a, b = p_node(i), t_node(j)\n",
        "            adj[a].append(b); rev[b].append(a)\n",
        "\n",
        "for i in range(P):\n",
        "    for j in range(T):\n",
        "        if Bout[i, j] > 0:\n",
        "            a, b = t_node(j), p_node(i)\n",
        "            adj[a].append(b); rev[b].append(a)\n",
        "\n",
        "def tarjan_scc(adj):\n",
        "    index = 0\n",
        "    stack = []\n",
        "    onstack = [False]*len(adj)\n",
        "    idx = [-1]*len(adj)\n",
        "    low = [0]*len(adj)\n",
        "    sccs = []\n",
        "\n",
        "    def strongconnect(v):\n",
        "        nonlocal index\n",
        "        idx[v] = index\n",
        "        low[v] = index\n",
        "        index += 1\n",
        "        stack.append(v)\n",
        "        onstack[v] = True\n",
        "\n",
        "        for w in adj[v]:\n",
        "            if idx[w] == -1:\n",
        "                strongconnect(w)\n",
        "                low[v] = min(low[v], low[w])\n",
        "            elif onstack[w]:\n",
        "                low[v] = min(low[v], idx[w])\n",
        "\n",
        "        if low[v] == idx[v]:\n",
        "            comp = []\n",
        "            while True:\n",
        "                w = stack.pop()\n",
        "                onstack[w] = False\n",
        "                comp.append(w)\n",
        "                if w == v:\n",
        "                    break\n",
        "            sccs.append(comp)\n",
        "\n",
        "    for v in range(len(adj)):\n",
        "        if idx[v] == -1:\n",
        "            strongconnect(v)\n",
        "    return sccs\n",
        "\n",
        "def scc_has_cycle(comp):\n",
        "    if len(comp) > 1:\n",
        "        return True\n",
        "    v = comp[0]\n",
        "    return v in adj[v]\n",
        "\n",
        "def scc_boundary_size(comp):\n",
        "    comp_set = set(comp)\n",
        "    out_edges = 0\n",
        "    in_edges = 0\n",
        "    for v in comp:\n",
        "        for w in adj[v]:\n",
        "            if w not in comp_set:\n",
        "                out_edges += 1\n",
        "        for u in rev[v]:\n",
        "            if u not in comp_set:\n",
        "                in_edges += 1\n",
        "    return in_edges, out_edges\n",
        "\n",
        "def support_of_witness(x, eps=1e-9):\n",
        "    return [j for j, v in enumerate(x) if v > eps]\n",
        "\n",
        "def lp_find_bounded_max_cover_invariant_for_scc(B_full, t_indices):\n",
        "    \"\"\"\n",
        "    BOUNDED variant:\n",
        "      maximize sum(x) subject to:\n",
        "        B_sub x = 0\n",
        "        x >= 0\n",
        "        sum(x) = 1\n",
        "    This is bounded (simplex normalization).\n",
        "    Implement as minimize -sum(x) with sum(x)=1.\n",
        "    \"\"\"\n",
        "    B_sub = B_full[:, t_indices].astype(float)\n",
        "    m, n = B_sub.shape\n",
        "\n",
        "    c = -np.ones(n)\n",
        "    A_eq = np.vstack([B_sub, np.ones((1, n))])   # add sum(x)=1\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[-1] = 1.0\n",
        "\n",
        "    bounds = [(0, None)] * n\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "def targeted_cover_lp(B_full, t_global, t_indices, min_xt=1.0):\n",
        "    pos = t_indices.index(t_global)\n",
        "    B_sub = B_full[:, t_indices].astype(float)\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B_sub\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[pos] = (min_xt, None)\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "fragile_k = 3\n",
        "bottleneck_boundary_k = 1\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "sccs = tarjan_scc(adj)\n",
        "t_scc = time.perf_counter() - t0\n",
        "\n",
        "scc_id = {}\n",
        "for k, comp in enumerate(sccs):\n",
        "    for v in comp:\n",
        "        scc_id[v] = k\n",
        "\n",
        "has_cycle = [scc_has_cycle(comp) for comp in sccs]\n",
        "\n",
        "labels = [\"Uncertain\"] * T\n",
        "witnesses = [None] * T\n",
        "\n",
        "# Gate-0\n",
        "t0 = time.perf_counter()\n",
        "for j in range(T):\n",
        "    sid = scc_id[t_node(j)]\n",
        "    if not has_cycle[sid]:\n",
        "        labels[j] = \"StructurallyUncovered\"\n",
        "t_gate0 = time.perf_counter() - t0\n",
        "\n",
        "# Gate-2 per-SCC (bounded max-cover witness)\n",
        "t0 = time.perf_counter()\n",
        "lp_calls_scc = 0\n",
        "\n",
        "for sid, comp in enumerate(sccs):\n",
        "    t_indices = [v - P for v in comp if v >= P]\n",
        "    if not t_indices:\n",
        "        continue\n",
        "\n",
        "    if not has_cycle[sid]:\n",
        "        for tj in t_indices:\n",
        "            labels[tj] = \"StructurallyUncovered\"\n",
        "        continue\n",
        "\n",
        "    inc, outc = scc_boundary_size(comp)\n",
        "\n",
        "    res = lp_find_bounded_max_cover_invariant_for_scc(B, t_indices)\n",
        "    lp_calls_scc += 1\n",
        "\n",
        "    if not res.success:\n",
        "        # If this ever happens on philo, we want to see it explicitly\n",
        "        for tj in t_indices:\n",
        "            labels[tj] = \"Uncertain\"\n",
        "        continue\n",
        "\n",
        "    x_sub = np.array(res.x)\n",
        "    x_sub[np.abs(x_sub) < 1e-9] = 0.0\n",
        "\n",
        "    full = np.zeros(T, dtype=float)\n",
        "    for k, tj in enumerate(t_indices):\n",
        "        full[tj] = x_sub[k]\n",
        "\n",
        "    supp = support_of_witness(full)\n",
        "    supp_size = len(supp)\n",
        "\n",
        "    for tj in t_indices:\n",
        "        if full[tj] <= 1e-12:\n",
        "            continue\n",
        "\n",
        "        lab = \"CoveredWitness\"\n",
        "        if supp_size <= fragile_k:\n",
        "            lab = \"FragileCovered\"\n",
        "        if inc <= bottleneck_boundary_k or outc <= bottleneck_boundary_k:\n",
        "            lab = \"BottleneckLimited\"\n",
        "\n",
        "        labels[tj] = lab\n",
        "        witnesses[tj] = full\n",
        "\n",
        "t_gate2_scc = time.perf_counter() - t0\n",
        "\n",
        "# Gate-2 targeted (resolve Uncertain)\n",
        "t0 = time.perf_counter()\n",
        "lp_calls_targeted = 0\n",
        "\n",
        "for j in range(T):\n",
        "    if labels[j] != \"Uncertain\":\n",
        "        continue\n",
        "\n",
        "    sid = scc_id[t_node(j)]\n",
        "    comp = sccs[sid]\n",
        "    t_indices = [v - P for v in comp if v >= P]\n",
        "\n",
        "    if not has_cycle[sid]:\n",
        "        labels[j] = \"StructurallyUncovered\"\n",
        "        continue\n",
        "\n",
        "    res = targeted_cover_lp(B, j, t_indices, min_xt=1.0)\n",
        "    lp_calls_targeted += 1\n",
        "\n",
        "    if not res.success:\n",
        "        labels[j] = \"StructurallyUncovered\"\n",
        "        continue\n",
        "\n",
        "    labels[j] = \"CoveredWitness\"\n",
        "\n",
        "t_gate2_targeted = time.perf_counter() - t0\n",
        "\n",
        "gict_uncovered = [j for j, lab in enumerate(labels) if lab == \"StructurallyUncovered\"]\n",
        "\n",
        "print(\"\\nGICT uncovered transitions (0-based):\", gict_uncovered)\n",
        "\n",
        "counts = defaultdict(int)\n",
        "for lab in labels:\n",
        "    counts[lab] += 1\n",
        "print(\"\\nGICT label histogram:\")\n",
        "for k in sorted(counts):\n",
        "    print(f\"  {k}: {counts[k]}\")\n",
        "\n",
        "print(\"\\nGICT timings + LP counts:\")\n",
        "print(\"  SCC time (s):\", t_scc)\n",
        "print(\"  Gate-0 time (s):\", t_gate0)\n",
        "print(\"  Gate-2 per-SCC time (s):\", t_gate2_scc)\n",
        "print(\"  Gate-2 targeted time (s):\", t_gate2_targeted)\n",
        "print(\"  LP calls (per-SCC):\", lp_calls_scc)\n",
        "print(\"  LP calls (targeted):\", lp_calls_targeted)\n",
        "print(\"  Total LP calls:\", lp_calls_scc + lp_calls_targeted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iZYBXYYkI0c",
        "outputId": "882929a9-3b3d-4ad6-a6ba-48900f252f0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PNML already present: philo.pnml\n",
            "\n",
            "Parsed PNML:\n",
            "  #places: 30\n",
            "  #transitions: 30\n",
            "  approx #nonzero arcs: 96\n",
            "  parse time (s): 0.011950092000006407\n",
            "\n",
            "BASELINE uncovered transitions (0-based): []\n",
            "Baseline time (s): 0.15114133000042784\n",
            "\n",
            "GICT uncovered transitions (0-based): []\n",
            "\n",
            "GICT label histogram:\n",
            "  BottleneckLimited: 3\n",
            "  CoveredWitness: 27\n",
            "\n",
            "GICT timings + LP counts:\n",
            "  SCC time (s): 0.0002372040003137954\n",
            "  Gate-0 time (s): 0.00020010400021419628\n",
            "  Gate-2 per-SCC time (s): 0.00685057600003347\n",
            "  Gate-2 targeted time (s): 0.261135134000142\n",
            "  LP calls (per-SCC): 1\n",
            "  LP calls (targeted): 27\n",
            "  Total LP calls: 28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import xml.etree.ElementTree as ET\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "pnml_path = \"philo.pnml\"\n",
        "\n",
        "def _strip_ns(tag):\n",
        "    return tag.split(\"}\", 1)[-1] if \"}\" in tag else tag\n",
        "\n",
        "def parse_pnml_to_matrices(pnml_file):\n",
        "    tree = ET.parse(pnml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    places = []\n",
        "    transitions = []\n",
        "    arcs = []\n",
        "\n",
        "    for elem in root.iter():\n",
        "        tag = _strip_ns(elem.tag)\n",
        "        if tag == \"place\":\n",
        "            pid = elem.attrib.get(\"id\")\n",
        "            if pid:\n",
        "                places.append(pid)\n",
        "        elif tag == \"transition\":\n",
        "            tid = elem.attrib.get(\"id\")\n",
        "            if tid:\n",
        "                transitions.append(tid)\n",
        "        elif tag == \"arc\":\n",
        "            src = elem.attrib.get(\"source\")\n",
        "            tgt = elem.attrib.get(\"target\")\n",
        "            if src and tgt:\n",
        "                w = 1\n",
        "                for child in elem.iter():\n",
        "                    if _strip_ns(child.tag) == \"inscription\":\n",
        "                        for g in child.iter():\n",
        "                            if _strip_ns(g.tag) == \"text\" and g.text is not None:\n",
        "                                txt = g.text.strip()\n",
        "                                if txt:\n",
        "                                    try:\n",
        "                                        w = int(txt)\n",
        "                                    except:\n",
        "                                        pass\n",
        "                arcs.append((src, tgt, w))\n",
        "\n",
        "    def uniq(seq):\n",
        "        seen = set()\n",
        "        out = []\n",
        "        for x in seq:\n",
        "            if x not in seen:\n",
        "                out.append(x)\n",
        "                seen.add(x)\n",
        "        return out\n",
        "\n",
        "    places = uniq(places)\n",
        "    transitions = uniq(transitions)\n",
        "\n",
        "    p_index = {p:i for i,p in enumerate(places)}\n",
        "    t_index = {t:i for i,t in enumerate(transitions)}\n",
        "\n",
        "    P = len(places)\n",
        "    T = len(transitions)\n",
        "\n",
        "    Bin = np.zeros((P, T), dtype=int)\n",
        "    Bout = np.zeros((P, T), dtype=int)\n",
        "\n",
        "    for src, tgt, w in arcs:\n",
        "        if src in p_index and tgt in t_index:\n",
        "            Bin[p_index[src], t_index[tgt]] += w\n",
        "        elif src in t_index and tgt in p_index:\n",
        "            Bout[p_index[tgt], t_index[src]] += w\n",
        "\n",
        "    B = Bout - Bin\n",
        "    return places, transitions, Bin, Bout, B\n",
        "\n",
        "places, transitions, Bin, Bout, B = parse_pnml_to_matrices(pnml_path)\n",
        "P, T = Bin.shape\n",
        "\n",
        "def lp_find_bounded_invariant(B_full):\n",
        "    B_sub = B_full.astype(float)\n",
        "    m, n = B_sub.shape\n",
        "\n",
        "    # Feasibility on simplex: Bx=0, x>=0, sum(x)=1\n",
        "    c = np.zeros(n)\n",
        "    A_eq = np.vstack([B_sub, np.ones((1, n))])\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[-1] = 1.0\n",
        "    bounds = [(0, None)] * n\n",
        "\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "res = lp_find_bounded_invariant(B)\n",
        "print(\"LP success:\", res.success)\n",
        "print(\"LP status:\", res.status, res.message)\n",
        "\n",
        "if res.success:\n",
        "    x = np.array(res.x)\n",
        "    x[np.abs(x) < 1e-12] = 0.0\n",
        "    supp = [j for j,v in enumerate(x) if v > 0]\n",
        "    print(\"Support size:\", len(supp))\n",
        "    print(\"Support indices:\", supp)\n",
        "    print(\"Support IDs:\")\n",
        "    for j in supp:\n",
        "        print(\" \", j, transitions[j], \"x=\", x[j])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5xrC8VMkc5o",
        "outputId": "457de8f3-830f-4322-8306-c8f96535fee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LP success: True\n",
            "LP status: 0 Optimization terminated successfully. (HiGHS Status 7: Optimal)\n",
            "Support size: 3\n",
            "Support indices: [14, 19, 24]\n",
            "Support IDs:\n",
            "  14 cId184-i943123747 x= 0.3333333333333333\n",
            "  19 cId200-i943123747 x= 0.3333333333333333\n",
            "  24 cId199-i943123747 x= 0.3333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "# ============================================================\n",
        "# 0) DOWNLOAD PNML\n",
        "# ============================================================\n",
        "pnml_url = \"https://www.pnml.org/version-2009/examples/philo.pnml\"\n",
        "pnml_path = \"philo.pnml\"\n",
        "\n",
        "if not os.path.exists(pnml_path):\n",
        "    print(\"Downloading PNML:\", pnml_url)\n",
        "    urllib.request.urlretrieve(pnml_url, pnml_path)\n",
        "else:\n",
        "    print(\"PNML already present:\", pnml_path)\n",
        "\n",
        "# ============================================================\n",
        "# 1) PNML PARSER\n",
        "# ============================================================\n",
        "def _strip_ns(tag):\n",
        "    return tag.split(\"}\", 1)[-1] if \"}\" in tag else tag\n",
        "\n",
        "def parse_pnml_to_matrices(pnml_file):\n",
        "    tree = ET.parse(pnml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    places = []\n",
        "    transitions = []\n",
        "    arcs = []\n",
        "\n",
        "    for elem in root.iter():\n",
        "        tag = _strip_ns(elem.tag)\n",
        "        if tag == \"place\":\n",
        "            pid = elem.attrib.get(\"id\")\n",
        "            if pid:\n",
        "                places.append(pid)\n",
        "        elif tag == \"transition\":\n",
        "            tid = elem.attrib.get(\"id\")\n",
        "            if tid:\n",
        "                transitions.append(tid)\n",
        "        elif tag == \"arc\":\n",
        "            src = elem.attrib.get(\"source\")\n",
        "            tgt = elem.attrib.get(\"target\")\n",
        "            aid = elem.attrib.get(\"id\", \"\")\n",
        "            if src and tgt:\n",
        "                w = 1\n",
        "                for child in elem.iter():\n",
        "                    if _strip_ns(child.tag) == \"inscription\":\n",
        "                        for g in child.iter():\n",
        "                            if _strip_ns(g.tag) == \"text\" and g.text is not None:\n",
        "                                txt = g.text.strip()\n",
        "                                if txt:\n",
        "                                    try:\n",
        "                                        w = int(txt)\n",
        "                                    except:\n",
        "                                        pass\n",
        "                arcs.append((src, tgt, w, aid))\n",
        "\n",
        "    def uniq(seq):\n",
        "        seen = set()\n",
        "        out = []\n",
        "        for x in seq:\n",
        "            if x not in seen:\n",
        "                out.append(x)\n",
        "                seen.add(x)\n",
        "        return out\n",
        "\n",
        "    places = uniq([p for p in places if p is not None])\n",
        "    transitions = uniq([t for t in transitions if t is not None])\n",
        "\n",
        "    p_index = {p:i for i,p in enumerate(places)}\n",
        "    t_index = {t:i for i,t in enumerate(transitions)}\n",
        "\n",
        "    P = len(places)\n",
        "    T = len(transitions)\n",
        "\n",
        "    Bin = np.zeros((P, T), dtype=int)\n",
        "    Bout = np.zeros((P, T), dtype=int)\n",
        "\n",
        "    for src, tgt, w, _aid in arcs:\n",
        "        if src in p_index and tgt in t_index:\n",
        "            Bin[p_index[src], t_index[tgt]] += w\n",
        "        elif src in t_index and tgt in p_index:\n",
        "            Bout[p_index[tgt], t_index[src]] += w\n",
        "\n",
        "    B = Bout - Bin\n",
        "    return places, transitions, Bin, Bout, B\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "places, transitions, Bin, Bout, B = parse_pnml_to_matrices(pnml_path)\n",
        "t_parse = time.perf_counter() - t0\n",
        "\n",
        "P, T = Bin.shape\n",
        "num_arcs = int((Bin > 0).sum() + (Bout > 0).sum())\n",
        "\n",
        "print(\"\\nParsed PNML:\")\n",
        "print(\"  #places:\", P)\n",
        "print(\"  #transitions:\", T)\n",
        "print(\"  approx #nonzero arcs:\", num_arcs)\n",
        "print(\"  parse time (s):\", t_parse)\n",
        "\n",
        "# ============================================================\n",
        "# 2) BASELINE: global LP per transition\n",
        "# ============================================================\n",
        "def is_transition_covered_lp(B, t_idx, min_xt=1.0):\n",
        "    m, n = B.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B.astype(float)\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[t_idx] = (min_xt, None)\n",
        "    res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "    return res.success\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "baseline_flags = [is_transition_covered_lp(B, j, 1.0) for j in range(T)]\n",
        "t_baseline = time.perf_counter() - t0\n",
        "baseline_uncovered = [j for j, ok in enumerate(baseline_flags) if not ok]\n",
        "\n",
        "print(\"\\nBASELINE uncovered transitions (0-based):\", baseline_uncovered)\n",
        "print(\"Baseline time (s):\", t_baseline)\n",
        "\n",
        "# ============================================================\n",
        "# 3) Dense invariant probe (global, diagnostic)\n",
        "# ============================================================\n",
        "def dense_simplex_invariant(B_sub, eps):\n",
        "    \"\"\"\n",
        "    Find x such that:\n",
        "      B_sub x = 0\n",
        "      x >= eps\n",
        "      sum(x) = 1\n",
        "    Feasible => a fully dense invariant exists at granularity eps.\n",
        "    \"\"\"\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = np.vstack([B_sub, np.ones((1, n))])\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[-1] = 1.0\n",
        "    bounds = [(eps, None)] * n\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "eps = 1e-6\n",
        "res_dense = dense_simplex_invariant(B.astype(float), eps)\n",
        "print(\"\\nDense-invariant probe:\")\n",
        "print(\"  eps:\", eps)\n",
        "print(\"  success:\", res_dense.success)\n",
        "print(\"  status:\", res_dense.status, res_dense.message)\n",
        "if res_dense.success:\n",
        "    x = np.array(res_dense.x)\n",
        "    x[np.abs(x) < 1e-12] = 0.0\n",
        "    supp = [j for j,v in enumerate(x) if v > 0]\n",
        "    print(\"  support size:\", len(supp), \"of\", T)\n",
        "    print(\"  min(x):\", float(x.min()), \" max(x):\", float(x.max()))\n",
        "else:\n",
        "    print(\"  (No fully dense invariant at this eps.)\")\n",
        "\n",
        "# ============================================================\n",
        "# 4) GICT using -dense per-SCC witness\n",
        "# ============================================================\n",
        "N = P + T\n",
        "def p_node(i): return i\n",
        "def t_node(j): return P + j\n",
        "\n",
        "adj = [[] for _ in range(N)]\n",
        "rev = [[] for _ in range(N)]\n",
        "\n",
        "for i in range(P):\n",
        "    for j in range(T):\n",
        "        if Bin[i, j] > 0:\n",
        "            a, b = p_node(i), t_node(j)\n",
        "            adj[a].append(b); rev[b].append(a)\n",
        "\n",
        "for i in range(P):\n",
        "    for j in range(T):\n",
        "        if Bout[i, j] > 0:\n",
        "            a, b = t_node(j), p_node(i)\n",
        "            adj[a].append(b); rev[b].append(a)\n",
        "\n",
        "def tarjan_scc(adj):\n",
        "    index = 0\n",
        "    stack = []\n",
        "    onstack = [False]*len(adj)\n",
        "    idx = [-1]*len(adj)\n",
        "    low = [0]*len(adj)\n",
        "    sccs = []\n",
        "\n",
        "    def strongconnect(v):\n",
        "        nonlocal index\n",
        "        idx[v] = index\n",
        "        low[v] = index\n",
        "        index += 1\n",
        "        stack.append(v)\n",
        "        onstack[v] = True\n",
        "\n",
        "        for w in adj[v]:\n",
        "            if idx[w] == -1:\n",
        "                strongconnect(w)\n",
        "                low[v] = min(low[v], low[w])\n",
        "            elif onstack[w]:\n",
        "                low[v] = min(low[v], idx[w])\n",
        "\n",
        "        if low[v] == idx[v]:\n",
        "            comp = []\n",
        "            while True:\n",
        "                w = stack.pop()\n",
        "                onstack[w] = False\n",
        "                comp.append(w)\n",
        "                if w == v:\n",
        "                    break\n",
        "            sccs.append(comp)\n",
        "\n",
        "    for v in range(len(adj)):\n",
        "        if idx[v] == -1:\n",
        "            strongconnect(v)\n",
        "    return sccs\n",
        "\n",
        "def scc_has_cycle(comp):\n",
        "    if len(comp) > 1:\n",
        "        return True\n",
        "    v = comp[0]\n",
        "    return v in adj[v]\n",
        "\n",
        "def scc_boundary_size(comp):\n",
        "    comp_set = set(comp)\n",
        "    out_edges = 0\n",
        "    in_edges = 0\n",
        "    for v in comp:\n",
        "        for w in adj[v]:\n",
        "            if w not in comp_set:\n",
        "                out_edges += 1\n",
        "        for u in rev[v]:\n",
        "            if u not in comp_set:\n",
        "                in_edges += 1\n",
        "    return in_edges, out_edges\n",
        "\n",
        "def support_of_witness(x, eps0=1e-12):\n",
        "    return [j for j, v in enumerate(x) if v > eps0]\n",
        "\n",
        "def lp_find_dense_invariant_for_scc(B_full, t_indices, eps_local):\n",
        "    B_sub = B_full[:, t_indices].astype(float)\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = np.vstack([B_sub, np.ones((1, n))])\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[-1] = 1.0\n",
        "    bounds = [(eps_local, None)] * n\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "def targeted_cover_lp(B_full, t_global, t_indices, min_xt=1.0):\n",
        "    pos = t_indices.index(t_global)\n",
        "    B_sub = B_full[:, t_indices].astype(float)\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B_sub\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[pos] = (min_xt, None)\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "fragile_k = 3\n",
        "bottleneck_boundary_k = 1\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "sccs = tarjan_scc(adj)\n",
        "t_scc = time.perf_counter() - t0\n",
        "\n",
        "scc_id = {}\n",
        "for k, comp in enumerate(sccs):\n",
        "    for v in comp:\n",
        "        scc_id[v] = k\n",
        "\n",
        "has_cycle = [scc_has_cycle(comp) for comp in sccs]\n",
        "\n",
        "labels = [\"Uncertain\"] * T\n",
        "witnesses = [None] * T\n",
        "\n",
        "# Gate-0\n",
        "t0 = time.perf_counter()\n",
        "for j in range(T):\n",
        "    sid = scc_id[t_node(j)]\n",
        "    if not has_cycle[sid]:\n",
        "        labels[j] = \"StructurallyUncovered\"\n",
        "t_gate0 = time.perf_counter() - t0\n",
        "\n",
        "# Gate-2 per-SCC (-dense witness)\n",
        "t0 = time.perf_counter()\n",
        "lp_calls_scc = 0\n",
        "\n",
        "for sid, comp in enumerate(sccs):\n",
        "    t_indices = [v - P for v in comp if v >= P]\n",
        "    if not t_indices:\n",
        "        continue\n",
        "\n",
        "    if not has_cycle[sid]:\n",
        "        for tj in t_indices:\n",
        "            labels[tj] = \"StructurallyUncovered\"\n",
        "        continue\n",
        "\n",
        "    inc, outc = scc_boundary_size(comp)\n",
        "\n",
        "    res = lp_find_dense_invariant_for_scc(B, t_indices, eps)\n",
        "    lp_calls_scc += 1\n",
        "\n",
        "    if not res.success:\n",
        "        # Fall back to uncertainty; targeted will resolve if needed\n",
        "        continue\n",
        "\n",
        "    x_sub = np.array(res.x)\n",
        "    x_sub[np.abs(x_sub) < 1e-12] = 0.0\n",
        "\n",
        "    full = np.zeros(T, dtype=float)\n",
        "    for k, tj in enumerate(t_indices):\n",
        "        full[tj] = x_sub[k]\n",
        "\n",
        "    supp = support_of_witness(full)\n",
        "    supp_size = len(supp)\n",
        "\n",
        "    for tj in t_indices:\n",
        "        if full[tj] <= 1e-12:\n",
        "            continue\n",
        "\n",
        "        lab = \"CoveredWitness\"\n",
        "        if supp_size <= fragile_k:\n",
        "            lab = \"FragileCovered\"\n",
        "        if inc <= bottleneck_boundary_k or outc <= bottleneck_boundary_k:\n",
        "            lab = \"BottleneckLimited\"\n",
        "\n",
        "        labels[tj] = lab\n",
        "        witnesses[tj] = full\n",
        "\n",
        "t_gate2_scc = time.perf_counter() - t0\n",
        "\n",
        "# Gate-2 targeted (resolve Uncertain)\n",
        "t0 = time.perf_counter()\n",
        "lp_calls_targeted = 0\n",
        "\n",
        "for j in range(T):\n",
        "    if labels[j] != \"Uncertain\":\n",
        "        continue\n",
        "\n",
        "    sid = scc_id[t_node(j)]\n",
        "    comp = sccs[sid]\n",
        "    t_indices = [v - P for v in comp if v >= P]\n",
        "\n",
        "    if not has_cycle[sid]:\n",
        "        labels[j] = \"StructurallyUncovered\"\n",
        "        continue\n",
        "\n",
        "    res = targeted_cover_lp(B, j, t_indices, min_xt=1.0)\n",
        "    lp_calls_targeted += 1\n",
        "\n",
        "    if not res.success:\n",
        "        labels[j] = \"StructurallyUncovered\"\n",
        "    else:\n",
        "        labels[j] = \"CoveredWitness\"\n",
        "\n",
        "t_gate2_targeted = time.perf_counter() - t0\n",
        "\n",
        "gict_uncovered = [j for j, lab in enumerate(labels) if lab == \"StructurallyUncovered\"]\n",
        "\n",
        "print(\"\\nGICT uncovered transitions (0-based):\", gict_uncovered)\n",
        "\n",
        "counts = defaultdict(int)\n",
        "for lab in labels:\n",
        "    counts[lab] += 1\n",
        "print(\"\\nGICT label histogram:\")\n",
        "for k in sorted(counts):\n",
        "    print(f\"  {k}: {counts[k]}\")\n",
        "\n",
        "print(\"\\nGICT timings + LP counts:\")\n",
        "print(\"  SCC time (s):\", t_scc)\n",
        "print(\"  Gate-0 time (s):\", t_gate0)\n",
        "print(\"  Gate-2 per-SCC time (s):\", t_gate2_scc)\n",
        "print(\"  Gate-2 targeted time (s):\", t_gate2_targeted)\n",
        "print(\"  LP calls (per-SCC):\", lp_calls_scc)\n",
        "print(\"  LP calls (targeted):\", lp_calls_targeted)\n",
        "print(\"  Total LP calls:\", lp_calls_scc + lp_calls_targeted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3_AOZl8ly37",
        "outputId": "ce187bcf-fd90-4e10-f781-e2e61730dfcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PNML already present: philo.pnml\n",
            "\n",
            "Parsed PNML:\n",
            "  #places: 30\n",
            "  #transitions: 30\n",
            "  approx #nonzero arcs: 96\n",
            "  parse time (s): 0.02714940399982879\n",
            "\n",
            "BASELINE uncovered transitions (0-based): []\n",
            "Baseline time (s): 0.38836018600022726\n",
            "\n",
            "Dense-invariant probe:\n",
            "  eps: 1e-06\n",
            "  success: True\n",
            "  status: 0 Optimization terminated successfully. (HiGHS Status 7: Optimal)\n",
            "  support size: 30 of 30\n",
            "  min(x): 1e-06  max(x): 0.3333233333333333\n",
            "\n",
            "GICT uncovered transitions (0-based): []\n",
            "\n",
            "GICT label histogram:\n",
            "  BottleneckLimited: 30\n",
            "\n",
            "GICT timings + LP counts:\n",
            "  SCC time (s): 0.000244751000082033\n",
            "  Gate-0 time (s): 0.0026438199997755873\n",
            "  Gate-2 per-SCC time (s): 0.016639252999993914\n",
            "  Gate-2 targeted time (s): 0.0015526870001849602\n",
            "  LP calls (per-SCC): 1\n",
            "  LP calls (targeted): 0\n",
            "  Total LP calls: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "# ============================================================\n",
        "# 0) DOWNLOAD PNML (Token-ring)\n",
        "# ============================================================\n",
        "pnml_url = \"https://www.pnml.org/version-2009/examples/Token-ring.pnml\"\n",
        "pnml_path = \"Token-ring.pnml\"\n",
        "\n",
        "if not os.path.exists(pnml_path):\n",
        "    print(\"Downloading PNML:\", pnml_url)\n",
        "    urllib.request.urlretrieve(pnml_url, pnml_path)\n",
        "else:\n",
        "    print(\"PNML already present:\", pnml_path)\n",
        "\n",
        "# ============================================================\n",
        "# 1) PNML PARSER -> (places, transitions, Bin, Bout, B)\n",
        "# ============================================================\n",
        "def _strip_ns(tag):\n",
        "    return tag.split(\"}\", 1)[-1] if \"}\" in tag else tag\n",
        "\n",
        "def parse_pnml_to_matrices(pnml_file):\n",
        "    tree = ET.parse(pnml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    places = []\n",
        "    transitions = []\n",
        "    arcs = []\n",
        "\n",
        "    for elem in root.iter():\n",
        "        tag = _strip_ns(elem.tag)\n",
        "        if tag == \"place\":\n",
        "            pid = elem.attrib.get(\"id\")\n",
        "            if pid:\n",
        "                places.append(pid)\n",
        "        elif tag == \"transition\":\n",
        "            tid = elem.attrib.get(\"id\")\n",
        "            if tid:\n",
        "                transitions.append(tid)\n",
        "        elif tag == \"arc\":\n",
        "            src = elem.attrib.get(\"source\")\n",
        "            tgt = elem.attrib.get(\"target\")\n",
        "            aid = elem.attrib.get(\"id\", \"\")\n",
        "            if src and tgt:\n",
        "                w = 1\n",
        "                for child in elem.iter():\n",
        "                    if _strip_ns(child.tag) == \"inscription\":\n",
        "                        for g in child.iter():\n",
        "                            if _strip_ns(g.tag) == \"text\" and g.text is not None:\n",
        "                                txt = g.text.strip()\n",
        "                                if txt:\n",
        "                                    try:\n",
        "                                        w = int(txt)\n",
        "                                    except:\n",
        "                                        pass\n",
        "                arcs.append((src, tgt, w, aid))\n",
        "\n",
        "    def uniq(seq):\n",
        "        seen = set()\n",
        "        out = []\n",
        "        for x in seq:\n",
        "            if x not in seen:\n",
        "                out.append(x)\n",
        "                seen.add(x)\n",
        "        return out\n",
        "\n",
        "    places = uniq([p for p in places if p is not None])\n",
        "    transitions = uniq([t for t in transitions if t is not None])\n",
        "\n",
        "    p_index = {p:i for i,p in enumerate(places)}\n",
        "    t_index = {t:i for i,t in enumerate(transitions)}\n",
        "\n",
        "    P = len(places)\n",
        "    T = len(transitions)\n",
        "\n",
        "    Bin = np.zeros((P, T), dtype=int)\n",
        "    Bout = np.zeros((P, T), dtype=int)\n",
        "\n",
        "    for src, tgt, w, _aid in arcs:\n",
        "        if src in p_index and tgt in t_index:\n",
        "            Bin[p_index[src], t_index[tgt]] += w\n",
        "        elif src in t_index and tgt in p_index:\n",
        "            Bout[p_index[tgt], t_index[src]] += w\n",
        "\n",
        "    B = Bout - Bin\n",
        "    return places, transitions, Bin, Bout, B\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "places, transitions, Bin, Bout, B = parse_pnml_to_matrices(pnml_path)\n",
        "t_parse = time.perf_counter() - t0\n",
        "\n",
        "P, T = Bin.shape\n",
        "num_arcs = int((Bin > 0).sum() + (Bout > 0).sum())\n",
        "\n",
        "print(\"\\nParsed PNML:\")\n",
        "print(\"  #places:\", P)\n",
        "print(\"  #transitions:\", T)\n",
        "print(\"  approx #nonzero arcs:\", num_arcs)\n",
        "print(\"  parse time (s):\", t_parse)\n",
        "\n",
        "# ============================================================\n",
        "# 2) BASELINE: LP per transition\n",
        "#    covered(t) if exists x>=0 s.t. Bx=0 and x_t >= 1\n",
        "# ============================================================\n",
        "def is_transition_covered_lp(B, t_idx, min_xt=1.0):\n",
        "    m, n = B.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B.astype(float)\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[t_idx] = (min_xt, None)\n",
        "    res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "    return res.success\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "baseline_flags = [is_transition_covered_lp(B, j, 1.0) for j in range(T)]\n",
        "t_baseline = time.perf_counter() - t0\n",
        "baseline_uncovered = [j for j, ok in enumerate(baseline_flags) if not ok]\n",
        "\n",
        "print(\"\\nBASELINE uncovered transitions (0-based):\", baseline_uncovered)\n",
        "print(\"Baseline time (s):\", t_baseline)\n",
        "\n",
        "# ============================================================\n",
        "# 3) Dense-invariant probe (global)\n",
        "#    Find x with Bx=0, x>=eps, sum(x)=1\n",
        "# ============================================================\n",
        "def dense_simplex_invariant(B_sub, eps):\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = np.vstack([B_sub, np.ones((1, n))])\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[-1] = 1.0\n",
        "    bounds = [(eps, None)] * n\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "eps = 1e-6\n",
        "res_dense = dense_simplex_invariant(B.astype(float), eps)\n",
        "\n",
        "print(\"\\nDense-invariant probe:\")\n",
        "print(\"  eps:\", eps)\n",
        "print(\"  success:\", res_dense.success)\n",
        "print(\"  status:\", res_dense.status, res_dense.message)\n",
        "if res_dense.success:\n",
        "    x = np.array(res_dense.x)\n",
        "    x[np.abs(x) < 1e-12] = 0.0\n",
        "    supp = [j for j, v in enumerate(x) if v > 0]\n",
        "    print(\"  support size:\", len(supp), \"of\", T)\n",
        "    print(\"  min(x):\", float(x.min()), \" max(x):\", float(x.max()))\n",
        "else:\n",
        "    print(\"  (No fully dense invariant at this eps.)\")\n",
        "\n",
        "# ============================================================\n",
        "# 4) GICT: Gate-0 SCC + Gate-2 per-SCC -dense LP + targeted fallback\n",
        "# ============================================================\n",
        "N = P + T\n",
        "def p_node(i): return i\n",
        "def t_node(j): return P + j\n",
        "\n",
        "adj = [[] for _ in range(N)]\n",
        "rev = [[] for _ in range(N)]\n",
        "\n",
        "for i in range(P):\n",
        "    for j in range(T):\n",
        "        if Bin[i, j] > 0:\n",
        "            a, b = p_node(i), t_node(j)\n",
        "            adj[a].append(b); rev[b].append(a)\n",
        "\n",
        "for i in range(P):\n",
        "    for j in range(T):\n",
        "        if Bout[i, j] > 0:\n",
        "            a, b = t_node(j), p_node(i)\n",
        "            adj[a].append(b); rev[b].append(a)\n",
        "\n",
        "def tarjan_scc(adj):\n",
        "    index = 0\n",
        "    stack = []\n",
        "    onstack = [False]*len(adj)\n",
        "    idx = [-1]*len(adj)\n",
        "    low = [0]*len(adj)\n",
        "    sccs = []\n",
        "\n",
        "    def strongconnect(v):\n",
        "        nonlocal index\n",
        "        idx[v] = index\n",
        "        low[v] = index\n",
        "        index += 1\n",
        "        stack.append(v)\n",
        "        onstack[v] = True\n",
        "\n",
        "        for w in adj[v]:\n",
        "            if idx[w] == -1:\n",
        "                strongconnect(w)\n",
        "                low[v] = min(low[v], low[w])\n",
        "            elif onstack[w]:\n",
        "                low[v] = min(low[v], idx[w])\n",
        "\n",
        "        if low[v] == idx[v]:\n",
        "            comp = []\n",
        "            while True:\n",
        "                w = stack.pop()\n",
        "                onstack[w] = False\n",
        "                comp.append(w)\n",
        "                if w == v:\n",
        "                    break\n",
        "            sccs.append(comp)\n",
        "\n",
        "    for v in range(len(adj)):\n",
        "        if idx[v] == -1:\n",
        "            strongconnect(v)\n",
        "    return sccs\n",
        "\n",
        "def scc_has_cycle(comp):\n",
        "    if len(comp) > 1:\n",
        "        return True\n",
        "    v = comp[0]\n",
        "    return v in adj[v]\n",
        "\n",
        "def scc_boundary_size(comp):\n",
        "    comp_set = set(comp)\n",
        "    out_edges = 0\n",
        "    in_edges = 0\n",
        "    for v in comp:\n",
        "        for w in adj[v]:\n",
        "            if w not in comp_set:\n",
        "                out_edges += 1\n",
        "        for u in rev[v]:\n",
        "            if u not in comp_set:\n",
        "                in_edges += 1\n",
        "    return in_edges, out_edges\n",
        "\n",
        "def support_of_witness(x, eps0=1e-12):\n",
        "    return [j for j, v in enumerate(x) if v > eps0]\n",
        "\n",
        "def lp_find_dense_invariant_for_scc(B_full, t_indices, eps_local):\n",
        "    B_sub = B_full[:, t_indices].astype(float)\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = np.vstack([B_sub, np.ones((1, n))])\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[-1] = 1.0\n",
        "    bounds = [(eps_local, None)] * n\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "def targeted_cover_lp(B_full, t_global, t_indices, min_xt=1.0):\n",
        "    pos = t_indices.index(t_global)\n",
        "    B_sub = B_full[:, t_indices].astype(float)\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B_sub\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[pos] = (min_xt, None)\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "fragile_k = 3\n",
        "bottleneck_boundary_k = 1\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "sccs = tarjan_scc(adj)\n",
        "t_scc = time.perf_counter() - t0\n",
        "\n",
        "scc_id = {}\n",
        "for k, comp in enumerate(sccs):\n",
        "    for v in comp:\n",
        "        scc_id[v] = k\n",
        "\n",
        "has_cycle = [scc_has_cycle(comp) for comp in sccs]\n",
        "\n",
        "labels = [\"Uncertain\"] * T\n",
        "witnesses = [None] * T\n",
        "\n",
        "# Gate-0\n",
        "t0 = time.perf_counter()\n",
        "for j in range(T):\n",
        "    sid = scc_id[t_node(j)]\n",
        "    if not has_cycle[sid]:\n",
        "        labels[j] = \"StructurallyUncovered\"\n",
        "t_gate0 = time.perf_counter() - t0\n",
        "\n",
        "# Gate-2 per-SCC -dense\n",
        "t0 = time.perf_counter()\n",
        "lp_calls_scc = 0\n",
        "\n",
        "for sid, comp in enumerate(sccs):\n",
        "    t_indices = [v - P for v in comp if v >= P]\n",
        "    if not t_indices:\n",
        "        continue\n",
        "\n",
        "    if not has_cycle[sid]:\n",
        "        for tj in t_indices:\n",
        "            labels[tj] = \"StructurallyUncovered\"\n",
        "        continue\n",
        "\n",
        "    inc, outc = scc_boundary_size(comp)\n",
        "\n",
        "    res = lp_find_dense_invariant_for_scc(B, t_indices, eps)\n",
        "    lp_calls_scc += 1\n",
        "\n",
        "    if not res.success:\n",
        "        # leave as Uncertain for targeted fallback\n",
        "        continue\n",
        "\n",
        "    x_sub = np.array(res.x)\n",
        "    x_sub[np.abs(x_sub) < 1e-12] = 0.0\n",
        "\n",
        "    full = np.zeros(T, dtype=float)\n",
        "    for k, tj in enumerate(t_indices):\n",
        "        full[tj] = x_sub[k]\n",
        "\n",
        "    supp = support_of_witness(full)\n",
        "    supp_size = len(supp)\n",
        "\n",
        "    for tj in t_indices:\n",
        "        if full[tj] <= 1e-12:\n",
        "            continue\n",
        "\n",
        "        lab = \"CoveredWitness\"\n",
        "        if supp_size <= fragile_k:\n",
        "            lab = \"FragileCovered\"\n",
        "        if inc <= bottleneck_boundary_k or outc <= bottleneck_boundary_k:\n",
        "            lab = \"BottleneckLimited\"\n",
        "\n",
        "        labels[tj] = lab\n",
        "        witnesses[tj] = full\n",
        "\n",
        "t_gate2_scc = time.perf_counter() - t0\n",
        "\n",
        "# Gate-2 targeted fallback (resolve Uncertain)\n",
        "t0 = time.perf_counter()\n",
        "lp_calls_targeted = 0\n",
        "\n",
        "for j in range(T):\n",
        "    if labels[j] != \"Uncertain\":\n",
        "        continue\n",
        "\n",
        "    sid = scc_id[t_node(j)]\n",
        "    comp = sccs[sid]\n",
        "    t_indices = [v - P for v in comp if v >= P]\n",
        "\n",
        "    if not has_cycle[sid]:\n",
        "        labels[j] = \"StructurallyUncovered\"\n",
        "        continue\n",
        "\n",
        "    res = targeted_cover_lp(B, j, t_indices, min_xt=1.0)\n",
        "    lp_calls_targeted += 1\n",
        "\n",
        "    if not res.success:\n",
        "        labels[j] = \"StructurallyUncovered\"\n",
        "    else:\n",
        "        labels[j] = \"CoveredWitness\"\n",
        "\n",
        "t_gate2_targeted = time.perf_counter() - t0\n",
        "\n",
        "gict_uncovered = [j for j, lab in enumerate(labels) if lab == \"StructurallyUncovered\"]\n",
        "\n",
        "print(\"\\nGICT uncovered transitions (0-based):\", gict_uncovered)\n",
        "\n",
        "counts = defaultdict(int)\n",
        "for lab in labels:\n",
        "    counts[lab] += 1\n",
        "print(\"\\nGICT label histogram:\")\n",
        "for k in sorted(counts):\n",
        "    print(f\"  {k}: {counts[k]}\")\n",
        "\n",
        "print(\"\\nGICT timings + LP counts:\")\n",
        "print(\"  SCC time (s):\", t_scc)\n",
        "print(\"  Gate-0 time (s):\", t_gate0)\n",
        "print(\"  Gate-2 per-SCC time (s):\", t_gate2_scc)\n",
        "print(\"  Gate-2 targeted time (s):\", t_gate2_targeted)\n",
        "print(\"  LP calls (per-SCC):\", lp_calls_scc)\n",
        "print(\"  LP calls (targeted):\", lp_calls_targeted)\n",
        "print(\"  Total LP calls:\", lp_calls_scc + lp_calls_targeted)\n",
        "\n",
        "print(\"\\nUncovered transition IDs (0-based index, id):\")\n",
        "for j in gict_uncovered:\n",
        "    print(\" \", j, transitions[j])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bOeGOuEn_o4",
        "outputId": "f33e3b66-2bd3-481d-aaeb-73026d52e4f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading PNML: https://www.pnml.org/version-2009/examples/Token-ring.pnml\n",
            "\n",
            "Parsed PNML:\n",
            "  #places: 18\n",
            "  #transitions: 15\n",
            "  approx #nonzero arcs: 67\n",
            "  parse time (s): 0.006331509000119695\n",
            "\n",
            "BASELINE uncovered transitions (0-based): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
            "Baseline time (s): 0.12982610599965483\n",
            "\n",
            "Dense-invariant probe:\n",
            "  eps: 1e-06\n",
            "  success: False\n",
            "  status: 2 The problem is infeasible. (HiGHS Status 8: model_status is Infeasible; primal_status is None)\n",
            "  (No fully dense invariant at this eps.)\n",
            "\n",
            "GICT uncovered transitions (0-based): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
            "\n",
            "GICT label histogram:\n",
            "  StructurallyUncovered: 15\n",
            "\n",
            "GICT timings + LP counts:\n",
            "  SCC time (s): 0.0002178119998461625\n",
            "  Gate-0 time (s): 0.00027244600005360553\n",
            "  Gate-2 per-SCC time (s): 0.005875397000181692\n",
            "  Gate-2 targeted time (s): 0.05052688100022351\n",
            "  LP calls (per-SCC): 1\n",
            "  LP calls (targeted): 15\n",
            "  Total LP calls: 16\n",
            "\n",
            "Uncovered transition IDs (0-based index, id):\n",
            "  0 cId301964552006875653980\n",
            "  1 cId300807812896203305081\n",
            "  2 cId300807812896203305012\n",
            "  3 cId300807812896203305013\n",
            "  4 cId301303558290705273077\n",
            "  5 cId301303558290705273023\n",
            "  6 cId301303558290705273071\n",
            "  7 cId301303558290705273022\n",
            "  8 cId301303558290705273021\n",
            "  9 cId301303558290705273024\n",
            "  10 cId301303558290705273055\n",
            "  11 cId301303558290705273011\n",
            "  12 cId301303558290705273067\n",
            "  13 cId301303558290705273076\n",
            "  14 cId301138309969036860220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "pnml_path = \"Token-ring.pnml\"\n",
        "\n",
        "def _strip_ns(tag):\n",
        "    return tag.split(\"}\", 1)[-1] if \"}\" in tag else tag\n",
        "\n",
        "def parse_pnml_to_matrices(pnml_file):\n",
        "    tree = ET.parse(pnml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    places = []\n",
        "    transitions = []\n",
        "    arcs = []\n",
        "\n",
        "    for elem in root.iter():\n",
        "        tag = _strip_ns(elem.tag)\n",
        "        if tag == \"place\":\n",
        "            pid = elem.attrib.get(\"id\")\n",
        "            if pid:\n",
        "                places.append(pid)\n",
        "        elif tag == \"transition\":\n",
        "            tid = elem.attrib.get(\"id\")\n",
        "            if tid:\n",
        "                transitions.append(tid)\n",
        "        elif tag == \"arc\":\n",
        "            src = elem.attrib.get(\"source\")\n",
        "            tgt = elem.attrib.get(\"target\")\n",
        "            if src and tgt:\n",
        "                w = 1\n",
        "                for child in elem.iter():\n",
        "                    if _strip_ns(child.tag) == \"inscription\":\n",
        "                        for g in child.iter():\n",
        "                            if _strip_ns(g.tag) == \"text\" and g.text is not None:\n",
        "                                txt = g.text.strip()\n",
        "                                if txt:\n",
        "                                    try:\n",
        "                                        w = int(txt)\n",
        "                                    except:\n",
        "                                        pass\n",
        "                arcs.append((src, tgt, w))\n",
        "\n",
        "    def uniq(seq):\n",
        "        seen = set()\n",
        "        out = []\n",
        "        for x in seq:\n",
        "            if x not in seen:\n",
        "                out.append(x)\n",
        "                seen.add(x)\n",
        "        return out\n",
        "\n",
        "    places = uniq([p for p in places if p is not None])\n",
        "    transitions = uniq([t for t in transitions if t is not None])\n",
        "\n",
        "    p_index = {p:i for i,p in enumerate(places)}\n",
        "    t_index = {t:i for i,t in enumerate(transitions)}\n",
        "\n",
        "    P = len(places)\n",
        "    T = len(transitions)\n",
        "\n",
        "    Bin = np.zeros((P, T), dtype=int)\n",
        "    Bout = np.zeros((P, T), dtype=int)\n",
        "\n",
        "    ignored = 0\n",
        "    for src, tgt, w in arcs:\n",
        "        if src in p_index and tgt in t_index:\n",
        "            Bin[p_index[src], t_index[tgt]] += w\n",
        "        elif src in t_index and tgt in p_index:\n",
        "            Bout[p_index[tgt], t_index[src]] += w\n",
        "        else:\n",
        "            ignored += 1\n",
        "\n",
        "    B = Bout - Bin\n",
        "    return places, transitions, arcs, ignored, Bin, Bout, B\n",
        "\n",
        "places, transitions, arcs, ignored, Bin, Bout, B = parse_pnml_to_matrices(pnml_path)\n",
        "P, T = Bin.shape\n",
        "\n",
        "print(\"Token-ring PNML sanity\")\n",
        "print(\"  P:\", P, \" T:\", T, \" arcs:\", len(arcs), \" ignored arcs:\", ignored)\n",
        "print(\"  nonzero Bin:\", int((Bin>0).sum()), \" nonzero Bout:\", int((Bout>0).sum()))\n",
        "\n",
        "# --------- Nullspace dimension over reals ----------\n",
        "# Nullspace dim = T - rank(B)\n",
        "rank = np.linalg.matrix_rank(B.astype(float))\n",
        "null_dim = T - rank\n",
        "print(\"\\nLinear algebra\")\n",
        "print(\"  rank(B):\", rank)\n",
        "print(\"  nullspace dimension (R):\", null_dim)\n",
        "\n",
        "# --------- Graph SCC / cycle check ----------\n",
        "N = P + T\n",
        "def p_node(i): return i\n",
        "def t_node(j): return P + j\n",
        "\n",
        "adj = [[] for _ in range(N)]\n",
        "rev = [[] for _ in range(N)]\n",
        "\n",
        "for i in range(P):\n",
        "    for j in range(T):\n",
        "        if Bin[i, j] > 0:\n",
        "            adj[p_node(i)].append(t_node(j))\n",
        "            rev[t_node(j)].append(p_node(i))\n",
        "\n",
        "for i in range(P):\n",
        "    for j in range(T):\n",
        "        if Bout[i, j] > 0:\n",
        "            adj[t_node(j)].append(p_node(i))\n",
        "            rev[p_node(i)].append(t_node(j))\n",
        "\n",
        "def tarjan_scc(adj):\n",
        "    index = 0\n",
        "    stack = []\n",
        "    onstack = [False]*len(adj)\n",
        "    idx = [-1]*len(adj)\n",
        "    low = [0]*len(adj)\n",
        "    sccs = []\n",
        "\n",
        "    def strongconnect(v):\n",
        "        nonlocal index\n",
        "        idx[v] = index\n",
        "        low[v] = index\n",
        "        index += 1\n",
        "        stack.append(v)\n",
        "        onstack[v] = True\n",
        "        for w in adj[v]:\n",
        "            if idx[w] == -1:\n",
        "                strongconnect(w)\n",
        "                low[v] = min(low[v], low[w])\n",
        "            elif onstack[w]:\n",
        "                low[v] = min(low[v], idx[w])\n",
        "        if low[v] == idx[v]:\n",
        "            comp = []\n",
        "            while True:\n",
        "                w = stack.pop()\n",
        "                onstack[w] = False\n",
        "                comp.append(w)\n",
        "                if w == v:\n",
        "                    break\n",
        "            sccs.append(comp)\n",
        "\n",
        "    for v in range(len(adj)):\n",
        "        if idx[v] == -1:\n",
        "            strongconnect(v)\n",
        "    return sccs\n",
        "\n",
        "def scc_has_cycle(comp):\n",
        "    if len(comp) > 1:\n",
        "        return True\n",
        "    v = comp[0]\n",
        "    return v in adj[v]\n",
        "\n",
        "sccs = tarjan_scc(adj)\n",
        "cycle_sccs = [c for c in sccs if scc_has_cycle(c)]\n",
        "\n",
        "print(\"\\nGraph structure\")\n",
        "print(\"  #SCCs:\", len(sccs))\n",
        "print(\"  #cycle SCCs:\", len(cycle_sccs))\n",
        "print(\"  largest SCC size:\", max(len(c) for c in sccs))\n",
        "\n",
        "# Per-transition: is its SCC cyclic?\n",
        "scc_id = {}\n",
        "for k, comp in enumerate(sccs):\n",
        "    for v in comp:\n",
        "        scc_id[v] = k\n",
        "cycle_flag = [scc_has_cycle(comp) for comp in sccs]\n",
        "\n",
        "print(\"\\nPer-transition SCC cyclicity:\")\n",
        "for j in range(T):\n",
        "    sid = scc_id[t_node(j)]\n",
        "    print(\" \", j, transitions[j], \"cycleSCC=\", cycle_flag[sid], \"SCCsize=\", len(sccs[sid]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhrLQvdpoW8-",
        "outputId": "cb3ac835-6757-4166-e13f-12f5807a9b8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token-ring PNML sanity\n",
            "  P: 18  T: 15  arcs: 67  ignored arcs: 0\n",
            "  nonzero Bin: 33  nonzero Bout: 34\n",
            "\n",
            "Linear algebra\n",
            "  rank(B): 12\n",
            "  nullspace dimension (R): 3\n",
            "\n",
            "Graph structure\n",
            "  #SCCs: 5\n",
            "  #cycle SCCs: 1\n",
            "  largest SCC size: 29\n",
            "\n",
            "Per-transition SCC cyclicity:\n",
            "  0 cId301964552006875653980 cycleSCC= True SCCsize= 29\n",
            "  1 cId300807812896203305081 cycleSCC= True SCCsize= 29\n",
            "  2 cId300807812896203305012 cycleSCC= True SCCsize= 29\n",
            "  3 cId300807812896203305013 cycleSCC= True SCCsize= 29\n",
            "  4 cId301303558290705273077 cycleSCC= True SCCsize= 29\n",
            "  5 cId301303558290705273023 cycleSCC= True SCCsize= 29\n",
            "  6 cId301303558290705273071 cycleSCC= True SCCsize= 29\n",
            "  7 cId301303558290705273022 cycleSCC= True SCCsize= 29\n",
            "  8 cId301303558290705273021 cycleSCC= True SCCsize= 29\n",
            "  9 cId301303558290705273024 cycleSCC= True SCCsize= 29\n",
            "  10 cId301303558290705273055 cycleSCC= True SCCsize= 29\n",
            "  11 cId301303558290705273011 cycleSCC= True SCCsize= 29\n",
            "  12 cId301303558290705273067 cycleSCC= True SCCsize= 29\n",
            "  13 cId301303558290705273076 cycleSCC= True SCCsize= 29\n",
            "  14 cId301138309969036860220 cycleSCC= True SCCsize= 29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import xml.etree.ElementTree as ET\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "pnml_path = \"Token-ring.pnml\"\n",
        "\n",
        "def _strip_ns(tag):\n",
        "    return tag.split(\"}\", 1)[-1] if \"}\" in tag else tag\n",
        "\n",
        "def parse_pnml_to_matrices(pnml_file):\n",
        "    tree = ET.parse(pnml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    places = []\n",
        "    transitions = []\n",
        "    arcs = []\n",
        "\n",
        "    for elem in root.iter():\n",
        "        tag = _strip_ns(elem.tag)\n",
        "        if tag == \"place\":\n",
        "            pid = elem.attrib.get(\"id\")\n",
        "            if pid:\n",
        "                places.append(pid)\n",
        "        elif tag == \"transition\":\n",
        "            tid = elem.attrib.get(\"id\")\n",
        "            if tid:\n",
        "                transitions.append(tid)\n",
        "        elif tag == \"arc\":\n",
        "            src = elem.attrib.get(\"source\")\n",
        "            tgt = elem.attrib.get(\"target\")\n",
        "            if src and tgt:\n",
        "                w = 1\n",
        "                for child in elem.iter():\n",
        "                    if _strip_ns(child.tag) == \"inscription\":\n",
        "                        for g in child.iter():\n",
        "                            if _strip_ns(g.tag) == \"text\" and g.text is not None:\n",
        "                                txt = g.text.strip()\n",
        "                                if txt:\n",
        "                                    try:\n",
        "                                        w = int(txt)\n",
        "                                    except:\n",
        "                                        pass\n",
        "                arcs.append((src, tgt, w))\n",
        "\n",
        "    def uniq(seq):\n",
        "        seen = set()\n",
        "        out = []\n",
        "        for x in seq:\n",
        "            if x not in seen:\n",
        "                out.append(x)\n",
        "                seen.add(x)\n",
        "        return out\n",
        "\n",
        "    places = uniq([p for p in places if p is not None])\n",
        "    transitions = uniq([t for t in transitions if t is not None])\n",
        "\n",
        "    p_index = {p:i for i,p in enumerate(places)}\n",
        "    t_index = {t:i for i,t in enumerate(transitions)}\n",
        "\n",
        "    P = len(places)\n",
        "    T = len(transitions)\n",
        "\n",
        "    Bin = np.zeros((P, T), dtype=int)\n",
        "    Bout = np.zeros((P, T), dtype=int)\n",
        "\n",
        "    for src, tgt, w in arcs:\n",
        "        if src in p_index and tgt in t_index:\n",
        "            Bin[p_index[src], t_index[tgt]] += w\n",
        "        elif src in t_index and tgt in p_index:\n",
        "            Bout[p_index[tgt], t_index[src]] += w\n",
        "\n",
        "    B = Bout - Bin\n",
        "    return places, transitions, Bin, Bout, B\n",
        "\n",
        "places, transitions, Bin, Bout, B = parse_pnml_to_matrices(pnml_path)\n",
        "P, T = Bin.shape\n",
        "\n",
        "print(\"Token-ring nonnegative T-invariant feasibility\")\n",
        "print(\"  P:\", P, \"T:\", T)\n",
        "\n",
        "# Feasibility: Bx=0, x>=0, sum(x)=1\n",
        "c = np.zeros(T)\n",
        "A_eq = np.vstack([B.astype(float), np.ones((1, T))])\n",
        "b_eq = np.zeros(P + 1)\n",
        "b_eq[-1] = 1.0\n",
        "bounds = [(0, None)] * T\n",
        "\n",
        "res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "print(\"\\nSimplex feasibility (Bx=0, x>=0, sum(x)=1):\")\n",
        "print(\"  success:\", res.success)\n",
        "print(\"  status:\", res.status, res.message)\n",
        "\n",
        "if res.success:\n",
        "    x = np.array(res.x)\n",
        "    x[np.abs(x) < 1e-12] = 0.0\n",
        "    supp = [j for j,v in enumerate(x) if v > 0]\n",
        "    print(\"  support size:\", len(supp), \"of\", T)\n",
        "    print(\"  support indices:\", supp)\n",
        "    print(\"  support IDs:\")\n",
        "    for j in supp:\n",
        "        print(\"   \", j, transitions[j], \"x=\", x[j])\n",
        "else:\n",
        "    print(\"  => No nonzero nonnegative T-invariant exists (cone is {0}).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eymxOeYUot05",
        "outputId": "1f24bc1c-af4d-4d1f-9c44-84f0ab1da498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token-ring nonnegative T-invariant feasibility\n",
            "  P: 18 T: 15\n",
            "\n",
            "Simplex feasibility (Bx=0, x>=0, sum(x)=1):\n",
            "  success: False\n",
            "  status: 2 The problem is infeasible. (HiGHS Status 8: model_status is Infeasible; primal_status is None)\n",
            "  => No nonzero nonnegative T-invariant exists (cone is {0}).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "# ============================================================\n",
        "# GICT Benchmark Runner (Cone-Empty short-circuit)\n",
        "#   - Downloads + parses PNML\n",
        "#   - Baseline: LP per transition (Bx=0, x>=0, x_t>=1)\n",
        "#   - GICT:\n",
        "#       Gate-0: acyclic SCC => StructurallyUncovered\n",
        "#       Gate-2 SCC-level:\n",
        "#           1) ConeEmpty test: (Bx=0, x>=0, sum(x)=1) on SCC transitions\n",
        "#              If infeasible => all in SCC are InvariantConeEmpty (treated uncovered)\n",
        "#           2) Else dense witness attempt: x>=eps, sum(x)=1\n",
        "#              If feasible => CoveredWitness/BottleneckLimited/FragileCovered\n",
        "#              If not feasible => leave Uncertain for targeted fallback\n",
        "#       Targeted fallback (only for Uncertain): per transition x_t>=1\n",
        "#   - Prints summary + comparison for each net\n",
        "# ============================================================\n",
        "\n",
        "# ----------------------------\n",
        "# PNML utilities\n",
        "# ----------------------------\n",
        "def _strip_ns(tag):\n",
        "    return tag.split(\"}\", 1)[-1] if \"}\" in tag else tag\n",
        "\n",
        "def parse_pnml_to_matrices(pnml_file):\n",
        "    tree = ET.parse(pnml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    places = []\n",
        "    transitions = []\n",
        "    arcs = []\n",
        "\n",
        "    for elem in root.iter():\n",
        "        tag = _strip_ns(elem.tag)\n",
        "        if tag == \"place\":\n",
        "            pid = elem.attrib.get(\"id\")\n",
        "            if pid:\n",
        "                places.append(pid)\n",
        "        elif tag == \"transition\":\n",
        "            tid = elem.attrib.get(\"id\")\n",
        "            if tid:\n",
        "                transitions.append(tid)\n",
        "        elif tag == \"arc\":\n",
        "            src = elem.attrib.get(\"source\")\n",
        "            tgt = elem.attrib.get(\"target\")\n",
        "            aid = elem.attrib.get(\"id\", \"\")\n",
        "            if src and tgt:\n",
        "                w = 1\n",
        "                for child in elem.iter():\n",
        "                    if _strip_ns(child.tag) == \"inscription\":\n",
        "                        for g in child.iter():\n",
        "                            if _strip_ns(g.tag) == \"text\" and g.text is not None:\n",
        "                                txt = g.text.strip()\n",
        "                                if txt:\n",
        "                                    try:\n",
        "                                        w = int(txt)\n",
        "                                    except:\n",
        "                                        pass\n",
        "                arcs.append((src, tgt, w, aid))\n",
        "\n",
        "    def uniq(seq):\n",
        "        seen = set()\n",
        "        out = []\n",
        "        for x in seq:\n",
        "            if x not in seen:\n",
        "                out.append(x)\n",
        "                seen.add(x)\n",
        "        return out\n",
        "\n",
        "    places = uniq([p for p in places if p is not None])\n",
        "    transitions = uniq([t for t in transitions if t is not None])\n",
        "\n",
        "    p_index = {p:i for i,p in enumerate(places)}\n",
        "    t_index = {t:i for i,t in enumerate(transitions)}\n",
        "\n",
        "    P = len(places)\n",
        "    T = len(transitions)\n",
        "\n",
        "    Bin = np.zeros((P, T), dtype=int)\n",
        "    Bout = np.zeros((P, T), dtype=int)\n",
        "\n",
        "    ignored = 0\n",
        "    for src, tgt, w, _aid in arcs:\n",
        "        if src in p_index and tgt in t_index:\n",
        "            Bin[p_index[src], t_index[tgt]] += w\n",
        "        elif src in t_index and tgt in p_index:\n",
        "            Bout[p_index[tgt], t_index[src]] += w\n",
        "        else:\n",
        "            ignored += 1\n",
        "\n",
        "    B = Bout - Bin\n",
        "    return places, transitions, arcs, ignored, Bin, Bout, B\n",
        "\n",
        "# ----------------------------\n",
        "# Graph / SCC utilities\n",
        "# ----------------------------\n",
        "def build_bipartite_graph(Bin, Bout):\n",
        "    P, T = Bin.shape\n",
        "    N = P + T\n",
        "\n",
        "    def p_node(i): return i\n",
        "    def t_node(j): return P + j\n",
        "\n",
        "    adj = [[] for _ in range(N)]\n",
        "    rev = [[] for _ in range(N)]\n",
        "\n",
        "    for i in range(P):\n",
        "        for j in range(T):\n",
        "            if Bin[i, j] > 0:\n",
        "                a, b = p_node(i), t_node(j)\n",
        "                adj[a].append(b); rev[b].append(a)\n",
        "\n",
        "    for i in range(P):\n",
        "        for j in range(T):\n",
        "            if Bout[i, j] > 0:\n",
        "                a, b = t_node(j), p_node(i)\n",
        "                adj[a].append(b); rev[b].append(a)\n",
        "\n",
        "    return adj, rev\n",
        "\n",
        "def tarjan_scc(adj):\n",
        "    index = 0\n",
        "    stack = []\n",
        "    onstack = [False]*len(adj)\n",
        "    idx = [-1]*len(adj)\n",
        "    low = [0]*len(adj)\n",
        "    sccs = []\n",
        "\n",
        "    def strongconnect(v):\n",
        "        nonlocal index\n",
        "        idx[v] = index\n",
        "        low[v] = index\n",
        "        index += 1\n",
        "        stack.append(v)\n",
        "        onstack[v] = True\n",
        "\n",
        "        for w in adj[v]:\n",
        "            if idx[w] == -1:\n",
        "                strongconnect(w)\n",
        "                low[v] = min(low[v], low[w])\n",
        "            elif onstack[w]:\n",
        "                low[v] = min(low[v], idx[w])\n",
        "\n",
        "        if low[v] == idx[v]:\n",
        "            comp = []\n",
        "            while True:\n",
        "                w = stack.pop()\n",
        "                onstack[w] = False\n",
        "                comp.append(w)\n",
        "                if w == v:\n",
        "                    break\n",
        "            sccs.append(comp)\n",
        "\n",
        "    for v in range(len(adj)):\n",
        "        if idx[v] == -1:\n",
        "            strongconnect(v)\n",
        "    return sccs\n",
        "\n",
        "def scc_has_cycle(comp, adj):\n",
        "    if len(comp) > 1:\n",
        "        return True\n",
        "    v = comp[0]\n",
        "    return v in adj[v]\n",
        "\n",
        "def scc_boundary_size(comp, adj, rev):\n",
        "    comp_set = set(comp)\n",
        "    out_edges = 0\n",
        "    in_edges = 0\n",
        "    for v in comp:\n",
        "        for w in adj[v]:\n",
        "            if w not in comp_set:\n",
        "                out_edges += 1\n",
        "        for u in rev[v]:\n",
        "            if u not in comp_set:\n",
        "                in_edges += 1\n",
        "    return in_edges, out_edges\n",
        "\n",
        "# ----------------------------\n",
        "# LP primitives\n",
        "# ----------------------------\n",
        "def baseline_transition_cover(B, t_idx, min_xt=1.0):\n",
        "    m, n = B.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B.astype(float)\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[t_idx] = (min_xt, None)\n",
        "    res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "    return res.success\n",
        "\n",
        "def cone_nonempty_simplex(B_sub):\n",
        "    # Feasibility: B_sub x = 0, x >= 0, sum(x)=1\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = np.vstack([B_sub, np.ones((1, n))])\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[-1] = 1.0\n",
        "    bounds = [(0, None)] * n\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "def dense_witness_simplex(B_sub, eps):\n",
        "    # Feasibility: B_sub x = 0, x >= eps, sum(x)=1\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = np.vstack([B_sub, np.ones((1, n))])\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[-1] = 1.0\n",
        "    bounds = [(eps, None)] * n\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "def targeted_cover_in_scc(B_full, t_global, t_indices, min_xt=1.0):\n",
        "    pos = t_indices.index(t_global)\n",
        "    B_sub = B_full[:, t_indices].astype(float)\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B_sub\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[pos] = (min_xt, None)\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers\n",
        "# ----------------------------\n",
        "def support_of_witness(x, eps0=1e-12):\n",
        "    return [j for j, v in enumerate(x) if v > eps0]\n",
        "\n",
        "def run_one_net(name, url, eps=1e-6, fragile_k=3, bottleneck_boundary_k=1):\n",
        "    # Download\n",
        "    filename = f\"{name}.pnml\"\n",
        "    if not os.path.exists(filename):\n",
        "        print(\"\\nDownloading PNML:\", url)\n",
        "        urllib.request.urlretrieve(url, filename)\n",
        "    else:\n",
        "        print(\"\\nPNML already present:\", filename)\n",
        "\n",
        "    # Parse\n",
        "    t0 = time.perf_counter()\n",
        "    places, transitions, arcs, ignored, Bin, Bout, B = parse_pnml_to_matrices(filename)\n",
        "    t_parse = time.perf_counter() - t0\n",
        "\n",
        "    P, T = Bin.shape\n",
        "    approx_arcs = int((Bin > 0).sum() + (Bout > 0).sum())\n",
        "\n",
        "    print(\"\\n==\", name, \"==\")\n",
        "    print(\"Parsed PNML:\")\n",
        "    print(\"  #places:\", P)\n",
        "    print(\"  #transitions:\", T)\n",
        "    print(\"  arcs in file:\", len(arcs), \" ignored arcs:\", ignored)\n",
        "    print(\"  approx #nonzero arcs:\", approx_arcs)\n",
        "    print(\"  parse time (s):\", t_parse)\n",
        "\n",
        "    # Baseline\n",
        "    t0 = time.perf_counter()\n",
        "    base_flags = [baseline_transition_cover(B, j, 1.0) for j in range(T)]\n",
        "    t_baseline = time.perf_counter() - t0\n",
        "    base_uncovered = [j for j, ok in enumerate(base_flags) if not ok]\n",
        "\n",
        "    print(\"\\nBASELINE uncovered transitions (0-based):\", base_uncovered)\n",
        "    print(\"Baseline time (s):\", t_baseline)\n",
        "\n",
        "    # Build SCCs\n",
        "    adj, rev = build_bipartite_graph(Bin, Bout)\n",
        "    N = P + T\n",
        "    def t_node(j): return P + j\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    sccs = tarjan_scc(adj)\n",
        "    t_scc = time.perf_counter() - t0\n",
        "\n",
        "    scc_id = {}\n",
        "    for k, comp in enumerate(sccs):\n",
        "        for v in comp:\n",
        "            scc_id[v] = k\n",
        "\n",
        "    has_cycle = [scc_has_cycle(comp, adj) for comp in sccs]\n",
        "\n",
        "    # GICT\n",
        "    labels = [\"Uncertain\"] * T\n",
        "    witnesses = [None] * T\n",
        "\n",
        "    # Gate-0: acyclic SCC => StructurallyUncovered\n",
        "    t0 = time.perf_counter()\n",
        "    for j in range(T):\n",
        "        sid = scc_id[t_node(j)]\n",
        "        if not has_cycle[sid]:\n",
        "            labels[j] = \"StructurallyUncovered\"\n",
        "    t_gate0 = time.perf_counter() - t0\n",
        "\n",
        "    # Gate-2 per-SCC with ConeEmpty short-circuit + dense witness\n",
        "    t0 = time.perf_counter()\n",
        "    lp_calls_scc = 0\n",
        "    coneempty_sccs = 0\n",
        "\n",
        "    for sid, comp in enumerate(sccs):\n",
        "        t_indices = [v - P for v in comp if v >= P]\n",
        "        if not t_indices:\n",
        "            continue\n",
        "\n",
        "        if not has_cycle[sid]:\n",
        "            for tj in t_indices:\n",
        "                labels[tj] = \"StructurallyUncovered\"\n",
        "            continue\n",
        "\n",
        "        # SCC boundary (for labels)\n",
        "        inc, outc = scc_boundary_size(comp, adj, rev)\n",
        "\n",
        "        # --- ConeEmpty short-circuit ---\n",
        "        B_sub = B[:, t_indices].astype(float)\n",
        "        res_cone = cone_nonempty_simplex(B_sub)\n",
        "        lp_calls_scc += 1\n",
        "\n",
        "        if not res_cone.success:\n",
        "            coneempty_sccs += 1\n",
        "            for tj in t_indices:\n",
        "                labels[tj] = \"InvariantConeEmpty\"\n",
        "            continue\n",
        "\n",
        "        # --- Dense witness attempt ---\n",
        "        res_dense = dense_witness_simplex(B_sub, eps)\n",
        "        lp_calls_scc += 1\n",
        "\n",
        "        if not res_dense.success:\n",
        "            # leave as Uncertain; targeted resolves\n",
        "            continue\n",
        "\n",
        "        x_sub = np.array(res_dense.x)\n",
        "        x_sub[np.abs(x_sub) < 1e-12] = 0.0\n",
        "\n",
        "        full = np.zeros(T, dtype=float)\n",
        "        for k, tj in enumerate(t_indices):\n",
        "            full[tj] = x_sub[k]\n",
        "\n",
        "        supp = support_of_witness(full)\n",
        "        supp_size = len(supp)\n",
        "\n",
        "        for tj in t_indices:\n",
        "            if full[tj] <= 1e-12:\n",
        "                continue\n",
        "\n",
        "            lab = \"CoveredWitness\"\n",
        "            if supp_size <= fragile_k:\n",
        "                lab = \"FragileCovered\"\n",
        "            if inc <= bottleneck_boundary_k or outc <= bottleneck_boundary_k:\n",
        "                lab = \"BottleneckLimited\"\n",
        "\n",
        "            labels[tj] = lab\n",
        "            witnesses[tj] = full\n",
        "\n",
        "    t_gate2_scc = time.perf_counter() - t0\n",
        "\n",
        "    # Targeted fallback only for Uncertain\n",
        "    t0 = time.perf_counter()\n",
        "    lp_calls_targeted = 0\n",
        "\n",
        "    for j in range(T):\n",
        "        if labels[j] != \"Uncertain\":\n",
        "            continue\n",
        "\n",
        "        sid = scc_id[t_node(j)]\n",
        "        comp = sccs[sid]\n",
        "        t_indices = [v - P for v in comp if v >= P]\n",
        "\n",
        "        if not has_cycle[sid]:\n",
        "            labels[j] = \"StructurallyUncovered\"\n",
        "            continue\n",
        "\n",
        "        res = targeted_cover_in_scc(B, j, t_indices, min_xt=1.0)\n",
        "        lp_calls_targeted += 1\n",
        "\n",
        "        if not res.success:\n",
        "            labels[j] = \"StructurallyUncovered\"\n",
        "        else:\n",
        "            labels[j] = \"CoveredWitness\"\n",
        "\n",
        "    t_gate2_targeted = time.perf_counter() - t0\n",
        "\n",
        "    # Final uncovered set (treat InvariantConeEmpty as uncovered for reporting)\n",
        "    gict_uncovered = [j for j, lab in enumerate(labels) if lab in (\"StructurallyUncovered\", \"InvariantConeEmpty\")]\n",
        "\n",
        "    # Histogram\n",
        "    counts = defaultdict(int)\n",
        "    for lab in labels:\n",
        "        counts[lab] += 1\n",
        "\n",
        "    print(\"\\nGICT uncovered transitions (0-based):\", gict_uncovered)\n",
        "    print(\"\\nGICT label histogram:\")\n",
        "    for k in sorted(counts):\n",
        "        print(f\"  {k}: {counts[k]}\")\n",
        "\n",
        "    print(\"\\nGICT timings + LP counts:\")\n",
        "    print(\"  SCC time (s):\", t_scc)\n",
        "    print(\"  Gate-0 time (s):\", t_gate0)\n",
        "    print(\"  Gate-2 per-SCC time (s):\", t_gate2_scc)\n",
        "    print(\"  Gate-2 targeted time (s):\", t_gate2_targeted)\n",
        "    print(\"  LP calls (per-SCC):\", lp_calls_scc)\n",
        "    print(\"  LP calls (targeted):\", lp_calls_targeted)\n",
        "    print(\"  Total LP calls:\", lp_calls_scc + lp_calls_targeted)\n",
        "    print(\"  ConeEmpty SCCs:\", coneempty_sccs)\n",
        "\n",
        "    return {\n",
        "        \"name\": name,\n",
        "        \"P\": P,\n",
        "        \"T\": T,\n",
        "        \"arcs\": len(arcs),\n",
        "        \"baseline_uncovered\": len(base_uncovered),\n",
        "        \"baseline_time_s\": t_baseline,\n",
        "        \"gict_uncovered\": len(gict_uncovered),\n",
        "        \"gict_total_lp\": (lp_calls_scc + lp_calls_targeted),\n",
        "        \"gict_lp_scc\": lp_calls_scc,\n",
        "        \"gict_lp_targeted\": lp_calls_targeted,\n",
        "        \"gict_time_gate0_s\": t_gate0,\n",
        "        \"gict_time_gate2_scc_s\": t_gate2_scc,\n",
        "        \"gict_time_gate2_targeted_s\": t_gate2_targeted,\n",
        "        \"labels\": dict(counts),\n",
        "        \"coneempty_sccs\": coneempty_sccs,\n",
        "    }\n",
        "\n",
        "def print_comparison_table(results):\n",
        "    print(\"\\n============================\")\n",
        "    print(\"COMPARISON SUMMARY\")\n",
        "    print(\"============================\")\n",
        "    header = (\n",
        "        \"Net | P | T | arcs | Baseline uncovered | Baseline time(s) | \"\n",
        "        \"GICT uncovered | GICT total LP | (SCC,Targeted) | ConeEmpty SCCs\"\n",
        "    )\n",
        "    print(header)\n",
        "    print(\"-\"*len(header))\n",
        "    for r in results:\n",
        "        print(\n",
        "            f\"{r['name']} | {r['P']} | {r['T']} | {r['arcs']} | \"\n",
        "            f\"{r['baseline_uncovered']} | {r['baseline_time_s']:.6f} | \"\n",
        "            f\"{r['gict_uncovered']} | {r['gict_total_lp']} | \"\n",
        "            f\"({r['gict_lp_scc']},{r['gict_lp_targeted']}) | {r['coneempty_sccs']}\"\n",
        "        )\n",
        "    print(\"\\nLabel histograms:\")\n",
        "    for r in results:\n",
        "        print(f\"  {r['name']}: {r['labels']}\")\n",
        "\n",
        "# ============================================================\n",
        "# RUN BOTH NETS BACK-TO-BACK\n",
        "# ============================================================\n",
        "results = []\n",
        "\n",
        "# philo\n",
        "results.append(run_one_net(\n",
        "    name=\"philo\",\n",
        "    url=\"https://www.pnml.org/version-2009/examples/philo.pnml\",\n",
        "    eps=1e-6\n",
        "))\n",
        "\n",
        "# token-ring\n",
        "results.append(run_one_net(\n",
        "    name=\"Token-ring\",\n",
        "    url=\"https://www.pnml.org/version-2009/examples/Token-ring.pnml\",\n",
        "    eps=1e-6\n",
        "))\n",
        "\n",
        "print_comparison_table(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epDRlRgNpvpp",
        "outputId": "4bd4bfd4-953b-4dd0-9619-b2d7cca9c16f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PNML already present: philo.pnml\n",
            "\n",
            "== philo ==\n",
            "Parsed PNML:\n",
            "  #places: 30\n",
            "  #transitions: 30\n",
            "  arcs in file: 96  ignored arcs: 0\n",
            "  approx #nonzero arcs: 96\n",
            "  parse time (s): 0.019517864000135887\n",
            "\n",
            "BASELINE uncovered transitions (0-based): []\n",
            "Baseline time (s): 0.5313419899998735\n",
            "\n",
            "GICT uncovered transitions (0-based): []\n",
            "\n",
            "GICT label histogram:\n",
            "  BottleneckLimited: 30\n",
            "\n",
            "GICT timings + LP counts:\n",
            "  SCC time (s): 9.08870001694595e-05\n",
            "  Gate-0 time (s): 1.2670000160142081e-05\n",
            "  Gate-2 per-SCC time (s): 0.033477883000159636\n",
            "  Gate-2 targeted time (s): 6.097000095905969e-06\n",
            "  LP calls (per-SCC): 2\n",
            "  LP calls (targeted): 0\n",
            "  Total LP calls: 2\n",
            "  ConeEmpty SCCs: 0\n",
            "\n",
            "PNML already present: Token-ring.pnml\n",
            "\n",
            "== Token-ring ==\n",
            "Parsed PNML:\n",
            "  #places: 18\n",
            "  #transitions: 15\n",
            "  arcs in file: 67  ignored arcs: 0\n",
            "  approx #nonzero arcs: 67\n",
            "  parse time (s): 0.005835731999923155\n",
            "\n",
            "BASELINE uncovered transitions (0-based): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
            "Baseline time (s): 0.08711666299996068\n",
            "\n",
            "GICT uncovered transitions (0-based): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
            "\n",
            "GICT label histogram:\n",
            "  InvariantConeEmpty: 15\n",
            "\n",
            "GICT timings + LP counts:\n",
            "  SCC time (s): 5.6910999774117954e-05\n",
            "  Gate-0 time (s): 4.63700007458101e-06\n",
            "  Gate-2 per-SCC time (s): 0.010716674999912357\n",
            "  Gate-2 targeted time (s): 4.277000243746443e-06\n",
            "  LP calls (per-SCC): 1\n",
            "  LP calls (targeted): 0\n",
            "  Total LP calls: 1\n",
            "  ConeEmpty SCCs: 1\n",
            "\n",
            "============================\n",
            "COMPARISON SUMMARY\n",
            "============================\n",
            "Net | P | T | arcs | Baseline uncovered | Baseline time(s) | GICT uncovered | GICT total LP | (SCC,Targeted) | ConeEmpty SCCs\n",
            "-----------------------------------------------------------------------------------------------------------------------------\n",
            "philo | 30 | 30 | 96 | 0 | 0.531342 | 0 | 2 | (2,0) | 0\n",
            "Token-ring | 18 | 15 | 67 | 15 | 0.087117 | 15 | 1 | (1,0) | 1\n",
            "\n",
            "Label histograms:\n",
            "  philo: {'BottleneckLimited': 30}\n",
            "  Token-ring: {'InvariantConeEmpty': 15}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "# ============================================================\n",
        "# 0) DOWNLOAD PNML (Piscine)\n",
        "# ============================================================\n",
        "pnml_url = \"https://www.pnml.org/version-2009/examples/Piscine.pnml\"\n",
        "pnml_path = \"Piscine.pnml\"\n",
        "\n",
        "if not os.path.exists(pnml_path):\n",
        "    print(\"Downloading PNML:\", pnml_url)\n",
        "    urllib.request.urlretrieve(pnml_url, pnml_path)\n",
        "else:\n",
        "    print(\"PNML already present:\", pnml_path)\n",
        "\n",
        "# ============================================================\n",
        "# 1) PNML PARSER -> (places, transitions, Bin, Bout, B)\n",
        "# ============================================================\n",
        "def _strip_ns(tag):\n",
        "    return tag.split(\"}\", 1)[-1] if \"}\" in tag else tag\n",
        "\n",
        "def parse_pnml_to_matrices(pnml_file):\n",
        "    tree = ET.parse(pnml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    places = []\n",
        "    transitions = []\n",
        "    arcs = []\n",
        "\n",
        "    for elem in root.iter():\n",
        "        tag = _strip_ns(elem.tag)\n",
        "        if tag == \"place\":\n",
        "            pid = elem.attrib.get(\"id\")\n",
        "            if pid:\n",
        "                places.append(pid)\n",
        "        elif tag == \"transition\":\n",
        "            tid = elem.attrib.get(\"id\")\n",
        "            if tid:\n",
        "                transitions.append(tid)\n",
        "        elif tag == \"arc\":\n",
        "            src = elem.attrib.get(\"source\")\n",
        "            tgt = elem.attrib.get(\"target\")\n",
        "            aid = elem.attrib.get(\"id\", \"\")\n",
        "            if src and tgt:\n",
        "                w = 1\n",
        "                for child in elem.iter():\n",
        "                    if _strip_ns(child.tag) == \"inscription\":\n",
        "                        for g in child.iter():\n",
        "                            if _strip_ns(g.tag) == \"text\" and g.text is not None:\n",
        "                                txt = g.text.strip()\n",
        "                                if txt:\n",
        "                                    try:\n",
        "                                        w = int(txt)\n",
        "                                    except:\n",
        "                                        pass\n",
        "                arcs.append((src, tgt, w, aid))\n",
        "\n",
        "    def uniq(seq):\n",
        "        seen = set()\n",
        "        out = []\n",
        "        for x in seq:\n",
        "            if x not in seen:\n",
        "                out.append(x)\n",
        "                seen.add(x)\n",
        "        return out\n",
        "\n",
        "    places = uniq([p for p in places if p is not None])\n",
        "    transitions = uniq([t for t in transitions if t is not None])\n",
        "\n",
        "    p_index = {p:i for i,p in enumerate(places)}\n",
        "    t_index = {t:i for i,t in enumerate(transitions)}\n",
        "\n",
        "    P = len(places)\n",
        "    T = len(transitions)\n",
        "\n",
        "    Bin = np.zeros((P, T), dtype=int)\n",
        "    Bout = np.zeros((P, T), dtype=int)\n",
        "\n",
        "    ignored = 0\n",
        "    for src, tgt, w, _aid in arcs:\n",
        "        if src in p_index and tgt in t_index:\n",
        "            Bin[p_index[src], t_index[tgt]] += w\n",
        "        elif src in t_index and tgt in p_index:\n",
        "            Bout[p_index[tgt], t_index[src]] += w\n",
        "        else:\n",
        "            ignored += 1\n",
        "\n",
        "    B = Bout - Bin\n",
        "    return places, transitions, arcs, ignored, Bin, Bout, B\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "places, transitions, arcs, ignored, Bin, Bout, B = parse_pnml_to_matrices(pnml_path)\n",
        "t_parse = time.perf_counter() - t0\n",
        "\n",
        "P, T = Bin.shape\n",
        "num_arcs = int((Bin > 0).sum() + (Bout > 0).sum())\n",
        "\n",
        "print(\"\\nParsed PNML:\")\n",
        "print(\"  #places:\", P)\n",
        "print(\"  #transitions:\", T)\n",
        "print(\"  arcs in file:\", len(arcs), \" ignored arcs:\", ignored)\n",
        "print(\"  approx #nonzero arcs:\", num_arcs)\n",
        "print(\"  parse time (s):\", t_parse)\n",
        "\n",
        "# ============================================================\n",
        "# 2) BASELINE: LP per transition (Bx=0, x>=0, x_t>=1)\n",
        "# ============================================================\n",
        "def baseline_transition_cover(B, t_idx, min_xt=1.0):\n",
        "    m, n = B.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B.astype(float)\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[t_idx] = (min_xt, None)\n",
        "    res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "    return res.success\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "base_flags = [baseline_transition_cover(B, j, 1.0) for j in range(T)]\n",
        "t_baseline = time.perf_counter() - t0\n",
        "base_uncovered = [j for j, ok in enumerate(base_flags) if not ok]\n",
        "\n",
        "print(\"\\nBASELINE uncovered transitions (0-based):\", base_uncovered)\n",
        "print(\"Baseline time (s):\", t_baseline)\n",
        "\n",
        "# ============================================================\n",
        "# 3) SCC utilities\n",
        "# ============================================================\n",
        "def build_bipartite_graph(Bin, Bout):\n",
        "    P, T = Bin.shape\n",
        "    N = P + T\n",
        "\n",
        "    def p_node(i): return i\n",
        "    def t_node(j): return P + j\n",
        "\n",
        "    adj = [[] for _ in range(N)]\n",
        "    rev = [[] for _ in range(N)]\n",
        "\n",
        "    for i in range(P):\n",
        "        for j in range(T):\n",
        "            if Bin[i, j] > 0:\n",
        "                a, b = p_node(i), t_node(j)\n",
        "                adj[a].append(b); rev[b].append(a)\n",
        "\n",
        "    for i in range(P):\n",
        "        for j in range(T):\n",
        "            if Bout[i, j] > 0:\n",
        "                a, b = t_node(j), p_node(i)\n",
        "                adj[a].append(b); rev[b].append(a)\n",
        "\n",
        "    return adj, rev\n",
        "\n",
        "def tarjan_scc(adj):\n",
        "    index = 0\n",
        "    stack = []\n",
        "    onstack = [False]*len(adj)\n",
        "    idx = [-1]*len(adj)\n",
        "    low = [0]*len(adj)\n",
        "    sccs = []\n",
        "\n",
        "    def strongconnect(v):\n",
        "        nonlocal index\n",
        "        idx[v] = index\n",
        "        low[v] = index\n",
        "        index += 1\n",
        "        stack.append(v)\n",
        "        onstack[v] = True\n",
        "\n",
        "        for w in adj[v]:\n",
        "            if idx[w] == -1:\n",
        "                strongconnect(w)\n",
        "                low[v] = min(low[v], low[w])\n",
        "            elif onstack[w]:\n",
        "                low[v] = min(low[v], idx[w])\n",
        "\n",
        "        if low[v] == idx[v]:\n",
        "            comp = []\n",
        "            while True:\n",
        "                w = stack.pop()\n",
        "                onstack[w] = False\n",
        "                comp.append(w)\n",
        "                if w == v:\n",
        "                    break\n",
        "            sccs.append(comp)\n",
        "\n",
        "    for v in range(len(adj)):\n",
        "        if idx[v] == -1:\n",
        "            strongconnect(v)\n",
        "    return sccs\n",
        "\n",
        "def scc_has_cycle(comp, adj):\n",
        "    if len(comp) > 1:\n",
        "        return True\n",
        "    v = comp[0]\n",
        "    return v in adj[v]\n",
        "\n",
        "def scc_boundary_size(comp, adj, rev):\n",
        "    comp_set = set(comp)\n",
        "    out_edges = 0\n",
        "    in_edges = 0\n",
        "    for v in comp:\n",
        "        for w in adj[v]:\n",
        "            if w not in comp_set:\n",
        "                out_edges += 1\n",
        "        for u in rev[v]:\n",
        "            if u not in comp_set:\n",
        "                in_edges += 1\n",
        "    return in_edges, out_edges\n",
        "\n",
        "# ============================================================\n",
        "# 4) LP primitives for GICT\n",
        "# ============================================================\n",
        "def cone_nonempty_simplex(B_sub):\n",
        "    # Feasibility: B_sub x = 0, x >= 0, sum(x)=1\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = np.vstack([B_sub, np.ones((1, n))])\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[-1] = 1.0\n",
        "    bounds = [(0, None)] * n\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "def dense_witness_simplex(B_sub, eps):\n",
        "    # Feasibility: B_sub x = 0, x >= eps, sum(x)=1\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = np.vstack([B_sub, np.ones((1, n))])\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[-1] = 1.0\n",
        "    bounds = [(eps, None)] * n\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "def targeted_cover_in_scc(B_full, t_global, t_indices, min_xt=1.0):\n",
        "    pos = t_indices.index(t_global)\n",
        "    B_sub = B_full[:, t_indices].astype(float)\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B_sub\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[pos] = (min_xt, None)\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "def support_of_witness(x, eps0=1e-12):\n",
        "    return [j for j, v in enumerate(x) if v > eps0]\n",
        "\n",
        "# ============================================================\n",
        "# 5) GICT with ConeEmpty short-circuit\n",
        "# ============================================================\n",
        "eps = 1e-6\n",
        "fragile_k = 3\n",
        "bottleneck_boundary_k = 1\n",
        "\n",
        "adj, rev = build_bipartite_graph(Bin, Bout)\n",
        "N = P + T\n",
        "\n",
        "def t_node(j): return P + j\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "sccs = tarjan_scc(adj)\n",
        "t_scc = time.perf_counter() - t0\n",
        "\n",
        "scc_id = {}\n",
        "for k, comp in enumerate(sccs):\n",
        "    for v in comp:\n",
        "        scc_id[v] = k\n",
        "has_cycle = [scc_has_cycle(comp, adj) for comp in sccs]\n",
        "\n",
        "labels = [\"Uncertain\"] * T\n",
        "witnesses = [None] * T\n",
        "\n",
        "# Gate-0\n",
        "t0 = time.perf_counter()\n",
        "for j in range(T):\n",
        "    sid = scc_id[t_node(j)]\n",
        "    if not has_cycle[sid]:\n",
        "        labels[j] = \"StructurallyUncovered\"\n",
        "t_gate0 = time.perf_counter() - t0\n",
        "\n",
        "# Gate-2 per-SCC\n",
        "t0 = time.perf_counter()\n",
        "lp_calls_scc = 0\n",
        "coneempty_sccs = 0\n",
        "\n",
        "for sid, comp in enumerate(sccs):\n",
        "    t_indices = [v - P for v in comp if v >= P]\n",
        "    if not t_indices:\n",
        "        continue\n",
        "\n",
        "    if not has_cycle[sid]:\n",
        "        for tj in t_indices:\n",
        "            labels[tj] = \"StructurallyUncovered\"\n",
        "        continue\n",
        "\n",
        "    inc, outc = scc_boundary_size(comp, adj, rev)\n",
        "\n",
        "    B_sub = B[:, t_indices].astype(float)\n",
        "\n",
        "    # ConeEmpty test\n",
        "    res_cone = cone_nonempty_simplex(B_sub)\n",
        "    lp_calls_scc += 1\n",
        "    if not res_cone.success:\n",
        "        coneempty_sccs += 1\n",
        "        for tj in t_indices:\n",
        "            labels[tj] = \"InvariantConeEmpty\"\n",
        "        continue\n",
        "\n",
        "    # Dense witness attempt\n",
        "    res_dense = dense_witness_simplex(B_sub, eps)\n",
        "    lp_calls_scc += 1\n",
        "    if not res_dense.success:\n",
        "        continue\n",
        "\n",
        "    x_sub = np.array(res_dense.x)\n",
        "    x_sub[np.abs(x_sub) < 1e-12] = 0.0\n",
        "\n",
        "    full = np.zeros(T, dtype=float)\n",
        "    for k, tj in enumerate(t_indices):\n",
        "        full[tj] = x_sub[k]\n",
        "\n",
        "    supp = support_of_witness(full)\n",
        "    supp_size = len(supp)\n",
        "\n",
        "    for tj in t_indices:\n",
        "        if full[tj] <= 1e-12:\n",
        "            continue\n",
        "\n",
        "        lab = \"CoveredWitness\"\n",
        "        if supp_size <= fragile_k:\n",
        "            lab = \"FragileCovered\"\n",
        "        if inc <= bottleneck_boundary_k or outc <= bottleneck_boundary_k:\n",
        "            lab = \"BottleneckLimited\"\n",
        "\n",
        "        labels[tj] = lab\n",
        "        witnesses[tj] = full\n",
        "\n",
        "t_gate2_scc = time.perf_counter() - t0\n",
        "\n",
        "# Targeted fallback\n",
        "t0 = time.perf_counter()\n",
        "lp_calls_targeted = 0\n",
        "\n",
        "for j in range(T):\n",
        "    if labels[j] != \"Uncertain\":\n",
        "        continue\n",
        "\n",
        "    sid = scc_id[t_node(j)]\n",
        "    comp = sccs[sid]\n",
        "    t_indices = [v - P for v in comp if v >= P]\n",
        "\n",
        "    if not has_cycle[sid]:\n",
        "        labels[j] = \"StructurallyUncovered\"\n",
        "        continue\n",
        "\n",
        "    res = targeted_cover_in_scc(B, j, t_indices, min_xt=1.0)\n",
        "    lp_calls_targeted += 1\n",
        "\n",
        "    if not res.success:\n",
        "        labels[j] = \"StructurallyUncovered\"\n",
        "    else:\n",
        "        labels[j] = \"CoveredWitness\"\n",
        "\n",
        "t_gate2_targeted = time.perf_counter() - t0\n",
        "\n",
        "gict_uncovered = [j for j, lab in enumerate(labels) if lab in (\"StructurallyUncovered\", \"InvariantConeEmpty\")]\n",
        "\n",
        "print(\"\\nGICT uncovered transitions (0-based):\", gict_uncovered)\n",
        "\n",
        "counts = defaultdict(int)\n",
        "for lab in labels:\n",
        "    counts[lab] += 1\n",
        "print(\"\\nGICT label histogram:\")\n",
        "for k in sorted(counts):\n",
        "    print(f\"  {k}: {counts[k]}\")\n",
        "\n",
        "print(\"\\nGICT timings + LP counts:\")\n",
        "print(\"  SCC time (s):\", t_scc)\n",
        "print(\"  Gate-0 time (s):\", t_gate0)\n",
        "print(\"  Gate-2 per-SCC time (s):\", t_gate2_scc)\n",
        "print(\"  Gate-2 targeted time (s):\", t_gate2_targeted)\n",
        "print(\"  LP calls (per-SCC):\", lp_calls_scc)\n",
        "print(\"  LP calls (targeted):\", lp_calls_targeted)\n",
        "print(\"  Total LP calls:\", lp_calls_scc + lp_calls_targeted)\n",
        "print(\"  ConeEmpty SCCs:\", coneempty_sccs)\n",
        "\n",
        "print(\"\\nUncovered transition IDs (0-based index, id):\")\n",
        "for j in gict_uncovered:\n",
        "    print(\" \", j, transitions[j])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3JLTKDgqrgy",
        "outputId": "0fe6b468-d852-4f8a-c168-a689e6275e51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading PNML: https://www.pnml.org/version-2009/examples/Piscine.pnml\n",
            "\n",
            "Parsed PNML:\n",
            "  #places: 9\n",
            "  #transitions: 7\n",
            "  arcs in file: 20  ignored arcs: 0\n",
            "  approx #nonzero arcs: 20\n",
            "  parse time (s): 0.0053992530001778505\n",
            "\n",
            "BASELINE uncovered transitions (0-based): []\n",
            "Baseline time (s): 0.05929801599995699\n",
            "\n",
            "GICT uncovered transitions (0-based): []\n",
            "\n",
            "GICT label histogram:\n",
            "  BottleneckLimited: 7\n",
            "\n",
            "GICT timings + LP counts:\n",
            "  SCC time (s): 0.00017155200021079509\n",
            "  Gate-0 time (s): 0.00017429499985155417\n",
            "  Gate-2 per-SCC time (s): 0.028100364000238187\n",
            "  Gate-2 targeted time (s): 0.0005361649991755257\n",
            "  LP calls (per-SCC): 2\n",
            "  LP calls (targeted): 0\n",
            "  Total LP calls: 2\n",
            "  ConeEmpty SCCs: 0\n",
            "\n",
            "Uncovered transition IDs (0-based index, id):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "# ============================================================\n",
        "# PNML parsing\n",
        "# ============================================================\n",
        "def _strip_ns(tag):\n",
        "    return tag.split(\"}\", 1)[-1] if \"}\" in tag else tag\n",
        "\n",
        "def parse_pnml_to_matrices(pnml_file):\n",
        "    tree = ET.parse(pnml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    places = []\n",
        "    transitions = []\n",
        "    arcs = []\n",
        "\n",
        "    for elem in root.iter():\n",
        "        tag = _strip_ns(elem.tag)\n",
        "        if tag == \"place\":\n",
        "            pid = elem.attrib.get(\"id\")\n",
        "            if pid:\n",
        "                places.append(pid)\n",
        "        elif tag == \"transition\":\n",
        "            tid = elem.attrib.get(\"id\")\n",
        "            if tid:\n",
        "                transitions.append(tid)\n",
        "        elif tag == \"arc\":\n",
        "            src = elem.attrib.get(\"source\")\n",
        "            tgt = elem.attrib.get(\"target\")\n",
        "            aid = elem.attrib.get(\"id\", \"\")\n",
        "            if src and tgt:\n",
        "                w = 1\n",
        "                for child in elem.iter():\n",
        "                    if _strip_ns(child.tag) == \"inscription\":\n",
        "                        for g in child.iter():\n",
        "                            if _strip_ns(g.tag) == \"text\" and g.text is not None:\n",
        "                                txt = g.text.strip()\n",
        "                                if txt:\n",
        "                                    try:\n",
        "                                        w = int(txt)\n",
        "                                    except:\n",
        "                                        pass\n",
        "                arcs.append((src, tgt, w, aid))\n",
        "\n",
        "    def uniq(seq):\n",
        "        seen = set()\n",
        "        out = []\n",
        "        for x in seq:\n",
        "            if x not in seen:\n",
        "                out.append(x)\n",
        "                seen.add(x)\n",
        "        return out\n",
        "\n",
        "    places = uniq([p for p in places if p is not None])\n",
        "    transitions = uniq([t for t in transitions if t is not None])\n",
        "\n",
        "    p_index = {p:i for i,p in enumerate(places)}\n",
        "    t_index = {t:i for i,t in enumerate(transitions)}\n",
        "\n",
        "    P = len(places)\n",
        "    T = len(transitions)\n",
        "\n",
        "    Bin = np.zeros((P, T), dtype=int)\n",
        "    Bout = np.zeros((P, T), dtype=int)\n",
        "\n",
        "    ignored = 0\n",
        "    for src, tgt, w, _aid in arcs:\n",
        "        if src in p_index and tgt in t_index:\n",
        "            Bin[p_index[src], t_index[tgt]] += w\n",
        "        elif src in t_index and tgt in p_index:\n",
        "            Bout[p_index[tgt], t_index[src]] += w\n",
        "        else:\n",
        "            ignored += 1\n",
        "\n",
        "    B = Bout - Bin\n",
        "    return places, transitions, arcs, ignored, Bin, Bout, B\n",
        "\n",
        "# ============================================================\n",
        "# Graph / SCC (Tarjan)\n",
        "# ============================================================\n",
        "def build_bipartite_graph(Bin, Bout):\n",
        "    P, T = Bin.shape\n",
        "    N = P + T\n",
        "\n",
        "    def p_node(i): return i\n",
        "    def t_node(j): return P + j\n",
        "\n",
        "    adj = [[] for _ in range(N)]\n",
        "    rev = [[] for _ in range(N)]\n",
        "\n",
        "    for i in range(P):\n",
        "        for j in range(T):\n",
        "            if Bin[i, j] > 0:\n",
        "                a, b = p_node(i), t_node(j)\n",
        "                adj[a].append(b); rev[b].append(a)\n",
        "\n",
        "    for i in range(P):\n",
        "        for j in range(T):\n",
        "            if Bout[i, j] > 0:\n",
        "                a, b = t_node(j), p_node(i)\n",
        "                adj[a].append(b); rev[b].append(a)\n",
        "\n",
        "    return adj, rev\n",
        "\n",
        "def tarjan_scc(adj):\n",
        "    index = 0\n",
        "    stack = []\n",
        "    onstack = [False]*len(adj)\n",
        "    idx = [-1]*len(adj)\n",
        "    low = [0]*len(adj)\n",
        "    sccs = []\n",
        "\n",
        "    def strongconnect(v):\n",
        "        nonlocal index\n",
        "        idx[v] = index\n",
        "        low[v] = index\n",
        "        index += 1\n",
        "        stack.append(v)\n",
        "        onstack[v] = True\n",
        "\n",
        "        for w in adj[v]:\n",
        "            if idx[w] == -1:\n",
        "                strongconnect(w)\n",
        "                low[v] = min(low[v], low[w])\n",
        "            elif onstack[w]:\n",
        "                low[v] = min(low[v], idx[w])\n",
        "\n",
        "        if low[v] == idx[v]:\n",
        "            comp = []\n",
        "            while True:\n",
        "                w = stack.pop()\n",
        "                onstack[w] = False\n",
        "                comp.append(w)\n",
        "                if w == v:\n",
        "                    break\n",
        "            sccs.append(comp)\n",
        "\n",
        "    for v in range(len(adj)):\n",
        "        if idx[v] == -1:\n",
        "            strongconnect(v)\n",
        "    return sccs\n",
        "\n",
        "def scc_has_cycle(comp, adj):\n",
        "    if len(comp) > 1:\n",
        "        return True\n",
        "    v = comp[0]\n",
        "    return v in adj[v]\n",
        "\n",
        "def scc_boundary_size(comp, adj, rev):\n",
        "    comp_set = set(comp)\n",
        "    out_edges = 0\n",
        "    in_edges = 0\n",
        "    for v in comp:\n",
        "        for w in adj[v]:\n",
        "            if w not in comp_set:\n",
        "                out_edges += 1\n",
        "        for u in rev[v]:\n",
        "            if u not in comp_set:\n",
        "                in_edges += 1\n",
        "    return in_edges, out_edges\n",
        "\n",
        "# ============================================================\n",
        "# LP primitives\n",
        "# ============================================================\n",
        "def baseline_transition_cover(B, t_idx, min_xt=1.0):\n",
        "    m, n = B.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B.astype(float)\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[t_idx] = (min_xt, None)\n",
        "    res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "    return res.success\n",
        "\n",
        "def dense_witness_simplex(B_sub, eps):\n",
        "    # Feasibility: B_sub x = 0, x >= eps, sum(x)=1\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = np.vstack([B_sub, np.ones((1, n))])\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[-1] = 1.0\n",
        "    bounds = [(eps, None)] * n\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "def cone_nonempty_simplex(B_sub):\n",
        "    # Feasibility: B_sub x = 0, x >= 0, sum(x)=1\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = np.vstack([B_sub, np.ones((1, n))])\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[-1] = 1.0\n",
        "    bounds = [(0, None)] * n\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "def targeted_cover_in_scc(B_full, t_global, t_indices, min_xt=1.0):\n",
        "    pos = t_indices.index(t_global)\n",
        "    B_sub = B_full[:, t_indices].astype(float)\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B_sub\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[pos] = (min_xt, None)\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "def support_of_witness(x, eps0=1e-12):\n",
        "    return [j for j, v in enumerate(x) if v > eps0]\n",
        "\n",
        "# ============================================================\n",
        "# Runner with dense-first SCC logic\n",
        "# ============================================================\n",
        "def run_one_net(name, url, eps=1e-6, fragile_k=3, bottleneck_boundary_k=1):\n",
        "    filename = f\"{name}.pnml\"\n",
        "    if not os.path.exists(filename):\n",
        "        print(\"\\nDownloading PNML:\", url)\n",
        "        urllib.request.urlretrieve(url, filename)\n",
        "    else:\n",
        "        print(\"\\nPNML already present:\", filename)\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    places, transitions, arcs, ignored, Bin, Bout, B = parse_pnml_to_matrices(filename)\n",
        "    t_parse = time.perf_counter() - t0\n",
        "\n",
        "    P, T = Bin.shape\n",
        "    approx_arcs = int((Bin > 0).sum() + (Bout > 0).sum())\n",
        "\n",
        "    print(\"\\n==\", name, \"==\")\n",
        "    print(\"Parsed PNML:\")\n",
        "    print(\"  #places:\", P)\n",
        "    print(\"  #transitions:\", T)\n",
        "    print(\"  arcs in file:\", len(arcs), \" ignored arcs:\", ignored)\n",
        "    print(\"  approx #nonzero arcs:\", approx_arcs)\n",
        "    print(\"  parse time (s):\", t_parse)\n",
        "\n",
        "    # Baseline\n",
        "    t0 = time.perf_counter()\n",
        "    base_flags = [baseline_transition_cover(B, j, 1.0) for j in range(T)]\n",
        "    t_baseline = time.perf_counter() - t0\n",
        "    base_uncovered = [j for j, ok in enumerate(base_flags) if not ok]\n",
        "\n",
        "    print(\"\\nBASELINE uncovered transitions (0-based):\", base_uncovered)\n",
        "    print(\"Baseline time (s):\", t_baseline)\n",
        "\n",
        "    # SCCs\n",
        "    adj, rev = build_bipartite_graph(Bin, Bout)\n",
        "    def t_node(j): return P + j\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    sccs = tarjan_scc(adj)\n",
        "    t_scc = time.perf_counter() - t0\n",
        "\n",
        "    scc_id = {}\n",
        "    for k, comp in enumerate(sccs):\n",
        "        for v in comp:\n",
        "            scc_id[v] = k\n",
        "    has_cycle = [scc_has_cycle(comp, adj) for comp in sccs]\n",
        "\n",
        "    labels = [\"Uncertain\"] * T\n",
        "    witnesses = [None] * T\n",
        "\n",
        "    # Gate-0\n",
        "    t0 = time.perf_counter()\n",
        "    for j in range(T):\n",
        "        sid = scc_id[t_node(j)]\n",
        "        if not has_cycle[sid]:\n",
        "            labels[j] = \"StructurallyUncovered\"\n",
        "    t_gate0 = time.perf_counter() - t0\n",
        "\n",
        "    # Gate-2 per-SCC (dense-first, then cone-nonempty only if needed)\n",
        "    t0 = time.perf_counter()\n",
        "    lp_calls_scc = 0\n",
        "    lp_calls_targeted = 0\n",
        "    coneempty_sccs = 0\n",
        "    nondense_sccs = 0  # cone nonempty but dense failed\n",
        "\n",
        "    for sid, comp in enumerate(sccs):\n",
        "        t_indices = [v - P for v in comp if v >= P]\n",
        "        if not t_indices:\n",
        "            continue\n",
        "\n",
        "        if not has_cycle[sid]:\n",
        "            for tj in t_indices:\n",
        "                labels[tj] = \"StructurallyUncovered\"\n",
        "            continue\n",
        "\n",
        "        inc, outc = scc_boundary_size(comp, adj, rev)\n",
        "        B_sub = B[:, t_indices].astype(float)\n",
        "\n",
        "        # 1) Dense witness first\n",
        "        res_dense = dense_witness_simplex(B_sub, eps)\n",
        "        lp_calls_scc += 1\n",
        "\n",
        "        if res_dense.success:\n",
        "            x_sub = np.array(res_dense.x)\n",
        "            x_sub[np.abs(x_sub) < 1e-12] = 0.0\n",
        "\n",
        "            full = np.zeros(T, dtype=float)\n",
        "            for k, tj in enumerate(t_indices):\n",
        "                full[tj] = x_sub[k]\n",
        "\n",
        "            supp = support_of_witness(full)\n",
        "            supp_size = len(supp)\n",
        "\n",
        "            for tj in t_indices:\n",
        "                if full[tj] <= 1e-12:\n",
        "                    continue\n",
        "\n",
        "                lab = \"CoveredWitness\"\n",
        "                if supp_size <= fragile_k:\n",
        "                    lab = \"FragileCovered\"\n",
        "                if inc <= bottleneck_boundary_k or outc <= bottleneck_boundary_k:\n",
        "                    lab = \"BottleneckLimited\"\n",
        "\n",
        "                labels[tj] = lab\n",
        "                witnesses[tj] = full\n",
        "            continue\n",
        "\n",
        "        # 2) Dense failed => check cone nonempty\n",
        "        res_cone = cone_nonempty_simplex(B_sub)\n",
        "        lp_calls_scc += 1\n",
        "\n",
        "        if not res_cone.success:\n",
        "            coneempty_sccs += 1\n",
        "            for tj in t_indices:\n",
        "                labels[tj] = \"InvariantConeEmpty\"\n",
        "            continue\n",
        "\n",
        "        # Cone is nonempty but not fully dense at eps\n",
        "        # Leave transitions Uncertain here; targeted fallback will resolve per transition.\n",
        "        nondense_sccs += 1\n",
        "\n",
        "    t_gate2_scc = time.perf_counter() - t0\n",
        "\n",
        "    # Targeted fallback\n",
        "    t0 = time.perf_counter()\n",
        "    for j in range(T):\n",
        "        if labels[j] != \"Uncertain\":\n",
        "            continue\n",
        "\n",
        "        sid = scc_id[t_node(j)]\n",
        "        comp = sccs[sid]\n",
        "        t_indices = [v - P for v in comp if v >= P]\n",
        "\n",
        "        if not has_cycle[sid]:\n",
        "            labels[j] = \"StructurallyUncovered\"\n",
        "            continue\n",
        "\n",
        "        res = targeted_cover_in_scc(B, j, t_indices, min_xt=1.0)\n",
        "        lp_calls_targeted += 1\n",
        "        if not res.success:\n",
        "            labels[j] = \"StructurallyUncovered\"\n",
        "        else:\n",
        "            labels[j] = \"CoveredWitness\"\n",
        "\n",
        "    t_gate2_targeted = time.perf_counter() - t0\n",
        "\n",
        "    gict_uncovered = [j for j, lab in enumerate(labels) if lab in (\"StructurallyUncovered\", \"InvariantConeEmpty\")]\n",
        "\n",
        "    counts = defaultdict(int)\n",
        "    for lab in labels:\n",
        "        counts[lab] += 1\n",
        "\n",
        "    print(\"\\nGICT uncovered transitions (0-based):\", gict_uncovered)\n",
        "    print(\"\\nGICT label histogram:\")\n",
        "    for k in sorted(counts):\n",
        "        print(f\"  {k}: {counts[k]}\")\n",
        "\n",
        "    print(\"\\nGICT timings + LP counts:\")\n",
        "    print(\"  SCC time (s):\", t_scc)\n",
        "    print(\"  Gate-0 time (s):\", t_gate0)\n",
        "    print(\"  Gate-2 per-SCC time (s):\", t_gate2_scc)\n",
        "    print(\"  Gate-2 targeted time (s):\", t_gate2_targeted)\n",
        "    print(\"  LP calls (per-SCC):\", lp_calls_scc)\n",
        "    print(\"  LP calls (targeted):\", lp_calls_targeted)\n",
        "    print(\"  Total LP calls:\", lp_calls_scc + lp_calls_targeted)\n",
        "    print(\"  ConeEmpty SCCs:\", coneempty_sccs)\n",
        "    print(\"  NonDense-but-nonempty SCCs:\", nondense_sccs)\n",
        "\n",
        "    return {\n",
        "        \"name\": name,\n",
        "        \"P\": P,\n",
        "        \"T\": T,\n",
        "        \"arcs\": len(arcs),\n",
        "        \"baseline_uncovered\": len(base_uncovered),\n",
        "        \"baseline_time_s\": t_baseline,\n",
        "        \"gict_uncovered\": len(gict_uncovered),\n",
        "        \"gict_total_lp\": (lp_calls_scc + lp_calls_targeted),\n",
        "        \"gict_lp_scc\": lp_calls_scc,\n",
        "        \"gict_lp_targeted\": lp_calls_targeted,\n",
        "        \"labels\": dict(counts),\n",
        "        \"coneempty_sccs\": coneempty_sccs,\n",
        "        \"nondense_sccs\": nondense_sccs,\n",
        "    }\n",
        "\n",
        "def print_comparison_table(results):\n",
        "    print(\"\\n============================\")\n",
        "    print(\"COMPARISON SUMMARY\")\n",
        "    print(\"============================\")\n",
        "    header = (\n",
        "        \"Net | P | T | arcs | Baseline uncovered | Baseline time(s) | \"\n",
        "        \"GICT uncovered | GICT total LP | (SCC,Targeted) | ConeEmpty SCCs | NonDense SCCs\"\n",
        "    )\n",
        "    print(header)\n",
        "    print(\"-\"*len(header))\n",
        "    for r in results:\n",
        "        print(\n",
        "            f\"{r['name']} | {r['P']} | {r['T']} | {r['arcs']} | \"\n",
        "            f\"{r['baseline_uncovered']} | {r['baseline_time_s']:.6f} | \"\n",
        "            f\"{r['gict_uncovered']} | {r['gict_total_lp']} | \"\n",
        "            f\"({r['gict_lp_scc']},{r['gict_lp_targeted']}) | \"\n",
        "            f\"{r['coneempty_sccs']} | {r['nondense_sccs']}\"\n",
        "        )\n",
        "    print(\"\\nLabel histograms:\")\n",
        "    for r in results:\n",
        "        print(f\"  {r['name']}: {r['labels']}\")\n",
        "\n",
        "# ============================================================\n",
        "# RUN THREE NETS BACK-TO-BACK\n",
        "# ============================================================\n",
        "results = []\n",
        "results.append(run_one_net(\"philo\", \"https://www.pnml.org/version-2009/examples/philo.pnml\", eps=1e-6))\n",
        "results.append(run_one_net(\"Token-ring\", \"https://www.pnml.org/version-2009/examples/Token-ring.pnml\", eps=1e-6))\n",
        "results.append(run_one_net(\"Piscine\", \"https://www.pnml.org/version-2009/examples/Piscine.pnml\", eps=1e-6))\n",
        "print_comparison_table(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JD-78G4lroK2",
        "outputId": "0c9c009b-ed6d-43a3-d962-87c10fdf3b35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PNML already present: philo.pnml\n",
            "\n",
            "== philo ==\n",
            "Parsed PNML:\n",
            "  #places: 30\n",
            "  #transitions: 30\n",
            "  arcs in file: 96  ignored arcs: 0\n",
            "  approx #nonzero arcs: 96\n",
            "  parse time (s): 0.002391719999650377\n",
            "\n",
            "BASELINE uncovered transitions (0-based): []\n",
            "Baseline time (s): 0.06752650500038726\n",
            "\n",
            "GICT uncovered transitions (0-based): []\n",
            "\n",
            "GICT label histogram:\n",
            "  BottleneckLimited: 30\n",
            "\n",
            "GICT timings + LP counts:\n",
            "  SCC time (s): 5.5252000493055675e-05\n",
            "  Gate-0 time (s): 4.660000740841497e-06\n",
            "  Gate-2 per-SCC time (s): 0.0021977489996061195\n",
            "  Gate-2 targeted time (s): 3.094000021519605e-06\n",
            "  LP calls (per-SCC): 1\n",
            "  LP calls (targeted): 0\n",
            "  Total LP calls: 1\n",
            "  ConeEmpty SCCs: 0\n",
            "  NonDense-but-nonempty SCCs: 0\n",
            "\n",
            "PNML already present: Token-ring.pnml\n",
            "\n",
            "== Token-ring ==\n",
            "Parsed PNML:\n",
            "  #places: 18\n",
            "  #transitions: 15\n",
            "  arcs in file: 67  ignored arcs: 0\n",
            "  approx #nonzero arcs: 67\n",
            "  parse time (s): 0.0010557579998931033\n",
            "\n",
            "BASELINE uncovered transitions (0-based): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
            "Baseline time (s): 0.02451929300059419\n",
            "\n",
            "GICT uncovered transitions (0-based): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
            "\n",
            "GICT label histogram:\n",
            "  InvariantConeEmpty: 15\n",
            "\n",
            "GICT timings + LP counts:\n",
            "  SCC time (s): 3.5275999834993854e-05\n",
            "  Gate-0 time (s): 3.1170002330327407e-06\n",
            "  Gate-2 per-SCC time (s): 0.00305549000040628\n",
            "  Gate-2 targeted time (s): 2.8570002541528083e-06\n",
            "  LP calls (per-SCC): 2\n",
            "  LP calls (targeted): 0\n",
            "  Total LP calls: 2\n",
            "  ConeEmpty SCCs: 1\n",
            "  NonDense-but-nonempty SCCs: 0\n",
            "\n",
            "PNML already present: Piscine.pnml\n",
            "\n",
            "== Piscine ==\n",
            "Parsed PNML:\n",
            "  #places: 9\n",
            "  #transitions: 7\n",
            "  arcs in file: 20  ignored arcs: 0\n",
            "  approx #nonzero arcs: 20\n",
            "  parse time (s): 0.0005254560001048958\n",
            "\n",
            "BASELINE uncovered transitions (0-based): []\n",
            "Baseline time (s): 0.012334624999311927\n",
            "\n",
            "GICT uncovered transitions (0-based): []\n",
            "\n",
            "GICT label histogram:\n",
            "  BottleneckLimited: 7\n",
            "\n",
            "GICT timings + LP counts:\n",
            "  SCC time (s): 1.680000059423037e-05\n",
            "  Gate-0 time (s): 1.549000444356352e-06\n",
            "  Gate-2 per-SCC time (s): 0.0018972320003740606\n",
            "  Gate-2 targeted time (s): 1.1470001481939107e-06\n",
            "  LP calls (per-SCC): 1\n",
            "  LP calls (targeted): 0\n",
            "  Total LP calls: 1\n",
            "  ConeEmpty SCCs: 0\n",
            "  NonDense-but-nonempty SCCs: 0\n",
            "\n",
            "============================\n",
            "COMPARISON SUMMARY\n",
            "============================\n",
            "Net | P | T | arcs | Baseline uncovered | Baseline time(s) | GICT uncovered | GICT total LP | (SCC,Targeted) | ConeEmpty SCCs | NonDense SCCs\n",
            "---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "philo | 30 | 30 | 96 | 0 | 0.067527 | 0 | 1 | (1,0) | 0 | 0\n",
            "Token-ring | 18 | 15 | 67 | 15 | 0.024519 | 15 | 2 | (2,0) | 1 | 0\n",
            "Piscine | 9 | 7 | 20 | 0 | 0.012335 | 0 | 1 | (1,0) | 0 | 0\n",
            "\n",
            "Label histograms:\n",
            "  philo: {'BottleneckLimited': 30}\n",
            "  Token-ring: {'InvariantConeEmpty': 15}\n",
            "  Piscine: {'BottleneckLimited': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "# ============================================================\n",
        "# 0) Download philo PNML if needed\n",
        "# ============================================================\n",
        "PHILO_URL = \"https://www.pnml.org/version-2009/examples/philo.pnml\"\n",
        "PHILO_FILE = \"philo.pnml\"\n",
        "MUT_FILE = \"philo_leak.pnml\"\n",
        "\n",
        "if not os.path.exists(PHILO_FILE):\n",
        "    print(\"Downloading PNML:\", PHILO_URL)\n",
        "    urllib.request.urlretrieve(PHILO_URL, PHILO_FILE)\n",
        "else:\n",
        "    print(\"PNML already present:\", PHILO_FILE)\n",
        "\n",
        "# ============================================================\n",
        "# 1) Mutate: add a new transition LEAK with one input arc from an existing place\n",
        "# ============================================================\n",
        "def strip_ns(tag):\n",
        "    return tag.split(\"}\", 1)[-1] if \"}\" in tag else tag\n",
        "\n",
        "def find_first_place_id(root):\n",
        "    for elem in root.iter():\n",
        "        if strip_ns(elem.tag) == \"place\":\n",
        "            pid = elem.attrib.get(\"id\")\n",
        "            if pid:\n",
        "                return pid\n",
        "    return None\n",
        "\n",
        "def find_net_element(root):\n",
        "    for elem in root.iter():\n",
        "        if strip_ns(elem.tag) == \"net\":\n",
        "            return elem\n",
        "    return None\n",
        "\n",
        "def unique_id(existing_ids, base):\n",
        "    if base not in existing_ids:\n",
        "        return base\n",
        "    k = 1\n",
        "    while f\"{base}_{k}\" in existing_ids:\n",
        "        k += 1\n",
        "    return f\"{base}_{k}\"\n",
        "\n",
        "def collect_all_ids(root):\n",
        "    ids = set()\n",
        "    for elem in root.iter():\n",
        "        if \"id\" in elem.attrib:\n",
        "            ids.add(elem.attrib[\"id\"])\n",
        "    return ids\n",
        "\n",
        "def add_leak_transition(pnml_in, pnml_out, leak_name=\"LEAK\", weight=1):\n",
        "    tree = ET.parse(pnml_in)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    net = find_net_element(root)\n",
        "    if net is None:\n",
        "        raise RuntimeError(\"Could not find <net> element in PNML.\")\n",
        "\n",
        "    first_place = find_first_place_id(root)\n",
        "    if first_place is None:\n",
        "        raise RuntimeError(\"Could not find any <place> in PNML.\")\n",
        "\n",
        "    existing_ids = collect_all_ids(root)\n",
        "\n",
        "    t_id = unique_id(existing_ids, \"t_leak\")\n",
        "    existing_ids.add(t_id)\n",
        "\n",
        "    a_id = unique_id(existing_ids, \"a_leak\")\n",
        "    existing_ids.add(a_id)\n",
        "\n",
        "    # Build <transition id=\"t_leak\"><name><text>LEAK</text></name></transition>\n",
        "    t_elem = ET.Element(\"transition\", {\"id\": t_id})\n",
        "    name_elem = ET.SubElement(t_elem, \"name\")\n",
        "    text_elem = ET.SubElement(name_elem, \"text\")\n",
        "    text_elem.text = leak_name\n",
        "\n",
        "    # Build <arc id=\"a_leak\" source=\"first_place\" target=\"t_leak\"><inscription><text>1</text></inscription></arc>\n",
        "    arc_elem = ET.Element(\"arc\", {\"id\": a_id, \"source\": first_place, \"target\": t_id})\n",
        "    ins_elem = ET.SubElement(arc_elem, \"inscription\")\n",
        "    ins_text = ET.SubElement(ins_elem, \"text\")\n",
        "    ins_text.text = str(int(weight))\n",
        "\n",
        "    # Append to net\n",
        "    net.append(t_elem)\n",
        "    net.append(arc_elem)\n",
        "\n",
        "    tree.write(pnml_out, encoding=\"utf-8\", xml_declaration=True)\n",
        "    return first_place, t_id, a_id\n",
        "\n",
        "first_place, t_id, a_id = add_leak_transition(PHILO_FILE, MUT_FILE, leak_name=\"LEAK\", weight=1)\n",
        "print(\"\\nWrote mutated PNML:\", MUT_FILE)\n",
        "print(\"  Added transition:\", t_id, \"(name=LEAK)\")\n",
        "print(\"  Added arc:\", a_id, \"source=\", first_place, \"-> target=\", t_id)\n",
        "\n",
        "# ============================================================\n",
        "# 2) PNML -> matrices\n",
        "# ============================================================\n",
        "def parse_pnml_to_matrices(pnml_file):\n",
        "    tree = ET.parse(pnml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    places = []\n",
        "    transitions = []\n",
        "    arcs = []\n",
        "\n",
        "    for elem in root.iter():\n",
        "        tag = strip_ns(elem.tag)\n",
        "        if tag == \"place\":\n",
        "            pid = elem.attrib.get(\"id\")\n",
        "            if pid:\n",
        "                places.append(pid)\n",
        "        elif tag == \"transition\":\n",
        "            tid = elem.attrib.get(\"id\")\n",
        "            if tid:\n",
        "                transitions.append(tid)\n",
        "        elif tag == \"arc\":\n",
        "            src = elem.attrib.get(\"source\")\n",
        "            tgt = elem.attrib.get(\"target\")\n",
        "            aid = elem.attrib.get(\"id\", \"\")\n",
        "            if src and tgt:\n",
        "                w = 1\n",
        "                for child in elem.iter():\n",
        "                    if strip_ns(child.tag) == \"inscription\":\n",
        "                        for g in child.iter():\n",
        "                            if strip_ns(g.tag) == \"text\" and g.text is not None:\n",
        "                                txt = g.text.strip()\n",
        "                                if txt:\n",
        "                                    try:\n",
        "                                        w = int(txt)\n",
        "                                    except:\n",
        "                                        pass\n",
        "                arcs.append((src, tgt, w, aid))\n",
        "\n",
        "    def uniq(seq):\n",
        "        seen = set()\n",
        "        out = []\n",
        "        for x in seq:\n",
        "            if x not in seen:\n",
        "                out.append(x)\n",
        "                seen.add(x)\n",
        "        return out\n",
        "\n",
        "    places = uniq([p for p in places if p is not None])\n",
        "    transitions = uniq([t for t in transitions if t is not None])\n",
        "\n",
        "    p_index = {p:i for i,p in enumerate(places)}\n",
        "    t_index = {t:i for i,t in enumerate(transitions)}\n",
        "\n",
        "    P = len(places)\n",
        "    T = len(transitions)\n",
        "\n",
        "    Bin = np.zeros((P, T), dtype=int)\n",
        "    Bout = np.zeros((P, T), dtype=int)\n",
        "\n",
        "    ignored = 0\n",
        "    for src, tgt, w, _aid in arcs:\n",
        "        if src in p_index and tgt in t_index:\n",
        "            Bin[p_index[src], t_index[tgt]] += w\n",
        "        elif src in t_index and tgt in p_index:\n",
        "            Bout[p_index[tgt], t_index[src]] += w\n",
        "        else:\n",
        "            ignored += 1\n",
        "\n",
        "    B = Bout - Bin\n",
        "    return places, transitions, arcs, ignored, Bin, Bout, B\n",
        "\n",
        "# ============================================================\n",
        "# 3) SCC + LP helpers (dense-first GICT)\n",
        "# ============================================================\n",
        "def build_bipartite_graph(Bin, Bout):\n",
        "    P, T = Bin.shape\n",
        "    N = P + T\n",
        "    def t_node(j): return P + j\n",
        "\n",
        "    adj = [[] for _ in range(N)]\n",
        "    rev = [[] for _ in range(N)]\n",
        "\n",
        "    for i in range(P):\n",
        "        for j in range(T):\n",
        "            if Bin[i, j] > 0:\n",
        "                adj[i].append(t_node(j))\n",
        "                rev[t_node(j)].append(i)\n",
        "\n",
        "    for i in range(P):\n",
        "        for j in range(T):\n",
        "            if Bout[i, j] > 0:\n",
        "                adj[t_node(j)].append(i)\n",
        "                rev[i].append(t_node(j))\n",
        "\n",
        "    return adj, rev\n",
        "\n",
        "def tarjan_scc(adj):\n",
        "    index = 0\n",
        "    stack = []\n",
        "    onstack = [False]*len(adj)\n",
        "    idx = [-1]*len(adj)\n",
        "    low = [0]*len(adj)\n",
        "    sccs = []\n",
        "\n",
        "    def strongconnect(v):\n",
        "        nonlocal index\n",
        "        idx[v] = index\n",
        "        low[v] = index\n",
        "        index += 1\n",
        "        stack.append(v)\n",
        "        onstack[v] = True\n",
        "\n",
        "        for w in adj[v]:\n",
        "            if idx[w] == -1:\n",
        "                strongconnect(w)\n",
        "                low[v] = min(low[v], low[w])\n",
        "            elif onstack[w]:\n",
        "                low[v] = min(low[v], idx[w])\n",
        "\n",
        "        if low[v] == idx[v]:\n",
        "            comp = []\n",
        "            while True:\n",
        "                w = stack.pop()\n",
        "                onstack[w] = False\n",
        "                comp.append(w)\n",
        "                if w == v:\n",
        "                    break\n",
        "            sccs.append(comp)\n",
        "\n",
        "    for v in range(len(adj)):\n",
        "        if idx[v] == -1:\n",
        "            strongconnect(v)\n",
        "    return sccs\n",
        "\n",
        "def scc_has_cycle(comp, adj):\n",
        "    if len(comp) > 1:\n",
        "        return True\n",
        "    v = comp[0]\n",
        "    return v in adj[v]\n",
        "\n",
        "def scc_boundary_size(comp, adj, rev):\n",
        "    comp_set = set(comp)\n",
        "    out_edges = 0\n",
        "    in_edges = 0\n",
        "    for v in comp:\n",
        "        for w in adj[v]:\n",
        "            if w not in comp_set:\n",
        "                out_edges += 1\n",
        "        for u in rev[v]:\n",
        "            if u not in comp_set:\n",
        "                in_edges += 1\n",
        "    return in_edges, out_edges\n",
        "\n",
        "def baseline_transition_cover(B, t_idx, min_xt=1.0):\n",
        "    m, n = B.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B.astype(float)\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[t_idx] = (min_xt, None)\n",
        "    res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "    return res.success\n",
        "\n",
        "def dense_witness_simplex(B_sub, eps):\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = np.vstack([B_sub, np.ones((1, n))])\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[-1] = 1.0\n",
        "    bounds = [(eps, None)] * n\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "def cone_nonempty_simplex(B_sub):\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = np.vstack([B_sub, np.ones((1, n))])\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[-1] = 1.0\n",
        "    bounds = [(0, None)] * n\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "def targeted_cover_in_scc(B_full, t_global, t_indices, min_xt=1.0):\n",
        "    pos = t_indices.index(t_global)\n",
        "    B_sub = B_full[:, t_indices].astype(float)\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B_sub\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[pos] = (min_xt, None)\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "def support_of_witness(x, eps0=1e-12):\n",
        "    return [j for j, v in enumerate(x) if v > eps0]\n",
        "\n",
        "def run_dense_first_gict(pnml_file, eps=1e-6, fragile_k=3, bottleneck_boundary_k=1):\n",
        "    t0 = time.perf_counter()\n",
        "    places, transitions, arcs, ignored, Bin, Bout, B = parse_pnml_to_matrices(pnml_file)\n",
        "    t_parse = time.perf_counter() - t0\n",
        "\n",
        "    P, T = Bin.shape\n",
        "    approx_arcs = int((Bin > 0).sum() + (Bout > 0).sum())\n",
        "\n",
        "    print(\"\\n==\", pnml_file, \"==\")\n",
        "    print(\"Parsed PNML:\")\n",
        "    print(\"  #places:\", P)\n",
        "    print(\"  #transitions:\", T)\n",
        "    print(\"  arcs in file:\", len(arcs), \" ignored arcs:\", ignored)\n",
        "    print(\"  approx #nonzero arcs:\", approx_arcs)\n",
        "    print(\"  parse time (s):\", t_parse)\n",
        "\n",
        "    # Baseline\n",
        "    t0 = time.perf_counter()\n",
        "    base_flags = [baseline_transition_cover(B, j, 1.0) for j in range(T)]\n",
        "    t_baseline = time.perf_counter() - t0\n",
        "    base_uncovered = [j for j, ok in enumerate(base_flags) if not ok]\n",
        "    print(\"\\nBASELINE uncovered transitions (0-based):\", base_uncovered)\n",
        "    print(\"Baseline time (s):\", t_baseline)\n",
        "\n",
        "    # SCCs\n",
        "    adj, rev = build_bipartite_graph(Bin, Bout)\n",
        "    def t_node(j): return P + j\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    sccs = tarjan_scc(adj)\n",
        "    t_scc = time.perf_counter() - t0\n",
        "\n",
        "    scc_id = {}\n",
        "    for k, comp in enumerate(sccs):\n",
        "        for v in comp:\n",
        "            scc_id[v] = k\n",
        "    has_cycle = [scc_has_cycle(comp, adj) for comp in sccs]\n",
        "\n",
        "    labels = [\"Uncertain\"] * T\n",
        "    witnesses = [None] * T\n",
        "\n",
        "    # Gate-0\n",
        "    t0 = time.perf_counter()\n",
        "    for j in range(T):\n",
        "        sid = scc_id[t_node(j)]\n",
        "        if not has_cycle[sid]:\n",
        "            labels[j] = \"StructurallyUncovered\"\n",
        "    t_gate0 = time.perf_counter() - t0\n",
        "\n",
        "    # Gate-2 per-SCC (dense-first, then cone test)\n",
        "    t0 = time.perf_counter()\n",
        "    lp_calls_scc = 0\n",
        "    lp_calls_targeted = 0\n",
        "    coneempty_sccs = 0\n",
        "    nondense_sccs = 0\n",
        "\n",
        "    for sid, comp in enumerate(sccs):\n",
        "        t_indices = [v - P for v in comp if v >= P]\n",
        "        if not t_indices:\n",
        "            continue\n",
        "\n",
        "        if not has_cycle[sid]:\n",
        "            for tj in t_indices:\n",
        "                labels[tj] = \"StructurallyUncovered\"\n",
        "            continue\n",
        "\n",
        "        inc, outc = scc_boundary_size(comp, adj, rev)\n",
        "        B_sub = B[:, t_indices].astype(float)\n",
        "\n",
        "        # Dense witness first\n",
        "        res_dense = dense_witness_simplex(B_sub, eps)\n",
        "        lp_calls_scc += 1\n",
        "\n",
        "        if res_dense.success:\n",
        "            x_sub = np.array(res_dense.x)\n",
        "            x_sub[np.abs(x_sub) < 1e-12] = 0.0\n",
        "\n",
        "            full = np.zeros(T, dtype=float)\n",
        "            for k, tj in enumerate(t_indices):\n",
        "                full[tj] = x_sub[k]\n",
        "\n",
        "            supp = support_of_witness(full)\n",
        "            supp_size = len(supp)\n",
        "\n",
        "            for tj in t_indices:\n",
        "                if full[tj] <= 1e-12:\n",
        "                    continue\n",
        "                lab = \"CoveredWitness\"\n",
        "                if supp_size <= fragile_k:\n",
        "                    lab = \"FragileCovered\"\n",
        "                if inc <= bottleneck_boundary_k or outc <= bottleneck_boundary_k:\n",
        "                    lab = \"BottleneckLimited\"\n",
        "                labels[tj] = lab\n",
        "                witnesses[tj] = full\n",
        "            continue\n",
        "\n",
        "        # Dense failed -> cone test\n",
        "        res_cone = cone_nonempty_simplex(B_sub)\n",
        "        lp_calls_scc += 1\n",
        "\n",
        "        if not res_cone.success:\n",
        "            coneempty_sccs += 1\n",
        "            for tj in t_indices:\n",
        "                labels[tj] = \"InvariantConeEmpty\"\n",
        "            continue\n",
        "\n",
        "        nondense_sccs += 1\n",
        "\n",
        "    t_gate2_scc = time.perf_counter() - t0\n",
        "\n",
        "    # Targeted fallback (only if any Uncertain remain)\n",
        "    t0 = time.perf_counter()\n",
        "    for j in range(T):\n",
        "        if labels[j] != \"Uncertain\":\n",
        "            continue\n",
        "        sid = scc_id[t_node(j)]\n",
        "        comp = sccs[sid]\n",
        "        t_indices = [v - P for v in comp if v >= P]\n",
        "        if not has_cycle[sid]:\n",
        "            labels[j] = \"StructurallyUncovered\"\n",
        "            continue\n",
        "        res = targeted_cover_in_scc(B, j, t_indices, min_xt=1.0)\n",
        "        lp_calls_targeted += 1\n",
        "        labels[j] = \"CoveredWitness\" if res.success else \"StructurallyUncovered\"\n",
        "    t_gate2_targeted = time.perf_counter() - t0\n",
        "\n",
        "    gict_uncovered = [j for j, lab in enumerate(labels) if lab in (\"StructurallyUncovered\", \"InvariantConeEmpty\")]\n",
        "\n",
        "    counts = defaultdict(int)\n",
        "    for lab in labels:\n",
        "        counts[lab] += 1\n",
        "\n",
        "    print(\"\\nGICT uncovered transitions (0-based):\", gict_uncovered)\n",
        "    print(\"\\nGICT label histogram:\")\n",
        "    for k in sorted(counts):\n",
        "        print(f\"  {k}: {counts[k]}\")\n",
        "\n",
        "    print(\"\\nGICT timings + LP counts:\")\n",
        "    print(\"  SCC time (s):\", t_scc)\n",
        "    print(\"  Gate-0 time (s):\", t_gate0)\n",
        "    print(\"  Gate-2 per-SCC time (s):\", t_gate2_scc)\n",
        "    print(\"  Gate-2 targeted time (s):\", t_gate2_targeted)\n",
        "    print(\"  LP calls (per-SCC):\", lp_calls_scc)\n",
        "    print(\"  LP calls (targeted):\", lp_calls_targeted)\n",
        "    print(\"  Total LP calls:\", lp_calls_scc + lp_calls_targeted)\n",
        "    print(\"  ConeEmpty SCCs:\", coneempty_sccs)\n",
        "    print(\"  NonDense-but-nonempty SCCs:\", nondense_sccs)\n",
        "\n",
        "    return {\n",
        "        \"P\": P, \"T\": T, \"arcs\": len(arcs),\n",
        "        \"baseline_uncovered\": len([j for j, ok in enumerate(base_flags) if not ok]),\n",
        "        \"gict_uncovered\": len(gict_uncovered),\n",
        "        \"labels\": dict(counts),\n",
        "        \"uncovered_idx\": gict_uncovered,\n",
        "        \"transitions\": transitions\n",
        "    }\n",
        "\n",
        "# ============================================================\n",
        "# 4) Run original vs mutated and highlight the new uncovered transition\n",
        "# ============================================================\n",
        "r0 = run_dense_first_gict(PHILO_FILE)\n",
        "r1 = run_dense_first_gict(MUT_FILE)\n",
        "\n",
        "print(\"\\n============================\")\n",
        "print(\"DIFF SUMMARY (original -> mutated)\")\n",
        "print(\"============================\")\n",
        "print(\"Original:  P=\", r0[\"P\"], \"T=\", r0[\"T\"], \"uncovered=\", r0[\"gict_uncovered\"], \"labels=\", r0[\"labels\"])\n",
        "print(\"Mutated:   P=\", r1[\"P\"], \"T=\", r1[\"T\"], \"uncovered=\", r1[\"gict_uncovered\"], \"labels=\", r1[\"labels\"])\n",
        "\n",
        "# Identify which transition ID corresponds to LEAK\n",
        "leak_indices = [i for i, tid in enumerate(r1[\"transitions\"]) if tid == t_id]\n",
        "if leak_indices:\n",
        "    li = leak_indices[0]\n",
        "    print(\"\\nLEAK transition index (0-based):\", li, \"id:\", t_id)\n",
        "    print(\"Is LEAK uncovered?\", li in r1[\"uncovered_idx\"])\n",
        "else:\n",
        "    print(\"\\nCould not locate LEAK transition ID in parsed transition list (unexpected).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daB-CQJXt1CW",
        "outputId": "f9614ef4-8965-4089-9872-8170ab8f1e5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PNML already present: philo.pnml\n",
            "\n",
            "Wrote mutated PNML: philo_leak.pnml\n",
            "  Added transition: t_leak (name=LEAK)\n",
            "  Added arc: a_leak source= cId175-i943123747 -> target= t_leak\n",
            "\n",
            "== philo.pnml ==\n",
            "Parsed PNML:\n",
            "  #places: 30\n",
            "  #transitions: 30\n",
            "  arcs in file: 96  ignored arcs: 0\n",
            "  approx #nonzero arcs: 96\n",
            "  parse time (s): 0.003939916000490484\n",
            "\n",
            "BASELINE uncovered transitions (0-based): []\n",
            "Baseline time (s): 0.1543160490000446\n",
            "\n",
            "GICT uncovered transitions (0-based): []\n",
            "\n",
            "GICT label histogram:\n",
            "  BottleneckLimited: 30\n",
            "\n",
            "GICT timings + LP counts:\n",
            "  SCC time (s): 9.720800062495982e-05\n",
            "  Gate-0 time (s): 9.41199959925143e-06\n",
            "  Gate-2 per-SCC time (s): 0.0041026599992619595\n",
            "  Gate-2 targeted time (s): 4.7519997679046355e-06\n",
            "  LP calls (per-SCC): 1\n",
            "  LP calls (targeted): 0\n",
            "  Total LP calls: 1\n",
            "  ConeEmpty SCCs: 0\n",
            "  NonDense-but-nonempty SCCs: 0\n",
            "\n",
            "== philo_leak.pnml ==\n",
            "Parsed PNML:\n",
            "  #places: 30\n",
            "  #transitions: 31\n",
            "  arcs in file: 97  ignored arcs: 0\n",
            "  approx #nonzero arcs: 97\n",
            "  parse time (s): 0.003929337000045052\n",
            "\n",
            "BASELINE uncovered transitions (0-based): [30]\n",
            "Baseline time (s): 0.13353871500021341\n",
            "\n",
            "GICT uncovered transitions (0-based): [30]\n",
            "\n",
            "GICT label histogram:\n",
            "  BottleneckLimited: 30\n",
            "  StructurallyUncovered: 1\n",
            "\n",
            "GICT timings + LP counts:\n",
            "  SCC time (s): 9.300500005338108e-05\n",
            "  Gate-0 time (s): 7.136000022001099e-06\n",
            "  Gate-2 per-SCC time (s): 0.00832482300029369\n",
            "  Gate-2 targeted time (s): 4.1860002966132015e-06\n",
            "  LP calls (per-SCC): 1\n",
            "  LP calls (targeted): 0\n",
            "  Total LP calls: 1\n",
            "  ConeEmpty SCCs: 0\n",
            "  NonDense-but-nonempty SCCs: 0\n",
            "\n",
            "============================\n",
            "DIFF SUMMARY (original -> mutated)\n",
            "============================\n",
            "Original:  P= 30 T= 30 uncovered= 0 labels= {'BottleneckLimited': 30}\n",
            "Mutated:   P= 30 T= 31 uncovered= 1 labels= {'BottleneckLimited': 30, 'StructurallyUncovered': 1}\n",
            "\n",
            "LEAK transition index (0-based): 30 id: t_leak\n",
            "Is LEAK uncovered? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "# ============================================================\n",
        "# 0) Download philo PNML if needed\n",
        "# ============================================================\n",
        "PHILO_URL = \"https://www.pnml.org/version-2009/examples/philo.pnml\"\n",
        "PHILO_FILE = \"philo.pnml\"\n",
        "MUT_FILE = \"philo_guard.pnml\"\n",
        "\n",
        "if not os.path.exists(PHILO_FILE):\n",
        "    print(\"Downloading PNML:\", PHILO_URL)\n",
        "    urllib.request.urlretrieve(PHILO_URL, PHILO_FILE)\n",
        "else:\n",
        "    print(\"PNML already present:\", PHILO_FILE)\n",
        "\n",
        "# ============================================================\n",
        "# 1) Guard mutation: add a new place p_guard and one arc p_guard -> t_target\n",
        "#    with no arcs producing into p_guard.\n",
        "#    This forces x[t_target]=0 in any nonnegative T-invariant (Bx=0, x>=0).\n",
        "# ============================================================\n",
        "def strip_ns(tag):\n",
        "    return tag.split(\"}\", 1)[-1] if \"}\" in tag else tag\n",
        "\n",
        "def find_net_element(root):\n",
        "    for elem in root.iter():\n",
        "        if strip_ns(elem.tag) == \"net\":\n",
        "            return elem\n",
        "    return None\n",
        "\n",
        "def collect_all_ids(root):\n",
        "    ids = set()\n",
        "    for elem in root.iter():\n",
        "        if \"id\" in elem.attrib:\n",
        "            ids.add(elem.attrib[\"id\"])\n",
        "    return ids\n",
        "\n",
        "def unique_id(existing_ids, base):\n",
        "    if base not in existing_ids:\n",
        "        return base\n",
        "    k = 1\n",
        "    while f\"{base}_{k}\" in existing_ids:\n",
        "        k += 1\n",
        "    return f\"{base}_{k}\"\n",
        "\n",
        "def pick_first_transition_id(root):\n",
        "    for elem in root.iter():\n",
        "        if strip_ns(elem.tag) == \"transition\":\n",
        "            tid = elem.attrib.get(\"id\")\n",
        "            if tid:\n",
        "                return tid\n",
        "    return None\n",
        "\n",
        "def add_guard_place_and_arc(pnml_in, pnml_out, guard_place_name=\"P_GUARD\", weight=1):\n",
        "    tree = ET.parse(pnml_in)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    net = find_net_element(root)\n",
        "    if net is None:\n",
        "        raise RuntimeError(\"Could not find <net> element in PNML.\")\n",
        "\n",
        "    t_target = pick_first_transition_id(root)\n",
        "    if t_target is None:\n",
        "        raise RuntimeError(\"Could not find any <transition> in PNML.\")\n",
        "\n",
        "    existing_ids = collect_all_ids(root)\n",
        "\n",
        "    p_id = unique_id(existing_ids, \"p_guard\")\n",
        "    existing_ids.add(p_id)\n",
        "\n",
        "    a_id = unique_id(existing_ids, \"a_guard\")\n",
        "    existing_ids.add(a_id)\n",
        "\n",
        "    # <place id=\"p_guard\"><name><text>P_GUARD</text></name></place>\n",
        "    p_elem = ET.Element(\"place\", {\"id\": p_id})\n",
        "    pname = ET.SubElement(p_elem, \"name\")\n",
        "    ptext = ET.SubElement(pname, \"text\")\n",
        "    ptext.text = guard_place_name\n",
        "\n",
        "    # No initial marking (so it is never produced/available)\n",
        "    # If you later want it explicitly empty, PNML often uses initialMarking=0,\n",
        "    # but leaving it absent is usually equivalent for structural invariants.\n",
        "\n",
        "    # <arc id=\"a_guard\" source=\"p_guard\" target=\"t_target\"><inscription><text>1</text></inscription></arc>\n",
        "    arc_elem = ET.Element(\"arc\", {\"id\": a_id, \"source\": p_id, \"target\": t_target})\n",
        "    ins = ET.SubElement(arc_elem, \"inscription\")\n",
        "    instext = ET.SubElement(ins, \"text\")\n",
        "    instext.text = str(int(weight))\n",
        "\n",
        "    # Append to net\n",
        "    net.append(p_elem)\n",
        "    net.append(arc_elem)\n",
        "\n",
        "    tree.write(pnml_out, encoding=\"utf-8\", xml_declaration=True)\n",
        "    return p_id, a_id, t_target\n",
        "\n",
        "p_guard_id, a_guard_id, t_target_id = add_guard_place_and_arc(PHILO_FILE, MUT_FILE, guard_place_name=\"P_GUARD\", weight=1)\n",
        "print(\"\\nWrote guard-mutated PNML:\", MUT_FILE)\n",
        "print(\"  Added place:\", p_guard_id, \"(name=P_GUARD)\")\n",
        "print(\"  Added arc:\", a_guard_id, \"source=\", p_guard_id, \"-> target=\", t_target_id)\n",
        "print(\"  Guarded transition (target):\", t_target_id)\n",
        "\n",
        "# ============================================================\n",
        "# 2) PNML -> matrices\n",
        "# ============================================================\n",
        "def parse_pnml_to_matrices(pnml_file):\n",
        "    tree = ET.parse(pnml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    places = []\n",
        "    transitions = []\n",
        "    arcs = []\n",
        "\n",
        "    for elem in root.iter():\n",
        "        tag = strip_ns(elem.tag)\n",
        "        if tag == \"place\":\n",
        "            pid = elem.attrib.get(\"id\")\n",
        "            if pid:\n",
        "                places.append(pid)\n",
        "        elif tag == \"transition\":\n",
        "            tid = elem.attrib.get(\"id\")\n",
        "            if tid:\n",
        "                transitions.append(tid)\n",
        "        elif tag == \"arc\":\n",
        "            src = elem.attrib.get(\"source\")\n",
        "            tgt = elem.attrib.get(\"target\")\n",
        "            aid = elem.attrib.get(\"id\", \"\")\n",
        "            if src and tgt:\n",
        "                w = 1\n",
        "                for child in elem.iter():\n",
        "                    if strip_ns(child.tag) == \"inscription\":\n",
        "                        for g in child.iter():\n",
        "                            if strip_ns(g.tag) == \"text\" and g.text is not None:\n",
        "                                txt = g.text.strip()\n",
        "                                if txt:\n",
        "                                    try:\n",
        "                                        w = int(txt)\n",
        "                                    except:\n",
        "                                        pass\n",
        "                arcs.append((src, tgt, w, aid))\n",
        "\n",
        "    def uniq(seq):\n",
        "        seen = set()\n",
        "        out = []\n",
        "        for x in seq:\n",
        "            if x not in seen:\n",
        "                out.append(x)\n",
        "                seen.add(x)\n",
        "        return out\n",
        "\n",
        "    places = uniq([p for p in places if p is not None])\n",
        "    transitions = uniq([t for t in transitions if t is not None])\n",
        "\n",
        "    p_index = {p:i for i,p in enumerate(places)}\n",
        "    t_index = {t:i for i,t in enumerate(transitions)}\n",
        "\n",
        "    P = len(places)\n",
        "    T = len(transitions)\n",
        "\n",
        "    Bin = np.zeros((P, T), dtype=int)\n",
        "    Bout = np.zeros((P, T), dtype=int)\n",
        "\n",
        "    ignored = 0\n",
        "    for src, tgt, w, _aid in arcs:\n",
        "        if src in p_index and tgt in t_index:\n",
        "            Bin[p_index[src], t_index[tgt]] += w\n",
        "        elif src in t_index and tgt in p_index:\n",
        "            Bout[p_index[tgt], t_index[src]] += w\n",
        "        else:\n",
        "            ignored += 1\n",
        "\n",
        "    B = Bout - Bin\n",
        "    return places, transitions, arcs, ignored, Bin, Bout, B\n",
        "\n",
        "# ============================================================\n",
        "# 3) SCC + LP helpers (dense-first GICT)\n",
        "# ============================================================\n",
        "def build_bipartite_graph(Bin, Bout):\n",
        "    P, T = Bin.shape\n",
        "    N = P + T\n",
        "    def t_node(j): return P + j\n",
        "\n",
        "    adj = [[] for _ in range(N)]\n",
        "    rev = [[] for _ in range(N)]\n",
        "\n",
        "    for i in range(P):\n",
        "        for j in range(T):\n",
        "            if Bin[i, j] > 0:\n",
        "                adj[i].append(t_node(j))\n",
        "                rev[t_node(j)].append(i)\n",
        "\n",
        "    for i in range(P):\n",
        "        for j in range(T):\n",
        "            if Bout[i, j] > 0:\n",
        "                adj[t_node(j)].append(i)\n",
        "                rev[i].append(t_node(j))\n",
        "\n",
        "    return adj, rev\n",
        "\n",
        "def tarjan_scc(adj):\n",
        "    index = 0\n",
        "    stack = []\n",
        "    onstack = [False]*len(adj)\n",
        "    idx = [-1]*len(adj)\n",
        "    low = [0]*len(adj)\n",
        "    sccs = []\n",
        "\n",
        "    def strongconnect(v):\n",
        "        nonlocal index\n",
        "        idx[v] = index\n",
        "        low[v] = index\n",
        "        index += 1\n",
        "        stack.append(v)\n",
        "        onstack[v] = True\n",
        "\n",
        "        for w in adj[v]:\n",
        "            if idx[w] == -1:\n",
        "                strongconnect(w)\n",
        "                low[v] = min(low[v], low[w])\n",
        "            elif onstack[w]:\n",
        "                low[v] = min(low[v], idx[w])\n",
        "\n",
        "        if low[v] == idx[v]:\n",
        "            comp = []\n",
        "            while True:\n",
        "                w = stack.pop()\n",
        "                onstack[w] = False\n",
        "                comp.append(w)\n",
        "                if w == v:\n",
        "                    break\n",
        "            sccs.append(comp)\n",
        "\n",
        "    for v in range(len(adj)):\n",
        "        if idx[v] == -1:\n",
        "            strongconnect(v)\n",
        "    return sccs\n",
        "\n",
        "def scc_has_cycle(comp, adj):\n",
        "    if len(comp) > 1:\n",
        "        return True\n",
        "    v = comp[0]\n",
        "    return v in adj[v]\n",
        "\n",
        "def scc_boundary_size(comp, adj, rev):\n",
        "    comp_set = set(comp)\n",
        "    out_edges = 0\n",
        "    in_edges = 0\n",
        "    for v in comp:\n",
        "        for w in adj[v]:\n",
        "            if w not in comp_set:\n",
        "                out_edges += 1\n",
        "        for u in rev[v]:\n",
        "            if u not in comp_set:\n",
        "                in_edges += 1\n",
        "    return in_edges, out_edges\n",
        "\n",
        "def baseline_transition_cover(B, t_idx, min_xt=1.0):\n",
        "    m, n = B.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B.astype(float)\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[t_idx] = (min_xt, None)\n",
        "    res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "    return res.success\n",
        "\n",
        "def dense_witness_simplex(B_sub, eps):\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = np.vstack([B_sub, np.ones((1, n))])\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[-1] = 1.0\n",
        "    bounds = [(eps, None)] * n\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "def cone_nonempty_simplex(B_sub):\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = np.vstack([B_sub, np.ones((1, n))])\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[-1] = 1.0\n",
        "    bounds = [(0, None)] * n\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "def targeted_cover_in_scc(B_full, t_global, t_indices, min_xt=1.0):\n",
        "    pos = t_indices.index(t_global)\n",
        "    B_sub = B_full[:, t_indices].astype(float)\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B_sub\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[pos] = (min_xt, None)\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "def support_of_witness(x, eps0=1e-12):\n",
        "    return [j for j, v in enumerate(x) if v > eps0]\n",
        "\n",
        "def run_dense_first_gict(pnml_file, eps=1e-6, fragile_k=3, bottleneck_boundary_k=1):\n",
        "    t0 = time.perf_counter()\n",
        "    places, transitions, arcs, ignored, Bin, Bout, B = parse_pnml_to_matrices(pnml_file)\n",
        "    t_parse = time.perf_counter() - t0\n",
        "\n",
        "    P, T = Bin.shape\n",
        "    approx_arcs = int((Bin > 0).sum() + (Bout > 0).sum())\n",
        "\n",
        "    print(\"\\n==\", pnml_file, \"==\")\n",
        "    print(\"Parsed PNML:\")\n",
        "    print(\"  #places:\", P)\n",
        "    print(\"  #transitions:\", T)\n",
        "    print(\"  arcs in file:\", len(arcs), \" ignored arcs:\", ignored)\n",
        "    print(\"  approx #nonzero arcs:\", approx_arcs)\n",
        "    print(\"  parse time (s):\", t_parse)\n",
        "\n",
        "    # Baseline\n",
        "    t0 = time.perf_counter()\n",
        "    base_flags = [baseline_transition_cover(B, j, 1.0) for j in range(T)]\n",
        "    t_baseline = time.perf_counter() - t0\n",
        "    base_uncovered = [j for j, ok in enumerate(base_flags) if not ok]\n",
        "    print(\"\\nBASELINE uncovered transitions (0-based):\", base_uncovered)\n",
        "    print(\"Baseline time (s):\", t_baseline)\n",
        "\n",
        "    # SCCs\n",
        "    adj, rev = build_bipartite_graph(Bin, Bout)\n",
        "    def t_node(j): return P + j\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    sccs = tarjan_scc(adj)\n",
        "    t_scc = time.perf_counter() - t0\n",
        "\n",
        "    scc_id = {}\n",
        "    for k, comp in enumerate(sccs):\n",
        "        for v in comp:\n",
        "            scc_id[v] = k\n",
        "    has_cycle = [scc_has_cycle(comp, adj) for comp in sccs]\n",
        "\n",
        "    labels = [\"Uncertain\"] * T\n",
        "\n",
        "    # Gate-0\n",
        "    t0 = time.perf_counter()\n",
        "    for j in range(T):\n",
        "        sid = scc_id[t_node(j)]\n",
        "        if not has_cycle[sid]:\n",
        "            labels[j] = \"StructurallyUncovered\"\n",
        "    t_gate0 = time.perf_counter() - t0\n",
        "\n",
        "    # Gate-2 per-SCC (dense-first, then cone test)\n",
        "    t0 = time.perf_counter()\n",
        "    lp_calls_scc = 0\n",
        "    lp_calls_targeted = 0\n",
        "    coneempty_sccs = 0\n",
        "    nondense_sccs = 0\n",
        "\n",
        "    for sid, comp in enumerate(sccs):\n",
        "        t_indices = [v - P for v in comp if v >= P]\n",
        "        if not t_indices:\n",
        "            continue\n",
        "\n",
        "        if not has_cycle[sid]:\n",
        "            for tj in t_indices:\n",
        "                labels[tj] = \"StructurallyUncovered\"\n",
        "            continue\n",
        "\n",
        "        inc, outc = scc_boundary_size(comp, adj, rev)\n",
        "        B_sub = B[:, t_indices].astype(float)\n",
        "\n",
        "        # Dense witness first\n",
        "        res_dense = dense_witness_simplex(B_sub, eps)\n",
        "        lp_calls_scc += 1\n",
        "\n",
        "        if res_dense.success:\n",
        "            x_sub = np.array(res_dense.x)\n",
        "            x_sub[np.abs(x_sub) < 1e-12] = 0.0\n",
        "\n",
        "            full = np.zeros(T, dtype=float)\n",
        "            for k, tj in enumerate(t_indices):\n",
        "                full[tj] = x_sub[k]\n",
        "\n",
        "            supp_size = len(support_of_witness(full))\n",
        "\n",
        "            for tj in t_indices:\n",
        "                if full[tj] <= 1e-12:\n",
        "                    continue\n",
        "                lab = \"CoveredWitness\"\n",
        "                if supp_size <= fragile_k:\n",
        "                    lab = \"FragileCovered\"\n",
        "                if inc <= bottleneck_boundary_k or outc <= bottleneck_boundary_k:\n",
        "                    lab = \"BottleneckLimited\"\n",
        "                labels[tj] = lab\n",
        "            continue\n",
        "\n",
        "        # Dense failed -> cone test\n",
        "        res_cone = cone_nonempty_simplex(B_sub)\n",
        "        lp_calls_scc += 1\n",
        "\n",
        "        if not res_cone.success:\n",
        "            coneempty_sccs += 1\n",
        "            for tj in t_indices:\n",
        "                labels[tj] = \"InvariantConeEmpty\"\n",
        "            continue\n",
        "\n",
        "        nondense_sccs += 1\n",
        "\n",
        "    t_gate2_scc = time.perf_counter() - t0\n",
        "\n",
        "    # Targeted fallback (only if any Uncertain remain)\n",
        "    t0 = time.perf_counter()\n",
        "    for j in range(T):\n",
        "        if labels[j] != \"Uncertain\":\n",
        "            continue\n",
        "        sid = scc_id[t_node(j)]\n",
        "        comp = sccs[sid]\n",
        "        t_indices = [v - P for v in comp if v >= P]\n",
        "        if not has_cycle[sid]:\n",
        "            labels[j] = \"StructurallyUncovered\"\n",
        "            continue\n",
        "        res = targeted_cover_in_scc(B, j, t_indices, min_xt=1.0)\n",
        "        lp_calls_targeted += 1\n",
        "        labels[j] = \"CoveredWitness\" if res.success else \"StructurallyUncovered\"\n",
        "    t_gate2_targeted = time.perf_counter() - t0\n",
        "\n",
        "    gict_uncovered = [j for j, lab in enumerate(labels) if lab in (\"StructurallyUncovered\", \"InvariantConeEmpty\")]\n",
        "\n",
        "    counts = defaultdict(int)\n",
        "    for lab in labels:\n",
        "        counts[lab] += 1\n",
        "\n",
        "    print(\"\\nGICT uncovered transitions (0-based):\", gict_uncovered)\n",
        "    print(\"\\nGICT label histogram:\")\n",
        "    for k in sorted(counts):\n",
        "        print(f\"  {k}: {counts[k]}\")\n",
        "\n",
        "    print(\"\\nGICT timings + LP counts:\")\n",
        "    print(\"  SCC time (s):\", t_scc)\n",
        "    print(\"  Gate-0 time (s):\", t_gate0)\n",
        "    print(\"  Gate-2 per-SCC time (s):\", t_gate2_scc)\n",
        "    print(\"  Gate-2 targeted time (s):\", t_gate2_targeted)\n",
        "    print(\"  LP calls (per-SCC):\", lp_calls_scc)\n",
        "    print(\"  LP calls (targeted):\", lp_calls_targeted)\n",
        "    print(\"  Total LP calls:\", lp_calls_scc + lp_calls_targeted)\n",
        "    print(\"  ConeEmpty SCCs:\", coneempty_sccs)\n",
        "    print(\"  NonDense-but-nonempty SCCs:\", nondense_sccs)\n",
        "\n",
        "    return {\n",
        "        \"P\": P, \"T\": T, \"arcs\": len(arcs),\n",
        "        \"baseline_uncovered_idx\": base_uncovered,\n",
        "        \"gict_uncovered_idx\": gict_uncovered,\n",
        "        \"labels\": dict(counts),\n",
        "        \"transitions\": transitions\n",
        "    }\n",
        "\n",
        "# ============================================================\n",
        "# 4) Run original vs guarded and report the guarded transition index\n",
        "# ============================================================\n",
        "r0 = run_dense_first_gict(PHILO_FILE)\n",
        "r1 = run_dense_first_gict(MUT_FILE)\n",
        "\n",
        "print(\"\\n============================\")\n",
        "print(\"DIFF SUMMARY (original -> guard-mutated)\")\n",
        "print(\"============================\")\n",
        "print(\"Original:  P=\", r0[\"P\"], \"T=\", r0[\"T\"], \"baseline_uncovered=\", len(r0[\"baseline_uncovered_idx\"]),\n",
        "      \"gict_uncovered=\", len(r0[\"gict_uncovered_idx\"]), \"labels=\", r0[\"labels\"])\n",
        "print(\"Guarded:   P=\", r1[\"P\"], \"T=\", r1[\"T\"], \"baseline_uncovered=\", len(r1[\"baseline_uncovered_idx\"]),\n",
        "      \"gict_uncovered=\", len(r1[\"gict_uncovered_idx\"]), \"labels=\", r1[\"labels\"])\n",
        "\n",
        "target_idx = None\n",
        "for i, tid in enumerate(r1[\"transitions\"]):\n",
        "    if tid == t_target_id:\n",
        "        target_idx = i\n",
        "        break\n",
        "\n",
        "print(\"\\nGuarded transition:\")\n",
        "print(\"  id:\", t_target_id)\n",
        "print(\"  index (0-based):\", target_idx)\n",
        "print(\"  baseline says uncovered?\", (target_idx in r1[\"baseline_uncovered_idx\"]))\n",
        "print(\"  GICT says uncovered?\", (target_idx in r1[\"gict_uncovered_idx\"]))\n",
        "print(\"  NOTE: This is a cyclic-SCC transition in the original net; the guard forces x[target]=0 in any nonnegative T-invariant.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6eJdggDu6S_",
        "outputId": "7135339b-0b47-43cf-fd8c-58769dea14d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PNML already present: philo.pnml\n",
            "\n",
            "Wrote guard-mutated PNML: philo_guard.pnml\n",
            "  Added place: p_guard (name=P_GUARD)\n",
            "  Added arc: a_guard source= p_guard -> target= cId183-i943123747\n",
            "  Guarded transition (target): cId183-i943123747\n",
            "\n",
            "== philo.pnml ==\n",
            "Parsed PNML:\n",
            "  #places: 30\n",
            "  #transitions: 30\n",
            "  arcs in file: 96  ignored arcs: 0\n",
            "  approx #nonzero arcs: 96\n",
            "  parse time (s): 0.0036955880004825303\n",
            "\n",
            "BASELINE uncovered transitions (0-based): []\n",
            "Baseline time (s): 0.13114500299980136\n",
            "\n",
            "GICT uncovered transitions (0-based): []\n",
            "\n",
            "GICT label histogram:\n",
            "  BottleneckLimited: 30\n",
            "\n",
            "GICT timings + LP counts:\n",
            "  SCC time (s): 0.00010192599984293338\n",
            "  Gate-0 time (s): 7.640000148967374e-06\n",
            "  Gate-2 per-SCC time (s): 0.005057432999819866\n",
            "  Gate-2 targeted time (s): 4.212000021652784e-06\n",
            "  LP calls (per-SCC): 1\n",
            "  LP calls (targeted): 0\n",
            "  Total LP calls: 1\n",
            "  ConeEmpty SCCs: 0\n",
            "  NonDense-but-nonempty SCCs: 0\n",
            "\n",
            "== philo_guard.pnml ==\n",
            "Parsed PNML:\n",
            "  #places: 31\n",
            "  #transitions: 30\n",
            "  arcs in file: 97  ignored arcs: 0\n",
            "  approx #nonzero arcs: 97\n",
            "  parse time (s): 0.007252718999552599\n",
            "\n",
            "BASELINE uncovered transitions (0-based): [0, 15]\n",
            "Baseline time (s): 0.1814628599995558\n",
            "\n",
            "GICT uncovered transitions (0-based): [0, 15]\n",
            "\n",
            "GICT label histogram:\n",
            "  CoveredWitness: 28\n",
            "  StructurallyUncovered: 2\n",
            "\n",
            "GICT timings + LP counts:\n",
            "  SCC time (s): 7.904999984020833e-05\n",
            "  Gate-0 time (s): 7.53599942981964e-06\n",
            "  Gate-2 per-SCC time (s): 0.01531837299990002\n",
            "  Gate-2 targeted time (s): 0.17305624399978115\n",
            "  LP calls (per-SCC): 2\n",
            "  LP calls (targeted): 30\n",
            "  Total LP calls: 32\n",
            "  ConeEmpty SCCs: 0\n",
            "  NonDense-but-nonempty SCCs: 1\n",
            "\n",
            "============================\n",
            "DIFF SUMMARY (original -> guard-mutated)\n",
            "============================\n",
            "Original:  P= 30 T= 30 baseline_uncovered= 0 gict_uncovered= 0 labels= {'BottleneckLimited': 30}\n",
            "Guarded:   P= 31 T= 30 baseline_uncovered= 2 gict_uncovered= 2 labels= {'StructurallyUncovered': 2, 'CoveredWitness': 28}\n",
            "\n",
            "Guarded transition:\n",
            "  id: cId183-i943123747\n",
            "  index (0-based): 0\n",
            "  baseline says uncovered? True\n",
            "  GICT says uncovered? True\n",
            "  NOTE: This is a cyclic-SCC transition in the original net; the guard forces x[target]=0 in any nonnegative T-invariant.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "# ============================================================\n",
        "# 0) Inputs\n",
        "# ============================================================\n",
        "PHILO_URL = \"https://www.pnml.org/version-2009/examples/philo.pnml\"\n",
        "PHILO_FILE = \"philo.pnml\"\n",
        "MUT_FILE = \"philo_guard.pnml\"\n",
        "\n",
        "# If you already created philo_guard.pnml earlier, this code will reuse it.\n",
        "# If not, it will create it with a P_GUARD place guarding the first transition.\n",
        "\n",
        "# ============================================================\n",
        "# 1) PNML helpers\n",
        "# ============================================================\n",
        "def strip_ns(tag):\n",
        "    return tag.split(\"}\", 1)[-1] if \"}\" in tag else tag\n",
        "\n",
        "def find_net_element(root):\n",
        "    for elem in root.iter():\n",
        "        if strip_ns(elem.tag) == \"net\":\n",
        "            return elem\n",
        "    return None\n",
        "\n",
        "def collect_all_ids(root):\n",
        "    ids = set()\n",
        "    for elem in root.iter():\n",
        "        if \"id\" in elem.attrib:\n",
        "            ids.add(elem.attrib[\"id\"])\n",
        "    return ids\n",
        "\n",
        "def unique_id(existing_ids, base):\n",
        "    if base not in existing_ids:\n",
        "        return base\n",
        "    k = 1\n",
        "    while f\"{base}_{k}\" in existing_ids:\n",
        "        k += 1\n",
        "    return f\"{base}_{k}\"\n",
        "\n",
        "def pick_first_transition_id(root):\n",
        "    for elem in root.iter():\n",
        "        if strip_ns(elem.tag) == \"transition\":\n",
        "            tid = elem.attrib.get(\"id\")\n",
        "            if tid:\n",
        "                return tid\n",
        "    return None\n",
        "\n",
        "def ensure_philo_downloaded():\n",
        "    if not os.path.exists(PHILO_FILE):\n",
        "        print(\"Downloading PNML:\", PHILO_URL)\n",
        "        urllib.request.urlretrieve(PHILO_URL, PHILO_FILE)\n",
        "    else:\n",
        "        print(\"PNML already present:\", PHILO_FILE)\n",
        "\n",
        "def make_guard_mutation_if_missing(pnml_in, pnml_out, guard_place_name=\"P_GUARD\", weight=1):\n",
        "    if os.path.exists(pnml_out):\n",
        "        print(\"Guard-mutated PNML already present:\", pnml_out)\n",
        "        return None, None, None\n",
        "\n",
        "    tree = ET.parse(pnml_in)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    net = find_net_element(root)\n",
        "    if net is None:\n",
        "        raise RuntimeError(\"Could not find <net> element in PNML.\")\n",
        "\n",
        "    t_target = pick_first_transition_id(root)\n",
        "    if t_target is None:\n",
        "        raise RuntimeError(\"Could not find any <transition> in PNML.\")\n",
        "\n",
        "    existing_ids = collect_all_ids(root)\n",
        "\n",
        "    p_id = unique_id(existing_ids, \"p_guard\")\n",
        "    existing_ids.add(p_id)\n",
        "\n",
        "    a_id = unique_id(existing_ids, \"a_guard\")\n",
        "    existing_ids.add(a_id)\n",
        "\n",
        "    # <place id=\"p_guard\"><name><text>P_GUARD</text></name></place>\n",
        "    p_elem = ET.Element(\"place\", {\"id\": p_id})\n",
        "    pname = ET.SubElement(p_elem, \"name\")\n",
        "    ptext = ET.SubElement(pname, \"text\")\n",
        "    ptext.text = guard_place_name\n",
        "\n",
        "    # <arc id=\"a_guard\" source=\"p_guard\" target=\"t_target\"><inscription><text>1</text></inscription></arc>\n",
        "    arc_elem = ET.Element(\"arc\", {\"id\": a_id, \"source\": p_id, \"target\": t_target})\n",
        "    ins = ET.SubElement(arc_elem, \"inscription\")\n",
        "    instext = ET.SubElement(ins, \"text\")\n",
        "    instext.text = str(int(weight))\n",
        "\n",
        "    net.append(p_elem)\n",
        "    net.append(arc_elem)\n",
        "\n",
        "    tree.write(pnml_out, encoding=\"utf-8\", xml_declaration=True)\n",
        "\n",
        "    print(\"\\nWrote guard-mutated PNML:\", pnml_out)\n",
        "    print(\"  Added place:\", p_id, \"(name=P_GUARD)\")\n",
        "    print(\"  Added arc:\", a_id, \"source=\", p_id, \"-> target=\", t_target)\n",
        "    print(\"  Guarded transition (target):\", t_target)\n",
        "    return p_id, a_id, t_target\n",
        "\n",
        "# ============================================================\n",
        "# 2) PNML -> incidence matrices\n",
        "# ============================================================\n",
        "def parse_pnml_to_matrices(pnml_file):\n",
        "    tree = ET.parse(pnml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    places = []\n",
        "    transitions = []\n",
        "    arcs = []\n",
        "\n",
        "    for elem in root.iter():\n",
        "        tag = strip_ns(elem.tag)\n",
        "        if tag == \"place\":\n",
        "            pid = elem.attrib.get(\"id\")\n",
        "            if pid:\n",
        "                places.append(pid)\n",
        "        elif tag == \"transition\":\n",
        "            tid = elem.attrib.get(\"id\")\n",
        "            if tid:\n",
        "                transitions.append(tid)\n",
        "        elif tag == \"arc\":\n",
        "            src = elem.attrib.get(\"source\")\n",
        "            tgt = elem.attrib.get(\"target\")\n",
        "            aid = elem.attrib.get(\"id\", \"\")\n",
        "            if src and tgt:\n",
        "                w = 1\n",
        "                for child in elem.iter():\n",
        "                    if strip_ns(child.tag) == \"inscription\":\n",
        "                        for g in child.iter():\n",
        "                            if strip_ns(g.tag) == \"text\" and g.text is not None:\n",
        "                                txt = g.text.strip()\n",
        "                                if txt:\n",
        "                                    try:\n",
        "                                        w = int(txt)\n",
        "                                    except:\n",
        "                                        pass\n",
        "                arcs.append((src, tgt, w, aid))\n",
        "\n",
        "    def uniq(seq):\n",
        "        seen = set()\n",
        "        out = []\n",
        "        for x in seq:\n",
        "            if x not in seen:\n",
        "                out.append(x)\n",
        "                seen.add(x)\n",
        "        return out\n",
        "\n",
        "    places = uniq([p for p in places if p is not None])\n",
        "    transitions = uniq([t for t in transitions if t is not None])\n",
        "\n",
        "    p_index = {p:i for i,p in enumerate(places)}\n",
        "    t_index = {t:i for i,t in enumerate(transitions)}\n",
        "\n",
        "    P = len(places)\n",
        "    T = len(transitions)\n",
        "\n",
        "    Bin = np.zeros((P, T), dtype=int)\n",
        "    Bout = np.zeros((P, T), dtype=int)\n",
        "\n",
        "    ignored = 0\n",
        "    for src, tgt, w, _aid in arcs:\n",
        "        if src in p_index and tgt in t_index:\n",
        "            Bin[p_index[src], t_index[tgt]] += w\n",
        "        elif src in t_index and tgt in p_index:\n",
        "            Bout[p_index[tgt], t_index[src]] += w\n",
        "        else:\n",
        "            ignored += 1\n",
        "\n",
        "    B = Bout - Bin\n",
        "    return places, transitions, arcs, ignored, Bin, Bout, B\n",
        "\n",
        "# ============================================================\n",
        "# 3) Graph + SCC\n",
        "# ============================================================\n",
        "def build_bipartite_graph(Bin, Bout):\n",
        "    P, T = Bin.shape\n",
        "    N = P + T\n",
        "    def t_node(j): return P + j\n",
        "\n",
        "    adj = [[] for _ in range(N)]\n",
        "    rev = [[] for _ in range(N)]\n",
        "\n",
        "    for i in range(P):\n",
        "        for j in range(T):\n",
        "            if Bin[i, j] > 0:\n",
        "                adj[i].append(t_node(j))\n",
        "                rev[t_node(j)].append(i)\n",
        "\n",
        "    for i in range(P):\n",
        "        for j in range(T):\n",
        "            if Bout[i, j] > 0:\n",
        "                adj[t_node(j)].append(i)\n",
        "                rev[i].append(t_node(j))\n",
        "\n",
        "    return adj, rev\n",
        "\n",
        "def tarjan_scc(adj):\n",
        "    index = 0\n",
        "    stack = []\n",
        "    onstack = [False]*len(adj)\n",
        "    idx = [-1]*len(adj)\n",
        "    low = [0]*len(adj)\n",
        "    sccs = []\n",
        "\n",
        "    def strongconnect(v):\n",
        "        nonlocal index\n",
        "        idx[v] = index\n",
        "        low[v] = index\n",
        "        index += 1\n",
        "        stack.append(v)\n",
        "        onstack[v] = True\n",
        "\n",
        "        for w in adj[v]:\n",
        "            if idx[w] == -1:\n",
        "                strongconnect(w)\n",
        "                low[v] = min(low[v], low[w])\n",
        "            elif onstack[w]:\n",
        "                low[v] = min(low[v], idx[w])\n",
        "\n",
        "        if low[v] == idx[v]:\n",
        "            comp = []\n",
        "            while True:\n",
        "                w = stack.pop()\n",
        "                onstack[w] = False\n",
        "                comp.append(w)\n",
        "                if w == v:\n",
        "                    break\n",
        "            sccs.append(comp)\n",
        "\n",
        "    for v in range(len(adj)):\n",
        "        if idx[v] == -1:\n",
        "            strongconnect(v)\n",
        "    return sccs\n",
        "\n",
        "def scc_has_cycle(comp, adj):\n",
        "    if len(comp) > 1:\n",
        "        return True\n",
        "    v = comp[0]\n",
        "    return v in adj[v]\n",
        "\n",
        "def scc_boundary_size(comp, adj, rev):\n",
        "    comp_set = set(comp)\n",
        "    out_edges = 0\n",
        "    in_edges = 0\n",
        "    for v in comp:\n",
        "        for w in adj[v]:\n",
        "            if w not in comp_set:\n",
        "                out_edges += 1\n",
        "        for u in rev[v]:\n",
        "            if u not in comp_set:\n",
        "                in_edges += 1\n",
        "    return in_edges, out_edges\n",
        "\n",
        "# ============================================================\n",
        "# 4) LP primitives\n",
        "# ============================================================\n",
        "def baseline_transition_cover(B, t_idx, min_xt=1.0):\n",
        "    m, n = B.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B.astype(float)\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[t_idx] = (min_xt, None)\n",
        "    res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "    return res.success\n",
        "\n",
        "def dense_witness_simplex(B_sub, eps):\n",
        "    # Feasibility: B_sub x = 0, x >= eps, sum(x)=1\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = np.vstack([B_sub, np.ones((1, n))])\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[-1] = 1.0\n",
        "    bounds = [(eps, None)] * n\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "def cone_nonempty_simplex(B_sub):\n",
        "    # Feasibility: B_sub x = 0, x >= 0, sum(x)=1\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = np.vstack([B_sub, np.ones((1, n))])\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[-1] = 1.0\n",
        "    bounds = [(0, None)] * n\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "def targeted_cover_in_scc(B_full, t_global, t_indices, min_xt=1.0):\n",
        "    pos = t_indices.index(t_global)\n",
        "    B_sub = B_full[:, t_indices].astype(float)\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B_sub\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[pos] = (min_xt, None)\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "# ============================================================\n",
        "# 5) GICT (dense-first + CONE-HEADMAN labeling)\n",
        "#     Key improvement: if dense fails but cone is nonempty, use the cone\n",
        "#     feasible solution as a *headman witness*:\n",
        "#       - label all transitions with x_j>0 as CoveredWitness immediately\n",
        "#       - only run targeted LP on x_j==0 transitions\n",
        "# ============================================================\n",
        "def run_gict_cone_headman(pnml_file, eps=1e-6, fragile_k=3, bottleneck_boundary_k=1, zero_tol=1e-12):\n",
        "    t0 = time.perf_counter()\n",
        "    places, transitions, arcs, ignored, Bin, Bout, B = parse_pnml_to_matrices(pnml_file)\n",
        "    t_parse = time.perf_counter() - t0\n",
        "\n",
        "    P, T = Bin.shape\n",
        "    approx_arcs = int((Bin > 0).sum() + (Bout > 0).sum())\n",
        "\n",
        "    print(\"\\n==\", pnml_file, \"==\")\n",
        "    print(\"Parsed PNML:\")\n",
        "    print(\"  #places:\", P)\n",
        "    print(\"  #transitions:\", T)\n",
        "    print(\"  arcs in file:\", len(arcs), \" ignored arcs:\", ignored)\n",
        "    print(\"  approx #nonzero arcs:\", approx_arcs)\n",
        "    print(\"  parse time (s):\", t_parse)\n",
        "\n",
        "    # Baseline\n",
        "    t0 = time.perf_counter()\n",
        "    base_flags = [baseline_transition_cover(B, j, 1.0) for j in range(T)]\n",
        "    t_baseline = time.perf_counter() - t0\n",
        "    base_uncovered = [j for j, ok in enumerate(base_flags) if not ok]\n",
        "\n",
        "    print(\"\\nBASELINE uncovered transitions (0-based):\", base_uncovered)\n",
        "    print(\"Baseline time (s):\", t_baseline)\n",
        "\n",
        "    # SCCs\n",
        "    adj, rev = build_bipartite_graph(Bin, Bout)\n",
        "    def t_node(j): return P + j\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    sccs = tarjan_scc(adj)\n",
        "    t_scc = time.perf_counter() - t0\n",
        "\n",
        "    scc_id = {}\n",
        "    for k, comp in enumerate(sccs):\n",
        "        for v in comp:\n",
        "            scc_id[v] = k\n",
        "    has_cycle = [scc_has_cycle(comp, adj) for comp in sccs]\n",
        "\n",
        "    labels = [\"Uncertain\"] * T\n",
        "\n",
        "    # Gate-0\n",
        "    t0 = time.perf_counter()\n",
        "    for j in range(T):\n",
        "        sid = scc_id[t_node(j)]\n",
        "        if not has_cycle[sid]:\n",
        "            labels[j] = \"StructurallyUncovered\"\n",
        "    t_gate0 = time.perf_counter() - t0\n",
        "\n",
        "    # Gate-2 per-SCC\n",
        "    t0 = time.perf_counter()\n",
        "    lp_calls_scc = 0\n",
        "    lp_calls_targeted = 0\n",
        "    coneempty_sccs = 0\n",
        "    nondense_sccs = 0\n",
        "    headman_labeled = 0\n",
        "\n",
        "    # Keep a list of (sid, t_indices, cone_solution_subvector) for SCCs that are non-dense but nonempty\n",
        "    headman_data = []\n",
        "\n",
        "    for sid, comp in enumerate(sccs):\n",
        "        t_indices = [v - P for v in comp if v >= P]\n",
        "        if not t_indices:\n",
        "            continue\n",
        "\n",
        "        if not has_cycle[sid]:\n",
        "            for tj in t_indices:\n",
        "                labels[tj] = \"StructurallyUncovered\"\n",
        "            continue\n",
        "\n",
        "        inc, outc = scc_boundary_size(comp, adj, rev)\n",
        "        B_sub = B[:, t_indices].astype(float)\n",
        "\n",
        "        # Dense first\n",
        "        res_dense = dense_witness_simplex(B_sub, eps)\n",
        "        lp_calls_scc += 1\n",
        "\n",
        "        if res_dense.success:\n",
        "            x_sub = np.array(res_dense.x, dtype=float)\n",
        "            x_sub[np.abs(x_sub) < zero_tol] = 0.0\n",
        "\n",
        "            # Use the dense witness to label all transitions in SCC as covered (they are all >= eps)\n",
        "            supp_size = int(np.sum(x_sub > zero_tol))\n",
        "            for k, tj in enumerate(t_indices):\n",
        "                if x_sub[k] <= zero_tol:\n",
        "                    continue\n",
        "                lab = \"CoveredWitness\"\n",
        "                if supp_size <= fragile_k:\n",
        "                    lab = \"FragileCovered\"\n",
        "                if inc <= bottleneck_boundary_k or outc <= bottleneck_boundary_k:\n",
        "                    lab = \"BottleneckLimited\"\n",
        "                labels[tj] = lab\n",
        "            continue\n",
        "\n",
        "        # Dense failed -> cone feasibility\n",
        "        res_cone = cone_nonempty_simplex(B_sub)\n",
        "        lp_calls_scc += 1\n",
        "\n",
        "        if not res_cone.success:\n",
        "            coneempty_sccs += 1\n",
        "            for tj in t_indices:\n",
        "                labels[tj] = \"InvariantConeEmpty\"\n",
        "            continue\n",
        "\n",
        "        # Cone is nonempty but not fully dense -> cone headman labeling\n",
        "        nondense_sccs += 1\n",
        "        x_sub = np.array(res_cone.x, dtype=float)\n",
        "        x_sub[np.abs(x_sub) < zero_tol] = 0.0\n",
        "\n",
        "        # Label positive entries as CoveredWitness immediately (headman witness)\n",
        "        for k, tj in enumerate(t_indices):\n",
        "            if x_sub[k] > zero_tol:\n",
        "                labels[tj] = \"CoveredWitness\"\n",
        "                headman_labeled += 1\n",
        "\n",
        "        # Store SCC data to target only zero entries later\n",
        "        headman_data.append((sid, t_indices, x_sub))\n",
        "\n",
        "    t_gate2_scc = time.perf_counter() - t0\n",
        "\n",
        "    # Targeted fallback ONLY for transitions still Uncertain.\n",
        "    # With cone headman labeling, this should usually be a tiny set.\n",
        "    t0 = time.perf_counter()\n",
        "    for j in range(T):\n",
        "        if labels[j] != \"Uncertain\":\n",
        "            continue\n",
        "\n",
        "        sid = scc_id[t_node(j)]\n",
        "        comp = sccs[sid]\n",
        "        t_indices = [v - P for v in comp if v >= P]\n",
        "\n",
        "        if not has_cycle[sid]:\n",
        "            labels[j] = \"StructurallyUncovered\"\n",
        "            continue\n",
        "\n",
        "        res = targeted_cover_in_scc(B, j, t_indices, min_xt=1.0)\n",
        "        lp_calls_targeted += 1\n",
        "        labels[j] = \"CoveredWitness\" if res.success else \"StructurallyUncovered\"\n",
        "\n",
        "    t_gate2_targeted = time.perf_counter() - t0\n",
        "\n",
        "    gict_uncovered = [j for j, lab in enumerate(labels) if lab in (\"StructurallyUncovered\", \"InvariantConeEmpty\")]\n",
        "\n",
        "    counts = defaultdict(int)\n",
        "    for lab in labels:\n",
        "        counts[lab] += 1\n",
        "\n",
        "    print(\"\\nGICT uncovered transitions (0-based):\", gict_uncovered)\n",
        "    print(\"\\nGICT label histogram:\")\n",
        "    for k in sorted(counts):\n",
        "        print(f\"  {k}: {counts[k]}\")\n",
        "\n",
        "    print(\"\\nGICT timings + LP counts:\")\n",
        "    print(\"  SCC time (s):\", t_scc)\n",
        "    print(\"  Gate-0 time (s):\", t_gate0)\n",
        "    print(\"  Gate-2 per-SCC time (s):\", t_gate2_scc)\n",
        "    print(\"  Gate-2 targeted time (s):\", t_gate2_targeted)\n",
        "    print(\"  LP calls (per-SCC):\", lp_calls_scc)\n",
        "    print(\"  LP calls (targeted):\", lp_calls_targeted)\n",
        "    print(\"  Total LP calls:\", lp_calls_scc + lp_calls_targeted)\n",
        "    print(\"  ConeEmpty SCCs:\", coneempty_sccs)\n",
        "    print(\"  NonDense-but-nonempty SCCs:\", nondense_sccs)\n",
        "    print(\"  Cone-headman labeled (count):\", headman_labeled)\n",
        "\n",
        "    return {\n",
        "        \"P\": P,\n",
        "        \"T\": T,\n",
        "        \"arcs\": len(arcs),\n",
        "        \"transitions\": transitions,\n",
        "        \"baseline_uncovered_idx\": base_uncovered,\n",
        "        \"gict_uncovered_idx\": gict_uncovered,\n",
        "        \"labels\": dict(counts),\n",
        "        \"lp_scc\": lp_calls_scc,\n",
        "        \"lp_targeted\": lp_calls_targeted,\n",
        "        \"lp_total\": lp_calls_scc + lp_calls_targeted,\n",
        "    }\n",
        "\n",
        "# ============================================================\n",
        "# 6) Run: original philo vs guard-mutated (with cone headman)\n",
        "# ============================================================\n",
        "ensure_philo_downloaded()\n",
        "_p, _a, t_target = make_guard_mutation_if_missing(PHILO_FILE, MUT_FILE, guard_place_name=\"P_GUARD\", weight=1)\n",
        "\n",
        "r0 = run_gict_cone_headman(PHILO_FILE)\n",
        "r1 = run_gict_cone_headman(MUT_FILE)\n",
        "\n",
        "print(\"\\n============================\")\n",
        "print(\"DIFF SUMMARY (original -> guard-mutated) [Cone Headman]\")\n",
        "print(\"============================\")\n",
        "print(\"Original: P=\", r0[\"P\"], \"T=\", r0[\"T\"], \"baseline_uncovered=\", len(r0[\"baseline_uncovered_idx\"]),\n",
        "      \"gict_uncovered=\", len(r0[\"gict_uncovered_idx\"]), \"LP total=\", r0[\"lp_total\"], \"labels=\", r0[\"labels\"])\n",
        "print(\"Guarded:  P=\", r1[\"P\"], \"T=\", r1[\"T\"], \"baseline_uncovered=\", len(r1[\"baseline_uncovered_idx\"]),\n",
        "      \"gict_uncovered=\", len(r1[\"gict_uncovered_idx\"]), \"LP total=\", r1[\"lp_total\"], \"labels=\", r1[\"labels\"])\n",
        "\n",
        "# If we created the mutation in this run, report the guarded transition index.\n",
        "if t_target is not None:\n",
        "    target_idx = None\n",
        "    for i, tid in enumerate(r1[\"transitions\"]):\n",
        "        if tid == t_target:\n",
        "            target_idx = i\n",
        "            break\n",
        "    print(\"\\nGuarded transition (from this run):\")\n",
        "    print(\"  id:\", t_target)\n",
        "    print(\"  index (0-based):\", target_idx)\n",
        "    print(\"  baseline says uncovered?\", (target_idx in r1[\"baseline_uncovered_idx\"]))\n",
        "    print(\"  GICT says uncovered?\", (target_idx in r1[\"gict_uncovered_idx\"]))\n",
        "else:\n",
        "    print(\"\\nNote: Guarded PNML already existed, so this run did not recreate it (guard target ID not re-reported here).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uau4H2Kv_so",
        "outputId": "9b7277a1-6c1c-4898-da97-6f4e4cc13dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PNML already present: philo.pnml\n",
            "Guard-mutated PNML already present: philo_guard.pnml\n",
            "\n",
            "== philo.pnml ==\n",
            "Parsed PNML:\n",
            "  #places: 30\n",
            "  #transitions: 30\n",
            "  arcs in file: 96  ignored arcs: 0\n",
            "  approx #nonzero arcs: 96\n",
            "  parse time (s): 0.003987251000580727\n",
            "\n",
            "BASELINE uncovered transitions (0-based): []\n",
            "Baseline time (s): 0.21021067700075946\n",
            "\n",
            "GICT uncovered transitions (0-based): []\n",
            "\n",
            "GICT label histogram:\n",
            "  BottleneckLimited: 30\n",
            "\n",
            "GICT timings + LP counts:\n",
            "  SCC time (s): 8.954699933383381e-05\n",
            "  Gate-0 time (s): 7.353000000875909e-06\n",
            "  Gate-2 per-SCC time (s): 0.004198593000182882\n",
            "  Gate-2 targeted time (s): 3.7669997254852206e-06\n",
            "  LP calls (per-SCC): 1\n",
            "  LP calls (targeted): 0\n",
            "  Total LP calls: 1\n",
            "  ConeEmpty SCCs: 0\n",
            "  NonDense-but-nonempty SCCs: 0\n",
            "  Cone-headman labeled (count): 0\n",
            "\n",
            "== philo_guard.pnml ==\n",
            "Parsed PNML:\n",
            "  #places: 31\n",
            "  #transitions: 30\n",
            "  arcs in file: 97  ignored arcs: 0\n",
            "  approx #nonzero arcs: 97\n",
            "  parse time (s): 0.006144190999293642\n",
            "\n",
            "BASELINE uncovered transitions (0-based): [0, 15]\n",
            "Baseline time (s): 0.16278371599946695\n",
            "\n",
            "GICT uncovered transitions (0-based): [0, 15]\n",
            "\n",
            "GICT label histogram:\n",
            "  CoveredWitness: 28\n",
            "  StructurallyUncovered: 2\n",
            "\n",
            "GICT timings + LP counts:\n",
            "  SCC time (s): 9.962700005416991e-05\n",
            "  Gate-0 time (s): 1.085700023395475e-05\n",
            "  Gate-2 per-SCC time (s): 0.015713623000010557\n",
            "  Gate-2 targeted time (s): 0.32167366900011984\n",
            "  LP calls (per-SCC): 2\n",
            "  LP calls (targeted): 27\n",
            "  Total LP calls: 29\n",
            "  ConeEmpty SCCs: 0\n",
            "  NonDense-but-nonempty SCCs: 1\n",
            "  Cone-headman labeled (count): 3\n",
            "\n",
            "============================\n",
            "DIFF SUMMARY (original -> guard-mutated) [Cone Headman]\n",
            "============================\n",
            "Original: P= 30 T= 30 baseline_uncovered= 0 gict_uncovered= 0 LP total= 1 labels= {'BottleneckLimited': 30}\n",
            "Guarded:  P= 31 T= 30 baseline_uncovered= 2 gict_uncovered= 2 LP total= 29 labels= {'StructurallyUncovered': 2, 'CoveredWitness': 28}\n",
            "\n",
            "Note: Guarded PNML already existed, so this run did not recreate it (guard target ID not re-reported here).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "# ============================================================\n",
        "# 0) Inputs\n",
        "# ============================================================\n",
        "PHILO_URL = \"https://www.pnml.org/version-2009/examples/philo.pnml\"\n",
        "PHILO_FILE = \"philo.pnml\"\n",
        "GUARD_FILE = \"philo_guard.pnml\"   # you already have this\n",
        "\n",
        "# ============================================================\n",
        "# 1) PNML helpers\n",
        "# ============================================================\n",
        "def strip_ns(tag):\n",
        "    return tag.split(\"}\", 1)[-1] if \"}\" in tag else tag\n",
        "\n",
        "def ensure_philo_downloaded():\n",
        "    if not os.path.exists(PHILO_FILE):\n",
        "        print(\"Downloading PNML:\", PHILO_URL)\n",
        "        urllib.request.urlretrieve(PHILO_URL, PHILO_FILE)\n",
        "    else:\n",
        "        print(\"PNML already present:\", PHILO_FILE)\n",
        "\n",
        "def parse_pnml_to_matrices(pnml_file):\n",
        "    tree = ET.parse(pnml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    places = []\n",
        "    transitions = []\n",
        "    arcs = []\n",
        "\n",
        "    for elem in root.iter():\n",
        "        tag = strip_ns(elem.tag)\n",
        "        if tag == \"place\":\n",
        "            pid = elem.attrib.get(\"id\")\n",
        "            if pid:\n",
        "                places.append(pid)\n",
        "        elif tag == \"transition\":\n",
        "            tid = elem.attrib.get(\"id\")\n",
        "            if tid:\n",
        "                transitions.append(tid)\n",
        "        elif tag == \"arc\":\n",
        "            src = elem.attrib.get(\"source\")\n",
        "            tgt = elem.attrib.get(\"target\")\n",
        "            aid = elem.attrib.get(\"id\", \"\")\n",
        "            if src and tgt:\n",
        "                w = 1\n",
        "                for child in elem.iter():\n",
        "                    if strip_ns(child.tag) == \"inscription\":\n",
        "                        for g in child.iter():\n",
        "                            if strip_ns(g.tag) == \"text\" and g.text is not None:\n",
        "                                txt = g.text.strip()\n",
        "                                if txt:\n",
        "                                    try:\n",
        "                                        w = int(txt)\n",
        "                                    except:\n",
        "                                        pass\n",
        "                arcs.append((src, tgt, w, aid))\n",
        "\n",
        "    def uniq(seq):\n",
        "        seen = set()\n",
        "        out = []\n",
        "        for x in seq:\n",
        "            if x not in seen:\n",
        "                out.append(x)\n",
        "                seen.add(x)\n",
        "        return out\n",
        "\n",
        "    places = uniq([p for p in places if p is not None])\n",
        "    transitions = uniq([t for t in transitions if t is not None])\n",
        "\n",
        "    p_index = {p:i for i,p in enumerate(places)}\n",
        "    t_index = {t:i for i,t in enumerate(transitions)}\n",
        "\n",
        "    P = len(places)\n",
        "    T = len(transitions)\n",
        "\n",
        "    Bin = np.zeros((P, T), dtype=int)\n",
        "    Bout = np.zeros((P, T), dtype=int)\n",
        "\n",
        "    ignored = 0\n",
        "    for src, tgt, w, _aid in arcs:\n",
        "        if src in p_index and tgt in t_index:\n",
        "            Bin[p_index[src], t_index[tgt]] += w\n",
        "        elif src in t_index and tgt in p_index:\n",
        "            Bout[p_index[tgt], t_index[src]] += w\n",
        "        else:\n",
        "            ignored += 1\n",
        "\n",
        "    B = Bout - Bin\n",
        "    return places, transitions, arcs, ignored, Bin, Bout, B\n",
        "\n",
        "# ============================================================\n",
        "# 2) SCC + graph\n",
        "# ============================================================\n",
        "def build_bipartite_graph(Bin, Bout):\n",
        "    P, T = Bin.shape\n",
        "    N = P + T\n",
        "    def t_node(j): return P + j\n",
        "\n",
        "    adj = [[] for _ in range(N)]\n",
        "    rev = [[] for _ in range(N)]\n",
        "\n",
        "    for i in range(P):\n",
        "        for j in range(T):\n",
        "            if Bin[i, j] > 0:\n",
        "                adj[i].append(t_node(j))\n",
        "                rev[t_node(j)].append(i)\n",
        "\n",
        "    for i in range(P):\n",
        "        for j in range(T):\n",
        "            if Bout[i, j] > 0:\n",
        "                adj[t_node(j)].append(i)\n",
        "                rev[i].append(t_node(j))\n",
        "\n",
        "    return adj, rev\n",
        "\n",
        "def tarjan_scc(adj):\n",
        "    index = 0\n",
        "    stack = []\n",
        "    onstack = [False]*len(adj)\n",
        "    idx = [-1]*len(adj)\n",
        "    low = [0]*len(adj)\n",
        "    sccs = []\n",
        "\n",
        "    def strongconnect(v):\n",
        "        nonlocal index\n",
        "        idx[v] = index\n",
        "        low[v] = index\n",
        "        index += 1\n",
        "        stack.append(v)\n",
        "        onstack[v] = True\n",
        "\n",
        "        for w in adj[v]:\n",
        "            if idx[w] == -1:\n",
        "                strongconnect(w)\n",
        "                low[v] = min(low[v], low[w])\n",
        "            elif onstack[w]:\n",
        "                low[v] = min(low[v], idx[w])\n",
        "\n",
        "        if low[v] == idx[v]:\n",
        "            comp = []\n",
        "            while True:\n",
        "                w = stack.pop()\n",
        "                onstack[w] = False\n",
        "                comp.append(w)\n",
        "                if w == v:\n",
        "                    break\n",
        "            sccs.append(comp)\n",
        "\n",
        "    for v in range(len(adj)):\n",
        "        if idx[v] == -1:\n",
        "            strongconnect(v)\n",
        "    return sccs\n",
        "\n",
        "def scc_has_cycle(comp, adj):\n",
        "    if len(comp) > 1:\n",
        "        return True\n",
        "    v = comp[0]\n",
        "    return v in adj[v]\n",
        "\n",
        "def scc_boundary_size(comp, adj, rev):\n",
        "    comp_set = set(comp)\n",
        "    out_edges = 0\n",
        "    in_edges = 0\n",
        "    for v in comp:\n",
        "        for w in adj[v]:\n",
        "            if w not in comp_set:\n",
        "                out_edges += 1\n",
        "        for u in rev[v]:\n",
        "            if u not in comp_set:\n",
        "                in_edges += 1\n",
        "    return in_edges, out_edges\n",
        "\n",
        "# ============================================================\n",
        "# 3) LP primitives\n",
        "# ============================================================\n",
        "def baseline_transition_cover(B, t_idx, min_xt=1.0):\n",
        "    m, n = B.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B.astype(float)\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[t_idx] = (min_xt, None)\n",
        "    res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "    return res.success\n",
        "\n",
        "def dense_witness_simplex(B_sub, eps):\n",
        "    # Feasibility: B_sub x = 0, x >= eps, sum(x)=1\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = np.vstack([B_sub, np.ones((1, n))])\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[-1] = 1.0\n",
        "    bounds = [(eps, None)] * n\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "def cone_nonempty_simplex(B_sub):\n",
        "    # Feasibility: B_sub x = 0, x >= 0, sum(x)=1\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = np.vstack([B_sub, np.ones((1, n))])\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[-1] = 1.0\n",
        "    bounds = [(0, None)] * n\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "def targeted_cover_in_scc(B_full, t_global, t_indices, min_xt=1.0):\n",
        "    pos = t_indices.index(t_global)\n",
        "    B_sub = B_full[:, t_indices].astype(float)\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B_sub\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[pos] = (min_xt, None)\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "# ============================================================\n",
        "# 4) Max-dense headman LP (linear)\n",
        "#     Maximize delta subject to:\n",
        "#       B_sub x = 0\n",
        "#       sum(x)=1\n",
        "#       x_i >= delta  (for all i in SCC)\n",
        "#       x >= 0, delta >= 0\n",
        "#     If optimum delta > 0 => fully dense exists (rare here, but then we're done).\n",
        "#     If optimum delta == 0 => returns an \"as-dense-as-possible\" headman witness.\n",
        "# ============================================================\n",
        "def max_dense_headman(B_sub):\n",
        "    m, n = B_sub.shape\n",
        "\n",
        "    # Decision variables: [x0..x_{n-1}, delta]  => dimension n+1\n",
        "    # Objective: maximize delta => minimize -delta\n",
        "    c = np.zeros(n + 1)\n",
        "    c[-1] = -1.0\n",
        "\n",
        "    # Equalities: B_sub x = 0, sum(x)=1\n",
        "    A_eq = np.zeros((m + 1, n + 1))\n",
        "    A_eq[:m, :n] = B_sub\n",
        "    A_eq[m, :n] = 1.0\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[m] = 1.0\n",
        "\n",
        "    # Inequalities: x_i - delta >= 0  =>  -x_i + delta <= 0\n",
        "    A_ub = np.zeros((n, n + 1))\n",
        "    b_ub = np.zeros(n)\n",
        "    for i in range(n):\n",
        "        A_ub[i, i] = -1.0\n",
        "        A_ub[i, -1] = 1.0\n",
        "\n",
        "    # Bounds: x_i >= 0, delta >= 0\n",
        "    bounds = [(0, None)] * n + [(0, None)]\n",
        "\n",
        "    res = linprog(c, A_eq=A_eq, b_eq=b_eq, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method=\"highs\")\n",
        "    return res\n",
        "\n",
        "# ============================================================\n",
        "# 5) GICT with Max-Dense Headman\n",
        "# ============================================================\n",
        "def run_gict_max_dense_headman(pnml_file, eps=1e-6, zero_tol=1e-12, fragile_k=3, bottleneck_boundary_k=1):\n",
        "    t0 = time.perf_counter()\n",
        "    places, transitions, arcs, ignored, Bin, Bout, B = parse_pnml_to_matrices(pnml_file)\n",
        "    t_parse = time.perf_counter() - t0\n",
        "\n",
        "    P, T = Bin.shape\n",
        "    approx_arcs = int((Bin > 0).sum() + (Bout > 0).sum())\n",
        "\n",
        "    print(\"\\n==\", pnml_file, \"==\")\n",
        "    print(\"Parsed PNML:\")\n",
        "    print(\"  #places:\", P)\n",
        "    print(\"  #transitions:\", T)\n",
        "    print(\"  arcs in file:\", len(arcs), \" ignored arcs:\", ignored)\n",
        "    print(\"  approx #nonzero arcs:\", approx_arcs)\n",
        "    print(\"  parse time (s):\", t_parse)\n",
        "\n",
        "    # Baseline\n",
        "    t0 = time.perf_counter()\n",
        "    base_flags = [baseline_transition_cover(B, j, 1.0) for j in range(T)]\n",
        "    t_baseline = time.perf_counter() - t0\n",
        "    base_uncovered = [j for j, ok in enumerate(base_flags) if not ok]\n",
        "    print(\"\\nBASELINE uncovered transitions (0-based):\", base_uncovered)\n",
        "    print(\"Baseline time (s):\", t_baseline)\n",
        "\n",
        "    # SCC\n",
        "    adj, rev = build_bipartite_graph(Bin, Bout)\n",
        "    def t_node(j): return P + j\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    sccs = tarjan_scc(adj)\n",
        "    t_scc = time.perf_counter() - t0\n",
        "\n",
        "    scc_id = {}\n",
        "    for k, comp in enumerate(sccs):\n",
        "        for v in comp:\n",
        "            scc_id[v] = k\n",
        "    has_cycle = [scc_has_cycle(comp, adj) for comp in sccs]\n",
        "\n",
        "    labels = [\"Uncertain\"] * T\n",
        "\n",
        "    # Gate-0\n",
        "    t0 = time.perf_counter()\n",
        "    for j in range(T):\n",
        "        sid = scc_id[t_node(j)]\n",
        "        if not has_cycle[sid]:\n",
        "            labels[j] = \"StructurallyUncovered\"\n",
        "    t_gate0 = time.perf_counter() - t0\n",
        "\n",
        "    # Gate-2 per-SCC (dense test -> if fail, headman max-delta -> then targeted only for headman-zeros)\n",
        "    t0 = time.perf_counter()\n",
        "    lp_calls_scc = 0\n",
        "    lp_calls_targeted = 0\n",
        "    coneempty_sccs = 0\n",
        "    nondense_sccs = 0\n",
        "\n",
        "    headman_total_support = 0\n",
        "    headman_scc_count = 0\n",
        "    headman_delta_values = []\n",
        "\n",
        "    # To target only zeros, we track which transitions were labeled by headman positivity\n",
        "    # Any remaining Uncertain after SCC pass gets targeted LP.\n",
        "    for sid, comp in enumerate(sccs):\n",
        "        t_indices = [v - P for v in comp if v >= P]\n",
        "        if not t_indices:\n",
        "            continue\n",
        "\n",
        "        if not has_cycle[sid]:\n",
        "            for tj in t_indices:\n",
        "                labels[tj] = \"StructurallyUncovered\"\n",
        "            continue\n",
        "\n",
        "        inc, outc = scc_boundary_size(comp, adj, rev)\n",
        "        B_sub = B[:, t_indices].astype(float)\n",
        "\n",
        "        # First try true dense feasibility with eps\n",
        "        res_dense = dense_witness_simplex(B_sub, eps)\n",
        "        lp_calls_scc += 1\n",
        "\n",
        "        if res_dense.success:\n",
        "            x_sub = np.array(res_dense.x, dtype=float)\n",
        "            x_sub[np.abs(x_sub) < zero_tol] = 0.0\n",
        "            supp_size = int(np.sum(x_sub > zero_tol))\n",
        "\n",
        "            for k, tj in enumerate(t_indices):\n",
        "                if x_sub[k] <= zero_tol:\n",
        "                    continue\n",
        "                lab = \"CoveredWitness\"\n",
        "                if supp_size <= fragile_k:\n",
        "                    lab = \"FragileCovered\"\n",
        "                if inc <= bottleneck_boundary_k or outc <= bottleneck_boundary_k:\n",
        "                    lab = \"BottleneckLimited\"\n",
        "                labels[tj] = lab\n",
        "            continue\n",
        "\n",
        "        # Dense failed: check cone nonempty quickly (so we can label cone-empty SCCs instantly)\n",
        "        res_cone = cone_nonempty_simplex(B_sub)\n",
        "        lp_calls_scc += 1\n",
        "        if not res_cone.success:\n",
        "            coneempty_sccs += 1\n",
        "            for tj in t_indices:\n",
        "                labels[tj] = \"InvariantConeEmpty\"\n",
        "            continue\n",
        "\n",
        "        # Cone nonempty but not dense: run max-dense headman (one LP)\n",
        "        nondense_sccs += 1\n",
        "        res_head = max_dense_headman(B_sub)\n",
        "        lp_calls_scc += 1\n",
        "\n",
        "        if not res_head.success:\n",
        "            # Fallback: if headman LP fails unexpectedly, do nothing here;\n",
        "            # targeted stage will classify remaining Uncertain transitions.\n",
        "            continue\n",
        "\n",
        "        xdelta = np.array(res_head.x, dtype=float)\n",
        "        x_sub = xdelta[:-1]\n",
        "        delta = float(xdelta[-1])\n",
        "        x_sub[np.abs(x_sub) < zero_tol] = 0.0\n",
        "\n",
        "        headman_scc_count += 1\n",
        "        headman_delta_values.append(delta)\n",
        "\n",
        "        supp = (x_sub > zero_tol)\n",
        "        supp_size = int(np.sum(supp))\n",
        "        headman_total_support += supp_size\n",
        "\n",
        "        # Label positive components immediately as CoveredWitness\n",
        "        for k, tj in enumerate(t_indices):\n",
        "            if x_sub[k] > zero_tol:\n",
        "                labels[tj] = \"CoveredWitness\"\n",
        "\n",
        "    t_gate2_scc = time.perf_counter() - t0\n",
        "\n",
        "    # Target only remaining Uncertain\n",
        "    t0 = time.perf_counter()\n",
        "    for j in range(T):\n",
        "        if labels[j] != \"Uncertain\":\n",
        "            continue\n",
        "\n",
        "        sid = scc_id[t_node(j)]\n",
        "        comp = sccs[sid]\n",
        "        t_indices = [v - P for v in comp if v >= P]\n",
        "\n",
        "        if not has_cycle[sid]:\n",
        "            labels[j] = \"StructurallyUncovered\"\n",
        "            continue\n",
        "\n",
        "        res = targeted_cover_in_scc(B, j, t_indices, min_xt=1.0)\n",
        "        lp_calls_targeted += 1\n",
        "        labels[j] = \"CoveredWitness\" if res.success else \"StructurallyUncovered\"\n",
        "\n",
        "    t_gate2_targeted = time.perf_counter() - t0\n",
        "\n",
        "    gict_uncovered = [j for j, lab in enumerate(labels) if lab in (\"StructurallyUncovered\", \"InvariantConeEmpty\")]\n",
        "\n",
        "    counts = defaultdict(int)\n",
        "    for lab in labels:\n",
        "        counts[lab] += 1\n",
        "\n",
        "    print(\"\\nGICT uncovered transitions (0-based):\", gict_uncovered)\n",
        "    print(\"\\nGICT label histogram:\")\n",
        "    for k in sorted(counts):\n",
        "        print(f\"  {k}: {counts[k]}\")\n",
        "\n",
        "    print(\"\\nGICT timings + LP counts:\")\n",
        "    print(\"  SCC time (s):\", t_scc)\n",
        "    print(\"  Gate-0 time (s):\", t_gate0)\n",
        "    print(\"  Gate-2 per-SCC time (s):\", t_gate2_scc)\n",
        "    print(\"  Gate-2 targeted time (s):\", t_gate2_targeted)\n",
        "    print(\"  LP calls (per-SCC):\", lp_calls_scc)\n",
        "    print(\"  LP calls (targeted):\", lp_calls_targeted)\n",
        "    print(\"  Total LP calls:\", lp_calls_scc + lp_calls_targeted)\n",
        "    print(\"  ConeEmpty SCCs:\", coneempty_sccs)\n",
        "    print(\"  NonDense-but-nonempty SCCs:\", nondense_sccs)\n",
        "\n",
        "    if headman_scc_count > 0:\n",
        "        avg_supp = headman_total_support / headman_scc_count\n",
        "        print(\"\\nHeadman stats:\")\n",
        "        print(\"  headman SCCs:\", headman_scc_count)\n",
        "        print(\"  avg headman support size:\", avg_supp)\n",
        "        print(\"  headman delta values (per SCC):\", headman_delta_values)\n",
        "\n",
        "    return {\n",
        "        \"P\": P,\n",
        "        \"T\": T,\n",
        "        \"arcs\": len(arcs),\n",
        "        \"transitions\": transitions,\n",
        "        \"baseline_uncovered_idx\": base_uncovered,\n",
        "        \"gict_uncovered_idx\": gict_uncovered,\n",
        "        \"labels\": dict(counts),\n",
        "        \"lp_scc\": lp_calls_scc,\n",
        "        \"lp_targeted\": lp_calls_targeted,\n",
        "        \"lp_total\": lp_calls_scc + lp_calls_targeted,\n",
        "    }\n",
        "\n",
        "# ============================================================\n",
        "# 6) Run philo + philo_guard\n",
        "# ============================================================\n",
        "ensure_philo_downloaded()\n",
        "if not os.path.exists(GUARD_FILE):\n",
        "    raise RuntimeError(\"philo_guard.pnml not found. Create it first (your earlier guard mutation already did).\")\n",
        "else:\n",
        "    print(\"PNML already present:\", GUARD_FILE)\n",
        "\n",
        "r0 = run_gict_max_dense_headman(PHILO_FILE)\n",
        "r1 = run_gict_max_dense_headman(GUARD_FILE)\n",
        "\n",
        "print(\"\\n============================\")\n",
        "print(\"DIFF SUMMARY [Max-Dense Headman]\")\n",
        "print(\"============================\")\n",
        "print(\"Original: P=\", r0[\"P\"], \"T=\", r0[\"T\"],\n",
        "      \"baseline_uncovered=\", len(r0[\"baseline_uncovered_idx\"]),\n",
        "      \"gict_uncovered=\", len(r0[\"gict_uncovered_idx\"]),\n",
        "      \"LP total=\", r0[\"lp_total\"], \"labels=\", r0[\"labels\"])\n",
        "print(\"Guarded:  P=\", r1[\"P\"], \"T=\", r1[\"T\"],\n",
        "      \"baseline_uncovered=\", len(r1[\"baseline_uncovered_idx\"]),\n",
        "      \"gict_uncovered=\", len(r1[\"gict_uncovered_idx\"]),\n",
        "      \"LP total=\", r1[\"lp_total\"], \"labels=\", r1[\"labels\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2Cfcby5xFsK",
        "outputId": "41428317-fadd-428a-c5e2-d8714cc23983"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PNML already present: philo.pnml\n",
            "PNML already present: philo_guard.pnml\n",
            "\n",
            "== philo.pnml ==\n",
            "Parsed PNML:\n",
            "  #places: 30\n",
            "  #transitions: 30\n",
            "  arcs in file: 96  ignored arcs: 0\n",
            "  approx #nonzero arcs: 96\n",
            "  parse time (s): 0.005628034999972442\n",
            "\n",
            "BASELINE uncovered transitions (0-based): []\n",
            "Baseline time (s): 0.1731824200005576\n",
            "\n",
            "GICT uncovered transitions (0-based): []\n",
            "\n",
            "GICT label histogram:\n",
            "  BottleneckLimited: 30\n",
            "\n",
            "GICT timings + LP counts:\n",
            "  SCC time (s): 0.00011329600056342315\n",
            "  Gate-0 time (s): 8.564999916416127e-06\n",
            "  Gate-2 per-SCC time (s): 0.0042289530001653475\n",
            "  Gate-2 targeted time (s): 4.45099976786878e-06\n",
            "  LP calls (per-SCC): 1\n",
            "  LP calls (targeted): 0\n",
            "  Total LP calls: 1\n",
            "  ConeEmpty SCCs: 0\n",
            "  NonDense-but-nonempty SCCs: 0\n",
            "\n",
            "== philo_guard.pnml ==\n",
            "Parsed PNML:\n",
            "  #places: 31\n",
            "  #transitions: 30\n",
            "  arcs in file: 97  ignored arcs: 0\n",
            "  approx #nonzero arcs: 97\n",
            "  parse time (s): 0.004739727000014682\n",
            "\n",
            "BASELINE uncovered transitions (0-based): [0, 15]\n",
            "Baseline time (s): 0.20161633300085668\n",
            "\n",
            "GICT uncovered transitions (0-based): [0, 15]\n",
            "\n",
            "GICT label histogram:\n",
            "  CoveredWitness: 28\n",
            "  StructurallyUncovered: 2\n",
            "\n",
            "GICT timings + LP counts:\n",
            "  SCC time (s): 0.00010085199937748257\n",
            "  Gate-0 time (s): 9.21400078368606e-06\n",
            "  Gate-2 per-SCC time (s): 0.03746654399947147\n",
            "  Gate-2 targeted time (s): 0.19338522599991848\n",
            "  LP calls (per-SCC): 3\n",
            "  LP calls (targeted): 27\n",
            "  Total LP calls: 30\n",
            "  ConeEmpty SCCs: 0\n",
            "  NonDense-but-nonempty SCCs: 1\n",
            "\n",
            "Headman stats:\n",
            "  headman SCCs: 1\n",
            "  avg headman support size: 3.0\n",
            "  headman delta values (per SCC): [-0.0]\n",
            "\n",
            "============================\n",
            "DIFF SUMMARY [Max-Dense Headman]\n",
            "============================\n",
            "Original: P= 30 T= 30 baseline_uncovered= 0 gict_uncovered= 0 LP total= 1 labels= {'BottleneckLimited': 30}\n",
            "Guarded:  P= 31 T= 30 baseline_uncovered= 2 gict_uncovered= 2 LP total= 30 labels= {'StructurallyUncovered': 2, 'CoveredWitness': 28}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "# ============================================================\n",
        "# 0) Inputs\n",
        "# ============================================================\n",
        "PHILO_URL = \"https://www.pnml.org/version-2009/examples/philo.pnml\"\n",
        "PHILO_FILE = \"philo.pnml\"\n",
        "GUARD_FILE = \"philo_guard.pnml\"   # assumes you already created this\n",
        "\n",
        "# ============================================================\n",
        "# 1) PNML parsing\n",
        "# ============================================================\n",
        "def strip_ns(tag):\n",
        "    return tag.split(\"}\", 1)[-1] if \"}\" in tag else tag\n",
        "\n",
        "def ensure_philo_downloaded():\n",
        "    if not os.path.exists(PHILO_FILE):\n",
        "        print(\"Downloading PNML:\", PHILO_URL)\n",
        "        urllib.request.urlretrieve(PHILO_URL, PHILO_FILE)\n",
        "    else:\n",
        "        print(\"PNML already present:\", PHILO_FILE)\n",
        "\n",
        "def parse_pnml_to_matrices(pnml_file):\n",
        "    tree = ET.parse(pnml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    places = []\n",
        "    transitions = []\n",
        "    arcs = []\n",
        "\n",
        "    for elem in root.iter():\n",
        "        tag = strip_ns(elem.tag)\n",
        "        if tag == \"place\":\n",
        "            pid = elem.attrib.get(\"id\")\n",
        "            if pid:\n",
        "                places.append(pid)\n",
        "        elif tag == \"transition\":\n",
        "            tid = elem.attrib.get(\"id\")\n",
        "            if tid:\n",
        "                transitions.append(tid)\n",
        "        elif tag == \"arc\":\n",
        "            src = elem.attrib.get(\"source\")\n",
        "            tgt = elem.attrib.get(\"target\")\n",
        "            aid = elem.attrib.get(\"id\", \"\")\n",
        "            if src and tgt:\n",
        "                w = 1\n",
        "                for child in elem.iter():\n",
        "                    if strip_ns(child.tag) == \"inscription\":\n",
        "                        for g in child.iter():\n",
        "                            if strip_ns(g.tag) == \"text\" and g.text is not None:\n",
        "                                txt = g.text.strip()\n",
        "                                if txt:\n",
        "                                    try:\n",
        "                                        w = int(txt)\n",
        "                                    except:\n",
        "                                        pass\n",
        "                arcs.append((src, tgt, w, aid))\n",
        "\n",
        "    def uniq(seq):\n",
        "        seen = set()\n",
        "        out = []\n",
        "        for x in seq:\n",
        "            if x not in seen:\n",
        "                out.append(x)\n",
        "                seen.add(x)\n",
        "        return out\n",
        "\n",
        "    places = uniq([p for p in places if p is not None])\n",
        "    transitions = uniq([t for t in transitions if t is not None])\n",
        "\n",
        "    p_index = {p:i for i,p in enumerate(places)}\n",
        "    t_index = {t:i for i,t in enumerate(transitions)}\n",
        "\n",
        "    P = len(places)\n",
        "    T = len(transitions)\n",
        "\n",
        "    Bin = np.zeros((P, T), dtype=int)\n",
        "    Bout = np.zeros((P, T), dtype=int)\n",
        "\n",
        "    ignored = 0\n",
        "    for src, tgt, w, _aid in arcs:\n",
        "        if src in p_index and tgt in t_index:\n",
        "            Bin[p_index[src], t_index[tgt]] += w\n",
        "        elif src in t_index and tgt in p_index:\n",
        "            Bout[p_index[tgt], t_index[src]] += w\n",
        "        else:\n",
        "            ignored += 1\n",
        "\n",
        "    B = Bout - Bin\n",
        "    return places, transitions, arcs, ignored, Bin, Bout, B\n",
        "\n",
        "# ============================================================\n",
        "# 2) Graph + SCC\n",
        "# ============================================================\n",
        "def build_bipartite_graph(Bin, Bout):\n",
        "    P, T = Bin.shape\n",
        "    N = P + T\n",
        "    def t_node(j): return P + j\n",
        "\n",
        "    adj = [[] for _ in range(N)]\n",
        "    rev = [[] for _ in range(N)]\n",
        "\n",
        "    for i in range(P):\n",
        "        for j in range(T):\n",
        "            if Bin[i, j] > 0:\n",
        "                adj[i].append(t_node(j))\n",
        "                rev[t_node(j)].append(i)\n",
        "\n",
        "    for i in range(P):\n",
        "        for j in range(T):\n",
        "            if Bout[i, j] > 0:\n",
        "                adj[t_node(j)].append(i)\n",
        "                rev[i].append(t_node(j))\n",
        "\n",
        "    return adj, rev\n",
        "\n",
        "def tarjan_scc(adj):\n",
        "    index = 0\n",
        "    stack = []\n",
        "    onstack = [False]*len(adj)\n",
        "    idx = [-1]*len(adj)\n",
        "    low = [0]*len(adj)\n",
        "    sccs = []\n",
        "\n",
        "    def strongconnect(v):\n",
        "        nonlocal index\n",
        "        idx[v] = index\n",
        "        low[v] = index\n",
        "        index += 1\n",
        "        stack.append(v)\n",
        "        onstack[v] = True\n",
        "\n",
        "        for w in adj[v]:\n",
        "            if idx[w] == -1:\n",
        "                strongconnect(w)\n",
        "                low[v] = min(low[v], low[w])\n",
        "            elif onstack[w]:\n",
        "                low[v] = min(low[v], idx[w])\n",
        "\n",
        "        if low[v] == idx[v]:\n",
        "            comp = []\n",
        "            while True:\n",
        "                w = stack.pop()\n",
        "                onstack[w] = False\n",
        "                comp.append(w)\n",
        "                if w == v:\n",
        "                    break\n",
        "            sccs.append(comp)\n",
        "\n",
        "    for v in range(len(adj)):\n",
        "        if idx[v] == -1:\n",
        "            strongconnect(v)\n",
        "    return sccs\n",
        "\n",
        "def scc_has_cycle(comp, adj):\n",
        "    if len(comp) > 1:\n",
        "        return True\n",
        "    v = comp[0]\n",
        "    return v in adj[v]\n",
        "\n",
        "# ============================================================\n",
        "# 3) LP primitives\n",
        "# ============================================================\n",
        "def baseline_transition_cover(B, t_idx, min_xt=1.0):\n",
        "    m, n = B.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B.astype(float)\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[t_idx] = (min_xt, None)\n",
        "    res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "    return res.success\n",
        "\n",
        "def dense_witness_simplex(B_sub, eps):\n",
        "    # Feasibility: B_sub x = 0, x >= eps, sum(x)=1\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = np.vstack([B_sub, np.ones((1, n))])\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[-1] = 1.0\n",
        "    bounds = [(eps, None)] * n\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "def cone_nonempty_simplex(B_sub):\n",
        "    # Feasibility: B_sub x = 0, x >= 0, sum(x)=1\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = np.vstack([B_sub, np.ones((1, n))])\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[-1] = 1.0\n",
        "    bounds = [(0, None)] * n\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "def targeted_cover_in_scc(B_full, t_global, t_indices, min_xt=1.0):\n",
        "    pos = t_indices.index(t_global)\n",
        "    B_sub = B_full[:, t_indices].astype(float)\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B_sub\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[pos] = (min_xt, None)\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "# ============================================================\n",
        "# 4) LP support headman:\n",
        "#     Maximize sum(y_i) s.t.\n",
        "#       Bx = 0, sum(x)=1, x>=0\n",
        "#       0 <= y <= 1\n",
        "#       x_i >= tau * y_i\n",
        "# ============================================================\n",
        "def lp_support_headman(B_sub, tau=1e-9):\n",
        "    m, n = B_sub.shape\n",
        "\n",
        "    # Variables: [x0..x_{n-1}, y0..y_{n-1}] => 2n\n",
        "    # Objective: maximize sum(y) => minimize -sum(y)\n",
        "    c = np.zeros(2*n)\n",
        "    c[n:] = -1.0\n",
        "\n",
        "    # Equalities: Bx=0 and sum(x)=1\n",
        "    A_eq = np.zeros((m + 1, 2*n))\n",
        "    A_eq[:m, :n] = B_sub\n",
        "    A_eq[m, :n] = 1.0\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[m] = 1.0\n",
        "\n",
        "    # Inequalities: x_i - tau*y_i >= 0  =>  -x_i + tau*y_i <= 0\n",
        "    A_ub = np.zeros((n, 2*n))\n",
        "    b_ub = np.zeros(n)\n",
        "    for i in range(n):\n",
        "        A_ub[i, i] = -1.0\n",
        "        A_ub[i, n + i] = tau\n",
        "\n",
        "    # Bounds: x>=0, 0<=y<=1\n",
        "    bounds = [(0, None)] * n + [(0, 1)] * n\n",
        "\n",
        "    res = linprog(c, A_eq=A_eq, b_eq=b_eq, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method=\"highs\")\n",
        "    return res\n",
        "\n",
        "# ============================================================\n",
        "# 5) GICT with LP-support headman\n",
        "# ============================================================\n",
        "def run_gict_lp_support_headman(pnml_file, eps=1e-6, tau=1e-9, zero_tol=1e-12, y_on=0.5):\n",
        "    t0 = time.perf_counter()\n",
        "    places, transitions, arcs, ignored, Bin, Bout, B = parse_pnml_to_matrices(pnml_file)\n",
        "    t_parse = time.perf_counter() - t0\n",
        "\n",
        "    P, T = Bin.shape\n",
        "    approx_arcs = int((Bin > 0).sum() + (Bout > 0).sum())\n",
        "\n",
        "    print(\"\\n==\", pnml_file, \"==\")\n",
        "    print(\"Parsed PNML:\")\n",
        "    print(\"  #places:\", P)\n",
        "    print(\"  #transitions:\", T)\n",
        "    print(\"  arcs in file:\", len(arcs), \" ignored arcs:\", ignored)\n",
        "    print(\"  approx #nonzero arcs:\", approx_arcs)\n",
        "    print(\"  parse time (s):\", t_parse)\n",
        "\n",
        "    # Baseline\n",
        "    t0 = time.perf_counter()\n",
        "    base_flags = [baseline_transition_cover(B, j, 1.0) for j in range(T)]\n",
        "    t_baseline = time.perf_counter() - t0\n",
        "    base_uncovered = [j for j, ok in enumerate(base_flags) if not ok]\n",
        "    print(\"\\nBASELINE uncovered transitions (0-based):\", base_uncovered)\n",
        "    print(\"Baseline time (s):\", t_baseline)\n",
        "\n",
        "    # SCC\n",
        "    adj, rev = build_bipartite_graph(Bin, Bout)\n",
        "    def t_node(j): return P + j\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    sccs = tarjan_scc(adj)\n",
        "    t_scc = time.perf_counter() - t0\n",
        "\n",
        "    scc_id = {}\n",
        "    for k, comp in enumerate(sccs):\n",
        "        for v in comp:\n",
        "            scc_id[v] = k\n",
        "    has_cycle = [scc_has_cycle(comp, adj) for comp in sccs]\n",
        "\n",
        "    labels = [\"Uncertain\"] * T\n",
        "\n",
        "    # Gate-0\n",
        "    t0 = time.perf_counter()\n",
        "    for j in range(T):\n",
        "        sid = scc_id[t_node(j)]\n",
        "        if not has_cycle[sid]:\n",
        "            labels[j] = \"StructurallyUncovered\"\n",
        "    t_gate0 = time.perf_counter() - t0\n",
        "\n",
        "    # Gate-2 per-SCC: dense -> (cone-empty shortcut) -> support headman\n",
        "    t0 = time.perf_counter()\n",
        "    lp_calls_scc = 0\n",
        "    lp_calls_targeted = 0\n",
        "    coneempty_sccs = 0\n",
        "    nondense_sccs = 0\n",
        "\n",
        "    headman_sccs = 0\n",
        "    headman_support_sizes = []\n",
        "\n",
        "    for sid, comp in enumerate(sccs):\n",
        "        t_indices = [v - P for v in comp if v >= P]\n",
        "        if not t_indices:\n",
        "            continue\n",
        "\n",
        "        if not has_cycle[sid]:\n",
        "            for tj in t_indices:\n",
        "                labels[tj] = \"StructurallyUncovered\"\n",
        "            continue\n",
        "\n",
        "        B_sub = B[:, t_indices].astype(float)\n",
        "\n",
        "        # Dense feasibility (eps)\n",
        "        res_dense = dense_witness_simplex(B_sub, eps)\n",
        "        lp_calls_scc += 1\n",
        "        if res_dense.success:\n",
        "            # all transitions in SCC are covered (>=eps)\n",
        "            for tj in t_indices:\n",
        "                labels[tj] = \"CoveredWitness\"\n",
        "            continue\n",
        "\n",
        "        # Cone nonempty?\n",
        "        res_cone = cone_nonempty_simplex(B_sub)\n",
        "        lp_calls_scc += 1\n",
        "        if not res_cone.success:\n",
        "            coneempty_sccs += 1\n",
        "            for tj in t_indices:\n",
        "                labels[tj] = \"InvariantConeEmpty\"\n",
        "            continue\n",
        "\n",
        "        # Non-dense but nonempty => support headman\n",
        "        nondense_sccs += 1\n",
        "        res_head = lp_support_headman(B_sub, tau=tau)\n",
        "        lp_calls_scc += 1\n",
        "\n",
        "        if not res_head.success:\n",
        "            # fallback to targeted stage\n",
        "            continue\n",
        "\n",
        "        headman_sccs += 1\n",
        "        sol = np.array(res_head.x, dtype=float)\n",
        "        x_sub = sol[:len(t_indices)]\n",
        "        y_sub = sol[len(t_indices):]\n",
        "\n",
        "        x_sub[np.abs(x_sub) < zero_tol] = 0.0\n",
        "        y_sub[np.abs(y_sub) < zero_tol] = 0.0\n",
        "\n",
        "        supp_y = (y_sub > y_on)\n",
        "        supp_size = int(np.sum(supp_y))\n",
        "        headman_support_sizes.append(supp_size)\n",
        "\n",
        "        # Label those selected by y as CoveredWitness\n",
        "        for k, tj in enumerate(t_indices):\n",
        "            if y_sub[k] > y_on:\n",
        "                labels[tj] = \"CoveredWitness\"\n",
        "\n",
        "    t_gate2_scc = time.perf_counter() - t0\n",
        "\n",
        "    # Target only remaining Uncertain\n",
        "    t0 = time.perf_counter()\n",
        "    for j in range(T):\n",
        "        if labels[j] != \"Uncertain\":\n",
        "            continue\n",
        "\n",
        "        sid = scc_id[t_node(j)]\n",
        "        comp = sccs[sid]\n",
        "        t_indices = [v - P for v in comp if v >= P]\n",
        "\n",
        "        if not has_cycle[sid]:\n",
        "            labels[j] = \"StructurallyUncovered\"\n",
        "            continue\n",
        "\n",
        "        res = targeted_cover_in_scc(B, j, t_indices, min_xt=1.0)\n",
        "        lp_calls_targeted += 1\n",
        "        labels[j] = \"CoveredWitness\" if res.success else \"StructurallyUncovered\"\n",
        "\n",
        "    t_gate2_targeted = time.perf_counter() - t0\n",
        "\n",
        "    gict_uncovered = [j for j, lab in enumerate(labels) if lab in (\"StructurallyUncovered\", \"InvariantConeEmpty\")]\n",
        "\n",
        "    counts = defaultdict(int)\n",
        "    for lab in labels:\n",
        "        counts[lab] += 1\n",
        "\n",
        "    print(\"\\nGICT uncovered transitions (0-based):\", gict_uncovered)\n",
        "    print(\"\\nGICT label histogram:\")\n",
        "    for k in sorted(counts):\n",
        "        print(f\"  {k}: {counts[k]}\")\n",
        "\n",
        "    print(\"\\nGICT timings + LP counts:\")\n",
        "    print(\"  SCC time (s):\", t_scc)\n",
        "    print(\"  Gate-0 time (s):\", t_gate0)\n",
        "    print(\"  Gate-2 per-SCC time (s):\", t_gate2_scc)\n",
        "    print(\"  Gate-2 targeted time (s):\", t_gate2_targeted)\n",
        "    print(\"  LP calls (per-SCC):\", lp_calls_scc)\n",
        "    print(\"  LP calls (targeted):\", lp_calls_targeted)\n",
        "    print(\"  Total LP calls:\", lp_calls_scc + lp_calls_targeted)\n",
        "    print(\"  ConeEmpty SCCs:\", coneempty_sccs)\n",
        "    print(\"  NonDense-but-nonempty SCCs:\", nondense_sccs)\n",
        "\n",
        "    if headman_sccs > 0:\n",
        "        print(\"\\nSupport-headman stats:\")\n",
        "        print(\"  headman SCCs:\", headman_sccs)\n",
        "        print(\"  headman support sizes:\", headman_support_sizes)\n",
        "        print(\"  avg support size:\", float(np.mean(headman_support_sizes)))\n",
        "\n",
        "    return {\n",
        "        \"P\": P,\n",
        "        \"T\": T,\n",
        "        \"arcs\": len(arcs),\n",
        "        \"baseline_uncovered_idx\": base_uncovered,\n",
        "        \"gict_uncovered_idx\": gict_uncovered,\n",
        "        \"labels\": dict(counts),\n",
        "        \"lp_total\": lp_calls_scc + lp_calls_targeted,\n",
        "        \"lp_scc\": lp_calls_scc,\n",
        "        \"lp_targeted\": lp_calls_targeted,\n",
        "    }\n",
        "\n",
        "# ============================================================\n",
        "# 6) Run philo + philo_guard\n",
        "# ============================================================\n",
        "ensure_philo_downloaded()\n",
        "if not os.path.exists(GUARD_FILE):\n",
        "    raise RuntimeError(\"philo_guard.pnml not found. Create it first (your earlier guard mutation already did).\")\n",
        "else:\n",
        "    print(\"PNML already present:\", GUARD_FILE)\n",
        "\n",
        "r0 = run_gict_lp_support_headman(PHILO_FILE, eps=1e-6, tau=1e-9, y_on=0.5)\n",
        "r1 = run_gict_lp_support_headman(GUARD_FILE, eps=1e-6, tau=1e-9, y_on=0.5)\n",
        "\n",
        "print(\"\\n============================\")\n",
        "print(\"DIFF SUMMARY [LP Support Headman]\")\n",
        "print(\"============================\")\n",
        "print(\"Original: baseline_uncovered=\", len(r0[\"baseline_uncovered_idx\"]),\n",
        "      \"gict_uncovered=\", len(r0[\"gict_uncovered_idx\"]),\n",
        "      \"LP total=\", r0[\"lp_total\"], \"labels=\", r0[\"labels\"])\n",
        "print(\"Guarded:  baseline_uncovered=\", len(r1[\"baseline_uncovered_idx\"]),\n",
        "      \"gict_uncovered=\", len(r1[\"gict_uncovered_idx\"]),\n",
        "      \"LP total=\", r1[\"lp_total\"], \"labels=\", r1[\"labels\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUSOcnohx7tc",
        "outputId": "ffe6e8f0-4452-4788-b98a-aa5fbae4b828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PNML already present: philo.pnml\n",
            "PNML already present: philo_guard.pnml\n",
            "\n",
            "== philo.pnml ==\n",
            "Parsed PNML:\n",
            "  #places: 30\n",
            "  #transitions: 30\n",
            "  arcs in file: 96  ignored arcs: 0\n",
            "  approx #nonzero arcs: 96\n",
            "  parse time (s): 0.12451511900053447\n",
            "\n",
            "BASELINE uncovered transitions (0-based): []\n",
            "Baseline time (s): 0.11414489399976446\n",
            "\n",
            "GICT uncovered transitions (0-based): []\n",
            "\n",
            "GICT label histogram:\n",
            "  CoveredWitness: 30\n",
            "\n",
            "GICT timings + LP counts:\n",
            "  SCC time (s): 9.775800026545767e-05\n",
            "  Gate-0 time (s): 9.063999641512055e-06\n",
            "  Gate-2 per-SCC time (s): 0.00379460400017706\n",
            "  Gate-2 targeted time (s): 4.786000317835715e-06\n",
            "  LP calls (per-SCC): 1\n",
            "  LP calls (targeted): 0\n",
            "  Total LP calls: 1\n",
            "  ConeEmpty SCCs: 0\n",
            "  NonDense-but-nonempty SCCs: 0\n",
            "\n",
            "== philo_guard.pnml ==\n",
            "Parsed PNML:\n",
            "  #places: 31\n",
            "  #transitions: 30\n",
            "  arcs in file: 97  ignored arcs: 0\n",
            "  approx #nonzero arcs: 97\n",
            "  parse time (s): 0.004177474000243819\n",
            "\n",
            "BASELINE uncovered transitions (0-based): [0, 15]\n",
            "Baseline time (s): 0.13188388100024895\n",
            "\n",
            "GICT uncovered transitions (0-based): []\n",
            "\n",
            "GICT label histogram:\n",
            "  CoveredWitness: 30\n",
            "\n",
            "GICT timings + LP counts:\n",
            "  SCC time (s): 0.0001038250002238783\n",
            "  Gate-0 time (s): 8.87199985299958e-06\n",
            "  Gate-2 per-SCC time (s): 0.016304663000482833\n",
            "  Gate-2 targeted time (s): 3.499999365885742e-06\n",
            "  LP calls (per-SCC): 3\n",
            "  LP calls (targeted): 0\n",
            "  Total LP calls: 3\n",
            "  ConeEmpty SCCs: 0\n",
            "  NonDense-but-nonempty SCCs: 1\n",
            "\n",
            "Support-headman stats:\n",
            "  headman SCCs: 1\n",
            "  headman support sizes: [30]\n",
            "  avg support size: 30.0\n",
            "\n",
            "============================\n",
            "DIFF SUMMARY [LP Support Headman]\n",
            "============================\n",
            "Original: baseline_uncovered= 0 gict_uncovered= 0 LP total= 1 labels= {'CoveredWitness': 30}\n",
            "Guarded:  baseline_uncovered= 2 gict_uncovered= 0 LP total= 3 labels= {'CoveredWitness': 30}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JBQ_xFcj0sAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "pnml_files = [\n",
        "    \"philo.pnml\",\n",
        "    \"philo_guard.pnml\",\n",
        "    \"philo_leak.pnml\",\n",
        "    \"Token-ring.pnml\",\n",
        "    \"Piscine.pnml\",\n",
        "]\n",
        "\n",
        "zip_name = \"pnml_benchmarks.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_name, \"w\", zipfile.ZIP_DEFLATED) as z:\n",
        "    for f in pnml_files:\n",
        "        if os.path.exists(f):\n",
        "            z.write(f)\n",
        "            print(\"Added:\", f)\n",
        "        else:\n",
        "            print(\"Missing (skipped):\", f)\n",
        "\n",
        "print(\"\\nCreated:\", zip_name)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCTC9jooyMCl",
        "outputId": "8f650b1a-aec1-47b3-cee6-8c5417e3ac10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added: philo.pnml\n",
            "Added: philo_guard.pnml\n",
            "Added: philo_leak.pnml\n",
            "Added: Token-ring.pnml\n",
            "Added: Piscine.pnml\n",
            "\n",
            "Created: pnml_benchmarks.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ImVOfAiEyWhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "# ============================================================\n",
        "# 0) Inputs\n",
        "# ============================================================\n",
        "PHILO_URL  = \"https://www.pnml.org/version-2009/examples/philo.pnml\"\n",
        "PHILO_FILE = \"philo.pnml\"\n",
        "GUARD_FILE = \"philo_guard.pnml\"   # assumes you already created this\n",
        "\n",
        "# ============================================================\n",
        "# 1) PNML parsing\n",
        "# ============================================================\n",
        "def strip_ns(tag):\n",
        "    return tag.split(\"}\", 1)[-1] if \"}\" in tag else tag\n",
        "\n",
        "def ensure_philo_downloaded():\n",
        "    if not os.path.exists(PHILO_FILE):\n",
        "        print(\"Downloading PNML:\", PHILO_URL)\n",
        "        urllib.request.urlretrieve(PHILO_URL, PHILO_FILE)\n",
        "    else:\n",
        "        print(\"PNML already present:\", PHILO_FILE)\n",
        "\n",
        "def parse_pnml_to_matrices(pnml_file):\n",
        "    tree = ET.parse(pnml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    places = []\n",
        "    transitions = []\n",
        "    arcs = []\n",
        "\n",
        "    for elem in root.iter():\n",
        "        tag = strip_ns(elem.tag)\n",
        "        if tag == \"place\":\n",
        "            pid = elem.attrib.get(\"id\")\n",
        "            if pid:\n",
        "                places.append(pid)\n",
        "        elif tag == \"transition\":\n",
        "            tid = elem.attrib.get(\"id\")\n",
        "            if tid:\n",
        "                transitions.append(tid)\n",
        "        elif tag == \"arc\":\n",
        "            src = elem.attrib.get(\"source\")\n",
        "            tgt = elem.attrib.get(\"target\")\n",
        "            aid = elem.attrib.get(\"id\", \"\")\n",
        "            if src and tgt:\n",
        "                w = 1\n",
        "                for child in elem.iter():\n",
        "                    if strip_ns(child.tag) == \"inscription\":\n",
        "                        for g in child.iter():\n",
        "                            if strip_ns(g.tag) == \"text\" and g.text is not None:\n",
        "                                txt = g.text.strip()\n",
        "                                if txt:\n",
        "                                    try:\n",
        "                                        w = int(txt)\n",
        "                                    except:\n",
        "                                        pass\n",
        "                arcs.append((src, tgt, w, aid))\n",
        "\n",
        "    def uniq(seq):\n",
        "        seen = set()\n",
        "        out = []\n",
        "        for x in seq:\n",
        "            if x not in seen:\n",
        "                out.append(x)\n",
        "                seen.add(x)\n",
        "        return out\n",
        "\n",
        "    places = uniq([p for p in places if p is not None])\n",
        "    transitions = uniq([t for t in transitions if t is not None])\n",
        "\n",
        "    p_index = {p:i for i,p in enumerate(places)}\n",
        "    t_index = {t:i for i,t in enumerate(transitions)}\n",
        "\n",
        "    P = len(places)\n",
        "    T = len(transitions)\n",
        "\n",
        "    Bin = np.zeros((P, T), dtype=int)\n",
        "    Bout = np.zeros((P, T), dtype=int)\n",
        "\n",
        "    ignored = 0\n",
        "    for src, tgt, w, _aid in arcs:\n",
        "        if src in p_index and tgt in t_index:\n",
        "            Bin[p_index[src], t_index[tgt]] += w\n",
        "        elif src in t_index and tgt in p_index:\n",
        "            Bout[p_index[tgt], t_index[src]] += w\n",
        "        else:\n",
        "            ignored += 1\n",
        "\n",
        "    B = Bout - Bin\n",
        "    return places, transitions, arcs, ignored, Bin, Bout, B\n",
        "\n",
        "# ============================================================\n",
        "# 2) Graph + SCC\n",
        "# ============================================================\n",
        "def build_bipartite_graph(Bin, Bout):\n",
        "    P, T = Bin.shape\n",
        "    N = P + T\n",
        "    def t_node(j): return P + j\n",
        "\n",
        "    adj = [[] for _ in range(N)]\n",
        "    rev = [[] for _ in range(N)]\n",
        "\n",
        "    for i in range(P):\n",
        "        for j in range(T):\n",
        "            if Bin[i, j] > 0:\n",
        "                adj[i].append(t_node(j))\n",
        "                rev[t_node(j)].append(i)\n",
        "\n",
        "    for i in range(P):\n",
        "        for j in range(T):\n",
        "            if Bout[i, j] > 0:\n",
        "                adj[t_node(j)].append(i)\n",
        "                rev[i].append(t_node(j))\n",
        "\n",
        "    return adj, rev\n",
        "\n",
        "def tarjan_scc(adj):\n",
        "    index = 0\n",
        "    stack = []\n",
        "    onstack = [False]*len(adj)\n",
        "    idx = [-1]*len(adj)\n",
        "    low = [0]*len(adj)\n",
        "    sccs = []\n",
        "\n",
        "    def strongconnect(v):\n",
        "        nonlocal index\n",
        "        idx[v] = index\n",
        "        low[v] = index\n",
        "        index += 1\n",
        "        stack.append(v)\n",
        "        onstack[v] = True\n",
        "\n",
        "        for w in adj[v]:\n",
        "            if idx[w] == -1:\n",
        "                strongconnect(w)\n",
        "                low[v] = min(low[v], low[w])\n",
        "            elif onstack[w]:\n",
        "                low[v] = min(low[v], idx[w])\n",
        "\n",
        "        if low[v] == idx[v]:\n",
        "            comp = []\n",
        "            while True:\n",
        "                w = stack.pop()\n",
        "                onstack[w] = False\n",
        "                comp.append(w)\n",
        "                if w == v:\n",
        "                    break\n",
        "            sccs.append(comp)\n",
        "\n",
        "    for v in range(len(adj)):\n",
        "        if idx[v] == -1:\n",
        "            strongconnect(v)\n",
        "    return sccs\n",
        "\n",
        "def scc_has_cycle(comp, adj):\n",
        "    if len(comp) > 1:\n",
        "        return True\n",
        "    v = comp[0]\n",
        "    return v in adj[v]\n",
        "\n",
        "# ============================================================\n",
        "# 3) LP primitives\n",
        "# ============================================================\n",
        "def baseline_transition_cover(B, t_idx, min_xt=1.0):\n",
        "    m, n = B.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B.astype(float)\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[t_idx] = (min_xt, None)\n",
        "    res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "    return res.success\n",
        "\n",
        "def dense_witness_simplex(B_sub, eps):\n",
        "    # Feasibility: B_sub x = 0, x >= eps, sum(x)=1\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = np.vstack([B_sub, np.ones((1, n))])\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[-1] = 1.0\n",
        "    bounds = [(eps, None)] * n\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "def cone_nonempty_simplex(B_sub):\n",
        "    # Feasibility: B_sub x = 0, x >= 0, sum(x)=1\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = np.vstack([B_sub, np.ones((1, n))])\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[-1] = 1.0\n",
        "    bounds = [(0, None)] * n\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "def targeted_cover_in_scc(B_full, t_global, t_indices, min_xt=1.0):\n",
        "    pos = t_indices.index(t_global)\n",
        "    B_sub = B_full[:, t_indices].astype(float)\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B_sub\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[pos] = (min_xt, None)\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "# ============================================================\n",
        "# 4) LP support headman v2 (numerically robust):\n",
        "#     Maximize sum(y_i) s.t.\n",
        "#       Bx = 0, sum(x)=1, x>=0\n",
        "#       0 <= y <= 1\n",
        "#       (A) x_i >= tau * y_i\n",
        "#       (B) x_i <= 1 * y_i     (since sum(x)=1 => x_i <= 1 anyway)\n",
        "#\n",
        "#     Then we trust COVERED-by-headman based on x_i >= tau/2.\n",
        "# ============================================================\n",
        "def lp_support_headman_v2(B_sub, tau=1e-7):\n",
        "    m, n = B_sub.shape\n",
        "\n",
        "    # Variables: [x0..x_{n-1}, y0..y_{n-1}] => 2n\n",
        "    # Objective: maximize sum(y) => minimize -sum(y)\n",
        "    c = np.zeros(2*n)\n",
        "    c[n:] = -1.0\n",
        "\n",
        "    # Equalities: Bx=0 and sum(x)=1\n",
        "    A_eq = np.zeros((m + 1, 2*n))\n",
        "    A_eq[:m, :n] = B_sub\n",
        "    A_eq[m, :n] = 1.0\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[m] = 1.0\n",
        "\n",
        "    # Inequalities:\n",
        "    # (A) x_i - tau*y_i >= 0  =>  -x_i + tau*y_i <= 0\n",
        "    # (B) x_i - 1*y_i <= 0\n",
        "    A_ub = np.zeros((2*n, 2*n))\n",
        "    b_ub = np.zeros(2*n)\n",
        "\n",
        "    for i in range(n):\n",
        "        # (A)\n",
        "        A_ub[i, i] = -1.0\n",
        "        A_ub[i, n + i] = tau\n",
        "        # (B)\n",
        "        A_ub[n + i, i] = 1.0\n",
        "        A_ub[n + i, n + i] = -1.0\n",
        "\n",
        "    # Bounds: x>=0, 0<=y<=1\n",
        "    bounds = [(0, None)] * n + [(0, 1)] * n\n",
        "\n",
        "    res = linprog(c, A_eq=A_eq, b_eq=b_eq, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method=\"highs\")\n",
        "    return res\n",
        "\n",
        "# ============================================================\n",
        "# 5) GICT with LP-support headman v2\n",
        "# ============================================================\n",
        "def run_gict_lp_support_headman_v2(\n",
        "    pnml_file,\n",
        "    eps=1e-6,\n",
        "    tau=1e-7,          # IMPORTANT: larger than before to beat tolerances\n",
        "    zero_tol=1e-12,\n",
        "    x_cover_thr=None   # default tau/2\n",
        "):\n",
        "    if x_cover_thr is None:\n",
        "        x_cover_thr = tau / 2.0\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    places, transitions, arcs, ignored, Bin, Bout, B = parse_pnml_to_matrices(pnml_file)\n",
        "    t_parse = time.perf_counter() - t0\n",
        "\n",
        "    P, T = Bin.shape\n",
        "    approx_arcs = int((Bin > 0).sum() + (Bout > 0).sum())\n",
        "\n",
        "    print(\"\\n==\", pnml_file, \"==\")\n",
        "    print(\"Parsed PNML:\")\n",
        "    print(\"  #places:\", P)\n",
        "    print(\"  #transitions:\", T)\n",
        "    print(\"  arcs in file:\", len(arcs), \" ignored arcs:\", ignored)\n",
        "    print(\"  approx #nonzero arcs:\", approx_arcs)\n",
        "    print(\"  parse time (s):\", t_parse)\n",
        "\n",
        "    # Baseline\n",
        "    t0 = time.perf_counter()\n",
        "    base_flags = [baseline_transition_cover(B, j, 1.0) for j in range(T)]\n",
        "    t_baseline = time.perf_counter() - t0\n",
        "    base_uncovered = [j for j, ok in enumerate(base_flags) if not ok]\n",
        "    print(\"\\nBASELINE uncovered transitions (0-based):\", base_uncovered)\n",
        "    print(\"Baseline time (s):\", t_baseline)\n",
        "\n",
        "    # SCC\n",
        "    adj, rev = build_bipartite_graph(Bin, Bout)\n",
        "    def t_node(j): return P + j\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    sccs = tarjan_scc(adj)\n",
        "    t_scc = time.perf_counter() - t0\n",
        "\n",
        "    scc_id = {}\n",
        "    for k, comp in enumerate(sccs):\n",
        "        for v in comp:\n",
        "            scc_id[v] = k\n",
        "    has_cycle = [scc_has_cycle(comp, adj) for comp in sccs]\n",
        "\n",
        "    labels = [\"Uncertain\"] * T\n",
        "\n",
        "    # Gate-0\n",
        "    t0 = time.perf_counter()\n",
        "    for j in range(T):\n",
        "        sid = scc_id[t_node(j)]\n",
        "        if not has_cycle[sid]:\n",
        "            labels[j] = \"StructurallyUncovered\"\n",
        "    t_gate0 = time.perf_counter() - t0\n",
        "\n",
        "    # Gate-2 per-SCC: dense -> cone-empty -> headman-v2\n",
        "    t0 = time.perf_counter()\n",
        "    lp_calls_scc = 0\n",
        "    lp_calls_targeted = 0\n",
        "    coneempty_sccs = 0\n",
        "    nondense_sccs = 0\n",
        "\n",
        "    headman_sccs = 0\n",
        "    headman_x_support_sizes = []\n",
        "    headman_y_support_sizes = []\n",
        "\n",
        "    for sid, comp in enumerate(sccs):\n",
        "        t_indices = [v - P for v in comp if v >= P]\n",
        "        if not t_indices:\n",
        "            continue\n",
        "\n",
        "        if not has_cycle[sid]:\n",
        "            for tj in t_indices:\n",
        "                labels[tj] = \"StructurallyUncovered\"\n",
        "            continue\n",
        "\n",
        "        B_sub = B[:, t_indices].astype(float)\n",
        "\n",
        "        # Dense feasibility (eps)\n",
        "        res_dense = dense_witness_simplex(B_sub, eps)\n",
        "        lp_calls_scc += 1\n",
        "        if res_dense.success:\n",
        "            for tj in t_indices:\n",
        "                labels[tj] = \"CoveredWitness\"\n",
        "            continue\n",
        "\n",
        "        # Cone nonempty?\n",
        "        res_cone = cone_nonempty_simplex(B_sub)\n",
        "        lp_calls_scc += 1\n",
        "        if not res_cone.success:\n",
        "            coneempty_sccs += 1\n",
        "            for tj in t_indices:\n",
        "                labels[tj] = \"InvariantConeEmpty\"\n",
        "            continue\n",
        "\n",
        "        # Non-dense but nonempty => support headman v2\n",
        "        nondense_sccs += 1\n",
        "        res_head = lp_support_headman_v2(B_sub, tau=tau)\n",
        "        lp_calls_scc += 1\n",
        "\n",
        "        if not res_head.success:\n",
        "            continue\n",
        "\n",
        "        headman_sccs += 1\n",
        "        sol = np.array(res_head.x, dtype=float)\n",
        "        n = len(t_indices)\n",
        "        x_sub = sol[:n]\n",
        "        y_sub = sol[n:]\n",
        "\n",
        "        x_sub[np.abs(x_sub) < zero_tol] = 0.0\n",
        "        y_sub[np.abs(y_sub) < zero_tol] = 0.0\n",
        "\n",
        "        x_supp = (x_sub >= x_cover_thr)\n",
        "        y_supp = (y_sub >= 0.5)\n",
        "\n",
        "        headman_x_support_sizes.append(int(np.sum(x_supp)))\n",
        "        headman_y_support_sizes.append(int(np.sum(y_supp)))\n",
        "\n",
        "        # Label by x (robust)\n",
        "        for k, tj in enumerate(t_indices):\n",
        "            if x_sub[k] >= x_cover_thr:\n",
        "                labels[tj] = \"CoveredWitness\"\n",
        "\n",
        "    t_gate2_scc = time.perf_counter() - t0\n",
        "\n",
        "    # Target only remaining Uncertain\n",
        "    t0 = time.perf_counter()\n",
        "    targeted_list = []\n",
        "    for j in range(T):\n",
        "        if labels[j] != \"Uncertain\":\n",
        "            continue\n",
        "\n",
        "        targeted_list.append(j)\n",
        "        sid = scc_id[t_node(j)]\n",
        "        comp = sccs[sid]\n",
        "        t_indices = [v - P for v in comp if v >= P]\n",
        "\n",
        "        if not has_cycle[sid]:\n",
        "            labels[j] = \"StructurallyUncovered\"\n",
        "            continue\n",
        "\n",
        "        res = targeted_cover_in_scc(B, j, t_indices, min_xt=1.0)\n",
        "        lp_calls_targeted += 1\n",
        "        labels[j] = \"CoveredWitness\" if res.success else \"StructurallyUncovered\"\n",
        "\n",
        "    t_gate2_targeted = time.perf_counter() - t0\n",
        "\n",
        "    gict_uncovered = [j for j, lab in enumerate(labels) if lab in (\"StructurallyUncovered\", \"InvariantConeEmpty\")]\n",
        "\n",
        "    counts = defaultdict(int)\n",
        "    for lab in labels:\n",
        "        counts[lab] += 1\n",
        "\n",
        "    print(\"\\nGICT uncovered transitions (0-based):\", gict_uncovered)\n",
        "    print(\"\\nGICT label histogram:\")\n",
        "    for k in sorted(counts):\n",
        "        print(f\"  {k}: {counts[k]}\")\n",
        "\n",
        "    print(\"\\nGICT timings + LP counts:\")\n",
        "    print(\"  SCC time (s):\", t_scc)\n",
        "    print(\"  Gate-0 time (s):\", t_gate0)\n",
        "    print(\"  Gate-2 per-SCC time (s):\", t_gate2_scc)\n",
        "    print(\"  Gate-2 targeted time (s):\", t_gate2_targeted)\n",
        "    print(\"  LP calls (per-SCC):\", lp_calls_scc)\n",
        "    print(\"  LP calls (targeted):\", lp_calls_targeted)\n",
        "    print(\"  Total LP calls:\", lp_calls_scc + lp_calls_targeted)\n",
        "    print(\"  ConeEmpty SCCs:\", coneempty_sccs)\n",
        "    print(\"  NonDense-but-nonempty SCCs:\", nondense_sccs)\n",
        "\n",
        "    if headman_sccs > 0:\n",
        "        print(\"\\nSupport-headman v2 stats:\")\n",
        "        print(\"  headman SCCs:\", headman_sccs)\n",
        "        print(\"  headman x-support sizes (x >= %.3g):\" % x_cover_thr, headman_x_support_sizes)\n",
        "        print(\"  headman y-support sizes (y >= 0.5):\", headman_y_support_sizes)\n",
        "        print(\"  avg x-support:\", float(np.mean(headman_x_support_sizes)))\n",
        "        print(\"  avg y-support:\", float(np.mean(headman_y_support_sizes)))\n",
        "\n",
        "    if targeted_list:\n",
        "        print(\"\\nTargeted transitions (0-based indices):\", targeted_list)\n",
        "\n",
        "    return {\n",
        "        \"P\": P,\n",
        "        \"T\": T,\n",
        "        \"arcs\": len(arcs),\n",
        "        \"baseline_uncovered_idx\": base_uncovered,\n",
        "        \"gict_uncovered_idx\": gict_uncovered,\n",
        "        \"labels\": dict(counts),\n",
        "        \"lp_total\": lp_calls_scc + lp_calls_targeted,\n",
        "        \"lp_scc\": lp_calls_scc,\n",
        "        \"lp_targeted\": lp_calls_targeted,\n",
        "        \"targeted_list\": targeted_list,\n",
        "    }\n",
        "\n",
        "# ============================================================\n",
        "# 6) Run philo + philo_guard\n",
        "# ============================================================\n",
        "ensure_philo_downloaded()\n",
        "if not os.path.exists(GUARD_FILE):\n",
        "    raise RuntimeError(\"philo_guard.pnml not found. Create it first (your earlier guard mutation already did).\")\n",
        "else:\n",
        "    print(\"PNML already present:\", GUARD_FILE)\n",
        "\n",
        "r0 = run_gict_lp_support_headman_v2(PHILO_FILE, eps=1e-6, tau=1e-7)\n",
        "r1 = run_gict_lp_support_headman_v2(GUARD_FILE, eps=1e-6, tau=1e-7)\n",
        "\n",
        "print(\"\\n============================\")\n",
        "print(\"DIFF SUMMARY [LP Support Headman v2]\")\n",
        "print(\"============================\")\n",
        "print(\"Original: baseline_uncovered=\", len(r0[\"baseline_uncovered_idx\"]),\n",
        "      \"gict_uncovered=\", len(r0[\"gict_uncovered_idx\"]),\n",
        "      \"LP total=\", r0[\"lp_total\"], \"labels=\", r0[\"labels\"])\n",
        "print(\"Guarded:  baseline_uncovered=\", len(r1[\"baseline_uncovered_idx\"]),\n",
        "      \"gict_uncovered=\", len(r1[\"gict_uncovered_idx\"]),\n",
        "      \"LP total=\", r1[\"lp_total\"], \"labels=\", r1[\"labels\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5AnlouJzbbB",
        "outputId": "36e73c96-27e0-4d3c-81d6-d427a9423bc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PNML already present: philo.pnml\n",
            "PNML already present: philo_guard.pnml\n",
            "\n",
            "== philo.pnml ==\n",
            "Parsed PNML:\n",
            "  #places: 30\n",
            "  #transitions: 30\n",
            "  arcs in file: 96  ignored arcs: 0\n",
            "  approx #nonzero arcs: 96\n",
            "  parse time (s): 0.00379769600021973\n",
            "\n",
            "BASELINE uncovered transitions (0-based): []\n",
            "Baseline time (s): 0.2744733110002926\n",
            "\n",
            "GICT uncovered transitions (0-based): []\n",
            "\n",
            "GICT label histogram:\n",
            "  CoveredWitness: 30\n",
            "\n",
            "GICT timings + LP counts:\n",
            "  SCC time (s): 9.537499954603845e-05\n",
            "  Gate-0 time (s): 9.042999408848118e-06\n",
            "  Gate-2 per-SCC time (s): 0.007643016000656644\n",
            "  Gate-2 targeted time (s): 5.38099993718788e-06\n",
            "  LP calls (per-SCC): 1\n",
            "  LP calls (targeted): 0\n",
            "  Total LP calls: 1\n",
            "  ConeEmpty SCCs: 0\n",
            "  NonDense-but-nonempty SCCs: 0\n",
            "\n",
            "== philo_guard.pnml ==\n",
            "Parsed PNML:\n",
            "  #places: 31\n",
            "  #transitions: 30\n",
            "  arcs in file: 97  ignored arcs: 0\n",
            "  approx #nonzero arcs: 97\n",
            "  parse time (s): 0.005659391999870422\n",
            "\n",
            "BASELINE uncovered transitions (0-based): [0, 15]\n",
            "Baseline time (s): 0.10849497500021243\n",
            "\n",
            "GICT uncovered transitions (0-based): []\n",
            "\n",
            "GICT label histogram:\n",
            "  CoveredWitness: 30\n",
            "\n",
            "GICT timings + LP counts:\n",
            "  SCC time (s): 0.00012961300035385648\n",
            "  Gate-0 time (s): 7.246999302878976e-06\n",
            "  Gate-2 per-SCC time (s): 0.011291079000329773\n",
            "  Gate-2 targeted time (s): 3.5750008464674465e-06\n",
            "  LP calls (per-SCC): 3\n",
            "  LP calls (targeted): 0\n",
            "  Total LP calls: 3\n",
            "  ConeEmpty SCCs: 0\n",
            "  NonDense-but-nonempty SCCs: 1\n",
            "\n",
            "Support-headman v2 stats:\n",
            "  headman SCCs: 1\n",
            "  headman x-support sizes (x >= 5e-08): [30]\n",
            "  headman y-support sizes (y >= 0.5): [30]\n",
            "  avg x-support: 30.0\n",
            "  avg y-support: 30.0\n",
            "\n",
            "============================\n",
            "DIFF SUMMARY [LP Support Headman v2]\n",
            "============================\n",
            "Original: baseline_uncovered= 0 gict_uncovered= 0 LP total= 1 labels= {'CoveredWitness': 30}\n",
            "Guarded:  baseline_uncovered= 2 gict_uncovered= 0 LP total= 3 labels= {'CoveredWitness': 30}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "# ============================================================\n",
        "# 0) Inputs\n",
        "# ============================================================\n",
        "PHILO_URL  = \"https://www.pnml.org/version-2009/examples/philo.pnml\"\n",
        "PHILO_FILE = \"philo.pnml\"\n",
        "GUARD_FILE = \"philo_guard.pnml\"   # assumes you already created this\n",
        "\n",
        "# ============================================================\n",
        "# 1) PNML parsing\n",
        "# ============================================================\n",
        "def strip_ns(tag):\n",
        "    return tag.split(\"}\", 1)[-1] if \"}\" in tag else tag\n",
        "\n",
        "def ensure_download(url, path):\n",
        "    if not os.path.exists(path):\n",
        "        print(\"Downloading PNML:\", url)\n",
        "        urllib.request.urlretrieve(url, path)\n",
        "    else:\n",
        "        print(\"PNML already present:\", path)\n",
        "\n",
        "def parse_pnml_to_matrices(pnml_file):\n",
        "    tree = ET.parse(pnml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    places = []\n",
        "    transitions = []\n",
        "    arcs = []\n",
        "\n",
        "    for elem in root.iter():\n",
        "        tag = strip_ns(elem.tag)\n",
        "        if tag == \"place\":\n",
        "            pid = elem.attrib.get(\"id\")\n",
        "            if pid:\n",
        "                places.append(pid)\n",
        "        elif tag == \"transition\":\n",
        "            tid = elem.attrib.get(\"id\")\n",
        "            if tid:\n",
        "                transitions.append(tid)\n",
        "        elif tag == \"arc\":\n",
        "            src = elem.attrib.get(\"source\")\n",
        "            tgt = elem.attrib.get(\"target\")\n",
        "            aid = elem.attrib.get(\"id\", \"\")\n",
        "            if src and tgt:\n",
        "                w = 1\n",
        "                for child in elem.iter():\n",
        "                    if strip_ns(child.tag) == \"inscription\":\n",
        "                        for g in child.iter():\n",
        "                            if strip_ns(g.tag) == \"text\" and g.text is not None:\n",
        "                                txt = g.text.strip()\n",
        "                                if txt:\n",
        "                                    try:\n",
        "                                        w = int(txt)\n",
        "                                    except:\n",
        "                                        pass\n",
        "                arcs.append((src, tgt, w, aid))\n",
        "\n",
        "    def uniq(seq):\n",
        "        seen = set()\n",
        "        out = []\n",
        "        for x in seq:\n",
        "            if x not in seen:\n",
        "                out.append(x)\n",
        "                seen.add(x)\n",
        "        return out\n",
        "\n",
        "    places = uniq([p for p in places if p is not None])\n",
        "    transitions = uniq([t for t in transitions if t is not None])\n",
        "\n",
        "    p_index = {p:i for i,p in enumerate(places)}\n",
        "    t_index = {t:i for i,t in enumerate(transitions)}\n",
        "\n",
        "    P = len(places)\n",
        "    T = len(transitions)\n",
        "\n",
        "    Bin = np.zeros((P, T), dtype=float)\n",
        "    Bout = np.zeros((P, T), dtype=float)\n",
        "\n",
        "    ignored = 0\n",
        "    for src, tgt, w, _aid in arcs:\n",
        "        if src in p_index and tgt in t_index:\n",
        "            Bin[p_index[src], t_index[tgt]] += float(w)\n",
        "        elif src in t_index and tgt in p_index:\n",
        "            Bout[p_index[tgt], t_index[src]] += float(w)\n",
        "        else:\n",
        "            ignored += 1\n",
        "\n",
        "    B = Bout - Bin\n",
        "    return places, transitions, arcs, ignored, Bin, Bout, B\n",
        "\n",
        "# ============================================================\n",
        "# 2) Graph + SCC\n",
        "# ============================================================\n",
        "def build_bipartite_graph(Bin, Bout):\n",
        "    P, T = Bin.shape\n",
        "    N = P + T\n",
        "    def t_node(j): return P + j\n",
        "\n",
        "    adj = [[] for _ in range(N)]\n",
        "    rev = [[] for _ in range(N)]\n",
        "\n",
        "    for i in range(P):\n",
        "        for j in range(T):\n",
        "            if Bin[i, j] > 0:\n",
        "                adj[i].append(t_node(j))\n",
        "                rev[t_node(j)].append(i)\n",
        "\n",
        "    for i in range(P):\n",
        "        for j in range(T):\n",
        "            if Bout[i, j] > 0:\n",
        "                adj[t_node(j)].append(i)\n",
        "                rev[i].append(t_node(j))\n",
        "\n",
        "    return adj, rev\n",
        "\n",
        "def tarjan_scc(adj):\n",
        "    index = 0\n",
        "    stack = []\n",
        "    onstack = [False]*len(adj)\n",
        "    idx = [-1]*len(adj)\n",
        "    low = [0]*len(adj)\n",
        "    sccs = []\n",
        "\n",
        "    def strongconnect(v):\n",
        "        nonlocal index\n",
        "        idx[v] = index\n",
        "        low[v] = index\n",
        "        index += 1\n",
        "        stack.append(v)\n",
        "        onstack[v] = True\n",
        "\n",
        "        for w in adj[v]:\n",
        "            if idx[w] == -1:\n",
        "                strongconnect(w)\n",
        "                low[v] = min(low[v], low[w])\n",
        "            elif onstack[w]:\n",
        "                low[v] = min(low[v], idx[w])\n",
        "\n",
        "        if low[v] == idx[v]:\n",
        "            comp = []\n",
        "            while True:\n",
        "                w = stack.pop()\n",
        "                onstack[w] = False\n",
        "                comp.append(w)\n",
        "                if w == v:\n",
        "                    break\n",
        "            sccs.append(comp)\n",
        "\n",
        "    for v in range(len(adj)):\n",
        "        if idx[v] == -1:\n",
        "            strongconnect(v)\n",
        "    return sccs\n",
        "\n",
        "def scc_has_cycle(comp, adj):\n",
        "    if len(comp) > 1:\n",
        "        return True\n",
        "    v = comp[0]\n",
        "    return v in adj[v]\n",
        "\n",
        "# ============================================================\n",
        "# 3) LP primitives\n",
        "# ============================================================\n",
        "def baseline_transition_cover(B, t_idx, min_xt=1.0):\n",
        "    m, n = B.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[t_idx] = (min_xt, None)\n",
        "    res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "    return res.success\n",
        "\n",
        "def dense_witness_simplex(B_sub, eps):\n",
        "    # Feasibility: B_sub x = 0, x >= eps, sum(x)=1\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = np.vstack([B_sub, np.ones((1, n))])\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[-1] = 1.0\n",
        "    bounds = [(eps, None)] * n\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "def cone_nonempty_simplex(B_sub):\n",
        "    # Feasibility: B_sub x = 0, x >= 0, sum(x)=1\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = np.vstack([B_sub, np.ones((1, n))])\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[-1] = 1.0\n",
        "    bounds = [(0, None)] * n\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "def targeted_cover_in_scc(B_full, t_global, t_indices, min_xt=1.0):\n",
        "    pos = t_indices.index(t_global)\n",
        "    B_sub = B_full[:, t_indices]\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B_sub\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[pos] = (min_xt, None)\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "# ============================================================\n",
        "# 4) LP support headman v2 (robust) + witness validation\n",
        "#\n",
        "# Variables: x (n), y (n)\n",
        "# Objective: maximize sum(y)  <=> minimize -sum(y)\n",
        "# Constraints:\n",
        "#   Bx = 0\n",
        "#   sum(x) = 1\n",
        "#   x >= 0\n",
        "#   0 <= y <= 1\n",
        "#   x_i >= tau*y_i     (couple up)\n",
        "#   x_i <= y_i         (couple down; since x_i <= 1 anyway)\n",
        "#\n",
        "# Validation after solve:\n",
        "#   residual = max|B x| must be <= resid_tol\n",
        "#   min(x) must be >= -minx_tol\n",
        "#\n",
        "# We label \"covered by headman\" using x_i >= x_cover_thr (default tau/2)\n",
        "# ============================================================\n",
        "def lp_support_headman_v2(B_sub, tau=1e-4):\n",
        "    m, n = B_sub.shape\n",
        "\n",
        "    c = np.zeros(2*n)\n",
        "    c[n:] = -1.0  # maximize sum(y)\n",
        "\n",
        "    # Equalities: Bx=0 and sum(x)=1\n",
        "    A_eq = np.zeros((m + 1, 2*n))\n",
        "    A_eq[:m, :n] = B_sub\n",
        "    A_eq[m, :n] = 1.0\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[m] = 1.0\n",
        "\n",
        "    # Inequalities: 2n\n",
        "    # (A) -x_i + tau*y_i <= 0\n",
        "    # (B)  x_i - y_i     <= 0\n",
        "    A_ub = np.zeros((2*n, 2*n))\n",
        "    b_ub = np.zeros(2*n)\n",
        "\n",
        "    for i in range(n):\n",
        "        # (A)\n",
        "        A_ub[i, i] = -1.0\n",
        "        A_ub[i, n + i] = tau\n",
        "        # (B)\n",
        "        A_ub[n + i, i] = 1.0\n",
        "        A_ub[n + i, n + i] = -1.0\n",
        "\n",
        "    bounds = [(0, None)] * n + [(0, 1)] * n\n",
        "\n",
        "    res = linprog(\n",
        "        c,\n",
        "        A_eq=A_eq, b_eq=b_eq,\n",
        "        A_ub=A_ub, b_ub=b_ub,\n",
        "        bounds=bounds,\n",
        "        method=\"highs\"\n",
        "    )\n",
        "    return res\n",
        "\n",
        "def validate_witness(B_sub, x, resid_tol=1e-10, minx_tol=1e-12):\n",
        "    r = B_sub @ x\n",
        "    resid = float(np.max(np.abs(r))) if r.size else 0.0\n",
        "    minx = float(np.min(x)) if x.size else 0.0\n",
        "    ok = (resid <= resid_tol) and (minx >= -minx_tol)\n",
        "    return ok, resid, minx\n",
        "\n",
        "# ============================================================\n",
        "# 5) GICT pipeline with Headman v2 + validation\n",
        "# ============================================================\n",
        "def run_gict_headman_v2(\n",
        "    pnml_file,\n",
        "    eps=1e-6,\n",
        "    tau=1e-4,\n",
        "    x_cover_thr=None,\n",
        "    resid_tol=1e-10,\n",
        "    minx_tol=1e-12,\n",
        "    quiet=False\n",
        "):\n",
        "    if x_cover_thr is None:\n",
        "        x_cover_thr = tau / 2.0\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    places, transitions, arcs, ignored, Bin, Bout, B = parse_pnml_to_matrices(pnml_file)\n",
        "    t_parse = time.perf_counter() - t0\n",
        "\n",
        "    P, T = Bin.shape\n",
        "    approx_arcs = int((Bin > 0).sum() + (Bout > 0).sum())\n",
        "\n",
        "    if not quiet:\n",
        "        print(\"\\n==\", pnml_file, \"==\")\n",
        "        print(\"Parsed PNML:\")\n",
        "        print(\"  #places:\", P)\n",
        "        print(\"  #transitions:\", T)\n",
        "        print(\"  arcs in file:\", len(arcs), \" ignored arcs:\", ignored)\n",
        "        print(\"  approx #nonzero arcs:\", approx_arcs)\n",
        "        print(\"  parse time (s):\", t_parse)\n",
        "\n",
        "    # Baseline\n",
        "    t0 = time.perf_counter()\n",
        "    base_flags = [baseline_transition_cover(B, j, 1.0) for j in range(T)]\n",
        "    t_baseline = time.perf_counter() - t0\n",
        "    base_uncovered = [j for j, ok in enumerate(base_flags) if not ok]\n",
        "\n",
        "    if not quiet:\n",
        "        print(\"\\nBASELINE uncovered transitions (0-based):\", base_uncovered)\n",
        "        print(\"Baseline time (s):\", t_baseline)\n",
        "\n",
        "    # SCC\n",
        "    adj, _rev = build_bipartite_graph(Bin, Bout)\n",
        "    def t_node(j): return P + j\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    sccs = tarjan_scc(adj)\n",
        "    t_scc = time.perf_counter() - t0\n",
        "\n",
        "    scc_id = {}\n",
        "    for k, comp in enumerate(sccs):\n",
        "        for v in comp:\n",
        "            scc_id[v] = k\n",
        "    has_cycle = [scc_has_cycle(comp, adj) for comp in sccs]\n",
        "\n",
        "    labels = [\"Uncertain\"] * T\n",
        "\n",
        "    # Gate-0\n",
        "    t0 = time.perf_counter()\n",
        "    for j in range(T):\n",
        "        sid = scc_id[t_node(j)]\n",
        "        if not has_cycle[sid]:\n",
        "            labels[j] = \"StructurallyUncovered\"\n",
        "    t_gate0 = time.perf_counter() - t0\n",
        "\n",
        "    # Gate-2 per-SCC: dense -> cone-empty -> headman-v2(+validate)\n",
        "    t0 = time.perf_counter()\n",
        "    lp_calls_scc = 0\n",
        "    lp_calls_targeted = 0\n",
        "    coneempty_sccs = 0\n",
        "    nondense_sccs = 0\n",
        "\n",
        "    headman_sccs = 0\n",
        "    headman_valid_sccs = 0\n",
        "    headman_x_support_sizes = []\n",
        "    headman_invalid_info = []\n",
        "\n",
        "    for sid, comp in enumerate(sccs):\n",
        "        t_indices = [v - P for v in comp if v >= P]\n",
        "        if not t_indices:\n",
        "            continue\n",
        "\n",
        "        if not has_cycle[sid]:\n",
        "            for tj in t_indices:\n",
        "                labels[tj] = \"StructurallyUncovered\"\n",
        "            continue\n",
        "\n",
        "        B_sub = B[:, t_indices]\n",
        "\n",
        "        # Dense feasibility\n",
        "        res_dense = dense_witness_simplex(B_sub, eps)\n",
        "        lp_calls_scc += 1\n",
        "        if res_dense.success:\n",
        "            for tj in t_indices:\n",
        "                labels[tj] = \"CoveredWitness\"\n",
        "            continue\n",
        "\n",
        "        # Cone nonempty?\n",
        "        res_cone = cone_nonempty_simplex(B_sub)\n",
        "        lp_calls_scc += 1\n",
        "        if not res_cone.success:\n",
        "            coneempty_sccs += 1\n",
        "            for tj in t_indices:\n",
        "                labels[tj] = \"InvariantConeEmpty\"\n",
        "            continue\n",
        "\n",
        "        # Non-dense but nonempty => headman v2\n",
        "        nondense_sccs += 1\n",
        "        headman_sccs += 1\n",
        "\n",
        "        res_head = lp_support_headman_v2(B_sub, tau=tau)\n",
        "        lp_calls_scc += 1\n",
        "\n",
        "        if not res_head.success:\n",
        "            continue\n",
        "\n",
        "        n = len(t_indices)\n",
        "        sol = np.array(res_head.x, dtype=float)\n",
        "        x_sub = sol[:n]\n",
        "        y_sub = sol[n:]\n",
        "\n",
        "        ok, resid, minx = validate_witness(B_sub, x_sub, resid_tol=resid_tol, minx_tol=minx_tol)\n",
        "        if not ok:\n",
        "            headman_invalid_info.append((sid, resid, minx))\n",
        "            # Do NOT label from this headman; leave as Uncertain for targeted stage\n",
        "            continue\n",
        "\n",
        "        headman_valid_sccs += 1\n",
        "\n",
        "        x_supp = (x_sub >= x_cover_thr)\n",
        "        headman_x_support_sizes.append(int(np.sum(x_supp)))\n",
        "\n",
        "        # Label by x threshold\n",
        "        for k, tj in enumerate(t_indices):\n",
        "            if x_sub[k] >= x_cover_thr:\n",
        "                labels[tj] = \"CoveredWitness\"\n",
        "\n",
        "    t_gate2_scc = time.perf_counter() - t0\n",
        "\n",
        "    # Target remaining Uncertain\n",
        "    t0 = time.perf_counter()\n",
        "    targeted_list = []\n",
        "    for j in range(T):\n",
        "        if labels[j] != \"Uncertain\":\n",
        "            continue\n",
        "\n",
        "        targeted_list.append(j)\n",
        "\n",
        "        sid = scc_id[t_node(j)]\n",
        "        comp = sccs[sid]\n",
        "        t_indices = [v - P for v in comp if v >= P]\n",
        "\n",
        "        if not has_cycle[sid]:\n",
        "            labels[j] = \"StructurallyUncovered\"\n",
        "            continue\n",
        "\n",
        "        res = targeted_cover_in_scc(B, j, t_indices, min_xt=1.0)\n",
        "        lp_calls_targeted += 1\n",
        "        labels[j] = \"CoveredWitness\" if res.success else \"StructurallyUncovered\"\n",
        "\n",
        "    t_gate2_targeted = time.perf_counter() - t0\n",
        "\n",
        "    gict_uncovered = [j for j, lab in enumerate(labels) if lab in (\"StructurallyUncovered\", \"InvariantConeEmpty\")]\n",
        "\n",
        "    counts = defaultdict(int)\n",
        "    for lab in labels:\n",
        "        counts[lab] += 1\n",
        "\n",
        "    if not quiet:\n",
        "        print(\"\\nGICT uncovered transitions (0-based):\", gict_uncovered)\n",
        "        print(\"\\nGICT label histogram:\")\n",
        "        for k in sorted(counts):\n",
        "            print(f\"  {k}: {counts[k]}\")\n",
        "\n",
        "        print(\"\\nGICT timings + LP counts:\")\n",
        "        print(\"  SCC time (s):\", t_scc)\n",
        "        print(\"  Gate-0 time (s):\", t_gate0)\n",
        "        print(\"  Gate-2 per-SCC time (s):\", t_gate2_scc)\n",
        "        print(\"  Gate-2 targeted time (s):\", t_gate2_targeted)\n",
        "        print(\"  LP calls (per-SCC):\", lp_calls_scc)\n",
        "        print(\"  LP calls (targeted):\", lp_calls_targeted)\n",
        "        print(\"  Total LP calls:\", lp_calls_scc + lp_calls_targeted)\n",
        "        print(\"  ConeEmpty SCCs:\", coneempty_sccs)\n",
        "        print(\"  NonDense-but-nonempty SCCs:\", nondense_sccs)\n",
        "\n",
        "        print(\"\\nHeadman v2 params:\")\n",
        "        print(\"  tau:\", tau)\n",
        "        print(\"  x_cover_thr:\", x_cover_thr)\n",
        "        print(\"  resid_tol:\", resid_tol)\n",
        "        print(\"  minx_tol:\", minx_tol)\n",
        "\n",
        "        print(\"\\nSupport-headman v2 stats:\")\n",
        "        print(\"  headman SCCs attempted:\", headman_sccs)\n",
        "        print(\"  headman SCCs valid:\", headman_valid_sccs)\n",
        "        if headman_x_support_sizes:\n",
        "            print(\"  headman x-support sizes:\", headman_x_support_sizes)\n",
        "            print(\"  avg x-support:\", float(np.mean(headman_x_support_sizes)))\n",
        "        if headman_invalid_info:\n",
        "            print(\"\\n  headman invalid SCCs (sid, resid, minx):\")\n",
        "            for sid, resid, minx in headman_invalid_info[:10]:\n",
        "                print(\"   \", sid, resid, minx)\n",
        "            if len(headman_invalid_info) > 10:\n",
        "                print(\"   ...\", len(headman_invalid_info) - 10, \"more\")\n",
        "\n",
        "        if targeted_list:\n",
        "            print(\"\\nTargeted transitions (0-based indices):\", targeted_list)\n",
        "\n",
        "    return {\n",
        "        \"P\": P,\n",
        "        \"T\": T,\n",
        "        \"arcs\": len(arcs),\n",
        "        \"baseline_uncovered_idx\": base_uncovered,\n",
        "        \"gict_uncovered_idx\": gict_uncovered,\n",
        "        \"labels\": dict(counts),\n",
        "        \"lp_total\": lp_calls_scc + lp_calls_targeted,\n",
        "        \"lp_scc\": lp_calls_scc,\n",
        "        \"lp_targeted\": lp_calls_targeted,\n",
        "        \"targeted_list\": targeted_list,\n",
        "        \"headman_valid_sccs\": headman_valid_sccs,\n",
        "        \"headman_invalid_sccs\": len(headman_invalid_info),\n",
        "    }\n",
        "\n",
        "# ============================================================\n",
        "# 6) Run philo + philo_guard\n",
        "# ============================================================\n",
        "ensure_download(PHILO_URL, PHILO_FILE)\n",
        "if not os.path.exists(GUARD_FILE):\n",
        "    raise RuntimeError(\"philo_guard.pnml not found. Create it first (your earlier guard mutation already did).\")\n",
        "else:\n",
        "    print(\"PNML already present:\", GUARD_FILE)\n",
        "\n",
        "# NOTE: tau default set to 1e-4 to defeat numerical fuzz.\n",
        "# If you want more aggressive \"support maximization\", try tau=1e-5 (but keep validation on).\n",
        "r0 = run_gict_headman_v2(PHILO_FILE, eps=1e-6, tau=1e-4)\n",
        "r1 = run_gict_headman_v2(GUARD_FILE, eps=1e-6, tau=1e-4)\n",
        "\n",
        "print(\"\\n============================\")\n",
        "print(\"DIFF SUMMARY [Headman v2 + validation]\")\n",
        "print(\"============================\")\n",
        "print(\"Original: baseline_uncovered=\", len(r0[\"baseline_uncovered_idx\"]),\n",
        "      \"gict_uncovered=\", len(r0[\"gict_uncovered_idx\"]),\n",
        "      \"LP total=\", r0[\"lp_total\"], \"labels=\", r0[\"labels\"],\n",
        "      \"headman_valid_sccs=\", r0[\"headman_valid_sccs\"],\n",
        "      \"headman_invalid_sccs=\", r0[\"headman_invalid_sccs\"])\n",
        "\n",
        "print(\"Guarded:  baseline_uncovered=\", len(r1[\"baseline_uncovered_idx\"]),\n",
        "      \"gict_uncovered=\", len(r1[\"gict_uncovered_idx\"]),\n",
        "      \"LP total=\", r1[\"lp_total\"], \"labels=\", r1[\"labels\"],\n",
        "      \"headman_valid_sccs=\", r1[\"headman_valid_sccs\"],\n",
        "      \"headman_invalid_sccs=\", r1[\"headman_invalid_sccs\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBtNitG40yaD",
        "outputId": "27d58512-e068-49a9-9503-e8b51fe39ce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PNML already present: philo.pnml\n",
            "PNML already present: philo_guard.pnml\n",
            "\n",
            "== philo.pnml ==\n",
            "Parsed PNML:\n",
            "  #places: 30\n",
            "  #transitions: 30\n",
            "  arcs in file: 96  ignored arcs: 0\n",
            "  approx #nonzero arcs: 96\n",
            "  parse time (s): 0.011877956999342132\n",
            "\n",
            "BASELINE uncovered transitions (0-based): []\n",
            "Baseline time (s): 0.30921797300015896\n",
            "\n",
            "GICT uncovered transitions (0-based): []\n",
            "\n",
            "GICT label histogram:\n",
            "  CoveredWitness: 30\n",
            "\n",
            "GICT timings + LP counts:\n",
            "  SCC time (s): 0.00010160700003325474\n",
            "  Gate-0 time (s): 7.78399953560438e-06\n",
            "  Gate-2 per-SCC time (s): 0.026838761999897542\n",
            "  Gate-2 targeted time (s): 5.723999493056908e-06\n",
            "  LP calls (per-SCC): 1\n",
            "  LP calls (targeted): 0\n",
            "  Total LP calls: 1\n",
            "  ConeEmpty SCCs: 0\n",
            "  NonDense-but-nonempty SCCs: 0\n",
            "\n",
            "Headman v2 params:\n",
            "  tau: 0.0001\n",
            "  x_cover_thr: 5e-05\n",
            "  resid_tol: 1e-10\n",
            "  minx_tol: 1e-12\n",
            "\n",
            "Support-headman v2 stats:\n",
            "  headman SCCs attempted: 0\n",
            "  headman SCCs valid: 0\n",
            "\n",
            "== philo_guard.pnml ==\n",
            "Parsed PNML:\n",
            "  #places: 31\n",
            "  #transitions: 30\n",
            "  arcs in file: 97  ignored arcs: 0\n",
            "  approx #nonzero arcs: 97\n",
            "  parse time (s): 0.0171885839999959\n",
            "\n",
            "BASELINE uncovered transitions (0-based): [0, 15]\n",
            "Baseline time (s): 0.2882205719997728\n",
            "\n",
            "GICT uncovered transitions (0-based): [0, 15]\n",
            "\n",
            "GICT label histogram:\n",
            "  CoveredWitness: 28\n",
            "  StructurallyUncovered: 2\n",
            "\n",
            "GICT timings + LP counts:\n",
            "  SCC time (s): 9.034799950313754e-05\n",
            "  Gate-0 time (s): 7.578999429824762e-06\n",
            "  Gate-2 per-SCC time (s): 0.03008162399964931\n",
            "  Gate-2 targeted time (s): 0.012391487000058987\n",
            "  LP calls (per-SCC): 3\n",
            "  LP calls (targeted): 2\n",
            "  Total LP calls: 5\n",
            "  ConeEmpty SCCs: 0\n",
            "  NonDense-but-nonempty SCCs: 1\n",
            "\n",
            "Headman v2 params:\n",
            "  tau: 0.0001\n",
            "  x_cover_thr: 5e-05\n",
            "  resid_tol: 1e-10\n",
            "  minx_tol: 1e-12\n",
            "\n",
            "Support-headman v2 stats:\n",
            "  headman SCCs attempted: 1\n",
            "  headman SCCs valid: 1\n",
            "  headman x-support sizes: [28]\n",
            "  avg x-support: 28.0\n",
            "\n",
            "Targeted transitions (0-based indices): [0, 15]\n",
            "\n",
            "============================\n",
            "DIFF SUMMARY [Headman v2 + validation]\n",
            "============================\n",
            "Original: baseline_uncovered= 0 gict_uncovered= 0 LP total= 1 labels= {'CoveredWitness': 30} headman_valid_sccs= 0 headman_invalid_sccs= 0\n",
            "Guarded:  baseline_uncovered= 2 gict_uncovered= 2 LP total= 5 labels= {'StructurallyUncovered': 2, 'CoveredWitness': 28} headman_valid_sccs= 1 headman_invalid_sccs= 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "# ============================================================\n",
        "# 0) Inputs\n",
        "# ============================================================\n",
        "PHILO_URL  = \"https://www.pnml.org/version-2009/examples/philo.pnml\"\n",
        "PHILO_FILE = \"philo.pnml\"\n",
        "GUARD_FILE = \"philo_guard.pnml\"   # assumes you already created this\n",
        "\n",
        "# ============================================================\n",
        "# 1) PNML parsing\n",
        "# ============================================================\n",
        "def strip_ns(tag):\n",
        "    return tag.split(\"}\", 1)[-1] if \"}\" in tag else tag\n",
        "\n",
        "def ensure_download(url, path):\n",
        "    if not os.path.exists(path):\n",
        "        print(\"Downloading PNML:\", url)\n",
        "        urllib.request.urlretrieve(url, path)\n",
        "    else:\n",
        "        print(\"PNML already present:\", path)\n",
        "\n",
        "def parse_pnml_to_matrices(pnml_file):\n",
        "    tree = ET.parse(pnml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    places = []\n",
        "    transitions = []\n",
        "    arcs = []\n",
        "\n",
        "    for elem in root.iter():\n",
        "        tag = strip_ns(elem.tag)\n",
        "        if tag == \"place\":\n",
        "            pid = elem.attrib.get(\"id\")\n",
        "            if pid:\n",
        "                places.append(pid)\n",
        "        elif tag == \"transition\":\n",
        "            tid = elem.attrib.get(\"id\")\n",
        "            if tid:\n",
        "                transitions.append(tid)\n",
        "        elif tag == \"arc\":\n",
        "            src = elem.attrib.get(\"source\")\n",
        "            tgt = elem.attrib.get(\"target\")\n",
        "            aid = elem.attrib.get(\"id\", \"\")\n",
        "            if src and tgt:\n",
        "                w = 1\n",
        "                for child in elem.iter():\n",
        "                    if strip_ns(child.tag) == \"inscription\":\n",
        "                        for g in child.iter():\n",
        "                            if strip_ns(g.tag) == \"text\" and g.text is not None:\n",
        "                                txt = g.text.strip()\n",
        "                                if txt:\n",
        "                                    try:\n",
        "                                        w = int(txt)\n",
        "                                    except:\n",
        "                                        pass\n",
        "                arcs.append((src, tgt, w, aid))\n",
        "\n",
        "    def uniq(seq):\n",
        "        seen = set()\n",
        "        out = []\n",
        "        for x in seq:\n",
        "            if x not in seen:\n",
        "                out.append(x)\n",
        "                seen.add(x)\n",
        "        return out\n",
        "\n",
        "    places = uniq([p for p in places if p is not None])\n",
        "    transitions = uniq([t for t in transitions if t is not None])\n",
        "\n",
        "    p_index = {p:i for i,p in enumerate(places)}\n",
        "    t_index = {t:i for i,t in enumerate(transitions)}\n",
        "\n",
        "    P = len(places)\n",
        "    T = len(transitions)\n",
        "\n",
        "    Bin = np.zeros((P, T), dtype=float)\n",
        "    Bout = np.zeros((P, T), dtype=float)\n",
        "\n",
        "    ignored = 0\n",
        "    for src, tgt, w, _aid in arcs:\n",
        "        if src in p_index and tgt in t_index:\n",
        "            Bin[p_index[src], t_index[tgt]] += float(w)\n",
        "        elif src in t_index and tgt in p_index:\n",
        "            Bout[p_index[tgt], t_index[src]] += float(w)\n",
        "        else:\n",
        "            ignored += 1\n",
        "\n",
        "    B = Bout - Bin\n",
        "    return places, transitions, arcs, ignored, Bin, Bout, B\n",
        "\n",
        "# ============================================================\n",
        "# 2) Graph + SCC\n",
        "# ============================================================\n",
        "def build_bipartite_graph(Bin, Bout):\n",
        "    P, T = Bin.shape\n",
        "    N = P + T\n",
        "    def t_node(j): return P + j\n",
        "\n",
        "    adj = [[] for _ in range(N)]\n",
        "\n",
        "    for i in range(P):\n",
        "        for j in range(T):\n",
        "            if Bin[i, j] > 0:\n",
        "                adj[i].append(t_node(j))\n",
        "\n",
        "    for i in range(P):\n",
        "        for j in range(T):\n",
        "            if Bout[i, j] > 0:\n",
        "                adj[t_node(j)].append(i)\n",
        "\n",
        "    return adj\n",
        "\n",
        "def tarjan_scc(adj):\n",
        "    index = 0\n",
        "    stack = []\n",
        "    onstack = [False]*len(adj)\n",
        "    idx = [-1]*len(adj)\n",
        "    low = [0]*len(adj)\n",
        "    sccs = []\n",
        "\n",
        "    def strongconnect(v):\n",
        "        nonlocal index\n",
        "        idx[v] = index\n",
        "        low[v] = index\n",
        "        index += 1\n",
        "        stack.append(v)\n",
        "        onstack[v] = True\n",
        "\n",
        "        for w in adj[v]:\n",
        "            if idx[w] == -1:\n",
        "                strongconnect(w)\n",
        "                low[v] = min(low[v], low[w])\n",
        "            elif onstack[w]:\n",
        "                low[v] = min(low[v], idx[w])\n",
        "\n",
        "        if low[v] == idx[v]:\n",
        "            comp = []\n",
        "            while True:\n",
        "                w = stack.pop()\n",
        "                onstack[w] = False\n",
        "                comp.append(w)\n",
        "                if w == v:\n",
        "                    break\n",
        "            sccs.append(comp)\n",
        "\n",
        "    for v in range(len(adj)):\n",
        "        if idx[v] == -1:\n",
        "            strongconnect(v)\n",
        "    return sccs\n",
        "\n",
        "def scc_has_cycle(comp, adj):\n",
        "    if len(comp) > 1:\n",
        "        return True\n",
        "    v = comp[0]\n",
        "    return v in adj[v]\n",
        "\n",
        "# ============================================================\n",
        "# 3) LP primitives\n",
        "# ============================================================\n",
        "def baseline_transition_cover(B, t_idx, min_xt=1.0):\n",
        "    m, n = B.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[t_idx] = (min_xt, None)\n",
        "    res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "    return res.success\n",
        "\n",
        "def dense_witness_simplex(B_sub, eps):\n",
        "    # Feasibility: B_sub x = 0, x >= eps, sum(x)=1\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = np.vstack([B_sub, np.ones((1, n))])\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[-1] = 1.0\n",
        "    bounds = [(eps, None)] * n\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "def targeted_cover_in_scc(B_full, t_global, t_indices, min_xt=1.0):\n",
        "    pos = t_indices.index(t_global)\n",
        "    B_sub = B_full[:, t_indices]\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(n)\n",
        "    A_eq = B_sub\n",
        "    b_eq = np.zeros(m)\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[pos] = (min_xt, None)\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\"highs\")\n",
        "\n",
        "# ============================================================\n",
        "# 4) Headman v2 (as cone-nonempty test) + validation\n",
        "# ============================================================\n",
        "def headman_v2_lp(B_sub, tau=1e-4):\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(2*n)\n",
        "    c[n:] = -1.0  # maximize sum(y)\n",
        "\n",
        "    A_eq = np.zeros((m + 1, 2*n))\n",
        "    A_eq[:m, :n] = B_sub\n",
        "    A_eq[m, :n] = 1.0\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[m] = 1.0\n",
        "\n",
        "    A_ub = np.zeros((2*n, 2*n))\n",
        "    b_ub = np.zeros(2*n)\n",
        "\n",
        "    for i in range(n):\n",
        "        # (A) -x_i + tau*y_i <= 0\n",
        "        A_ub[i, i] = -1.0\n",
        "        A_ub[i, n + i] = tau\n",
        "        # (B) x_i - y_i <= 0\n",
        "        A_ub[n + i, i] = 1.0\n",
        "        A_ub[n + i, n + i] = -1.0\n",
        "\n",
        "    bounds = [(0, None)] * n + [(0, 1)] * n\n",
        "    return linprog(c, A_eq=A_eq, b_eq=b_eq, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method=\"highs\")\n",
        "\n",
        "def validate_witness(B_sub, x, resid_tol=1e-10, minx_tol=1e-12):\n",
        "    r = B_sub @ x\n",
        "    resid = float(np.max(np.abs(r))) if r.size else 0.0\n",
        "    minx = float(np.min(x)) if x.size else 0.0\n",
        "    ok = (resid <= resid_tol) and (minx >= -minx_tol)\n",
        "    return ok, resid, minx\n",
        "\n",
        "# ============================================================\n",
        "# 5) GICT with merged cone check:\n",
        "#    per SCC:\n",
        "#      - if not cyclic => uncovered\n",
        "#      - try dense witness\n",
        "#      - else try headman v2:\n",
        "#          if infeasible => cone empty => InvariantConeEmpty\n",
        "#          else if valid => mark x-support covered; leave rest Uncertain\n",
        "#          else (numerically invalid) => fall back to targeted for entire SCC\n",
        "# ============================================================\n",
        "def run_gict_merged_cone_headman(\n",
        "    pnml_file,\n",
        "    eps=1e-6,\n",
        "    tau=1e-4,\n",
        "    x_cover_thr=None,\n",
        "    resid_tol=1e-10,\n",
        "    minx_tol=1e-12,\n",
        "    quiet=False\n",
        "):\n",
        "    if x_cover_thr is None:\n",
        "        x_cover_thr = tau / 2.0\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    places, transitions, arcs, ignored, Bin, Bout, B = parse_pnml_to_matrices(pnml_file)\n",
        "    t_parse = time.perf_counter() - t0\n",
        "\n",
        "    P, T = Bin.shape\n",
        "    approx_arcs = int((Bin > 0).sum() + (Bout > 0).sum())\n",
        "\n",
        "    if not quiet:\n",
        "        print(\"\\n==\", pnml_file, \"==\")\n",
        "        print(\"Parsed PNML:\")\n",
        "        print(\"  #places:\", P)\n",
        "        print(\"  #transitions:\", T)\n",
        "        print(\"  arcs in file:\", len(arcs), \" ignored arcs:\", ignored)\n",
        "        print(\"  approx #nonzero arcs:\", approx_arcs)\n",
        "        print(\"  parse time (s):\", t_parse)\n",
        "\n",
        "    # Baseline\n",
        "    t0 = time.perf_counter()\n",
        "    base_flags = [baseline_transition_cover(B, j, 1.0) for j in range(T)]\n",
        "    t_baseline = time.perf_counter() - t0\n",
        "    base_uncovered = [j for j, ok in enumerate(base_flags) if not ok]\n",
        "\n",
        "    if not quiet:\n",
        "        print(\"\\nBASELINE uncovered transitions (0-based):\", base_uncovered)\n",
        "        print(\"Baseline time (s):\", t_baseline)\n",
        "\n",
        "    # SCC\n",
        "    adj = build_bipartite_graph(Bin, Bout)\n",
        "    def t_node(j): return P + j\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    sccs = tarjan_scc(adj)\n",
        "    t_scc = time.perf_counter() - t0\n",
        "\n",
        "    scc_id = {}\n",
        "    for k, comp in enumerate(sccs):\n",
        "        for v in comp:\n",
        "            scc_id[v] = k\n",
        "    has_cycle = [scc_has_cycle(comp, adj) for comp in sccs]\n",
        "\n",
        "    labels = [\"Uncertain\"] * T\n",
        "\n",
        "    # Gate-0\n",
        "    t0 = time.perf_counter()\n",
        "    for j in range(T):\n",
        "        sid = scc_id[t_node(j)]\n",
        "        if not has_cycle[sid]:\n",
        "            labels[j] = \"StructurallyUncovered\"\n",
        "    t_gate0 = time.perf_counter() - t0\n",
        "\n",
        "    # Gate-2 per-SCC (merged)\n",
        "    t0 = time.perf_counter()\n",
        "    lp_calls_scc = 0\n",
        "    lp_calls_targeted = 0\n",
        "    coneempty_sccs = 0\n",
        "    nondense_sccs = 0\n",
        "    headman_attempted = 0\n",
        "    headman_valid = 0\n",
        "    headman_invalid = 0\n",
        "    headman_x_support_sizes = []\n",
        "    scc_force_targeted = set()  # SCCs whose headman witness was invalid -> target all transitions in SCC\n",
        "\n",
        "    for sid, comp in enumerate(sccs):\n",
        "        t_indices = [v - P for v in comp if v >= P]\n",
        "        if not t_indices:\n",
        "            continue\n",
        "\n",
        "        if not has_cycle[sid]:\n",
        "            for tj in t_indices:\n",
        "                labels[tj] = \"StructurallyUncovered\"\n",
        "            continue\n",
        "\n",
        "        B_sub = B[:, t_indices]\n",
        "\n",
        "        # Dense witness\n",
        "        res_dense = dense_witness_simplex(B_sub, eps)\n",
        "        lp_calls_scc += 1\n",
        "        if res_dense.success:\n",
        "            for tj in t_indices:\n",
        "                labels[tj] = \"CoveredWitness\"\n",
        "            continue\n",
        "\n",
        "        # Not dense, try headman (this also implies cone-nonempty if feasible)\n",
        "        nondense_sccs += 1\n",
        "        headman_attempted += 1\n",
        "        res_head = headman_v2_lp(B_sub, tau=tau)\n",
        "        lp_calls_scc += 1\n",
        "\n",
        "        if not res_head.success:\n",
        "            # infeasible => no x>=0, sum(x)=1 solution => cone empty\n",
        "            coneempty_sccs += 1\n",
        "            for tj in t_indices:\n",
        "                labels[tj] = \"InvariantConeEmpty\"\n",
        "            continue\n",
        "\n",
        "        # feasible -> validate witness\n",
        "        n = len(t_indices)\n",
        "        sol = np.array(res_head.x, dtype=float)\n",
        "        x_sub = sol[:n]\n",
        "\n",
        "        ok, resid, minx = validate_witness(B_sub, x_sub, resid_tol=resid_tol, minx_tol=minx_tol)\n",
        "        if not ok:\n",
        "            headman_invalid += 1\n",
        "            scc_force_targeted.add(sid)\n",
        "            # Leave labels as Uncertain; we will target all transitions in this SCC\n",
        "            continue\n",
        "\n",
        "        headman_valid += 1\n",
        "        x_supp = (x_sub >= x_cover_thr)\n",
        "        headman_x_support_sizes.append(int(np.sum(x_supp)))\n",
        "\n",
        "        # Label by x threshold\n",
        "        for k, tj in enumerate(t_indices):\n",
        "            if x_sub[k] >= x_cover_thr:\n",
        "                labels[tj] = \"CoveredWitness\"\n",
        "\n",
        "    t_gate2_scc = time.perf_counter() - t0\n",
        "\n",
        "    # Target remaining Uncertain (optionally forced for whole SCC if headman invalid)\n",
        "    t0 = time.perf_counter()\n",
        "    targeted_list = []\n",
        "    for j in range(T):\n",
        "        if labels[j] != \"Uncertain\":\n",
        "            continue\n",
        "\n",
        "        sid = scc_id[t_node(j)]\n",
        "        if sid in scc_force_targeted:\n",
        "            # force targeted check for this SCC's transitions\n",
        "            comp = sccs[sid]\n",
        "            t_indices = [v - P for v in comp if v >= P]\n",
        "            for tj in t_indices:\n",
        "                if labels[tj] == \"Uncertain\":\n",
        "                    targeted_list.append(tj)\n",
        "                    res = targeted_cover_in_scc(B, tj, t_indices, min_xt=1.0)\n",
        "                    lp_calls_targeted += 1\n",
        "                    labels[tj] = \"CoveredWitness\" if res.success else \"StructurallyUncovered\"\n",
        "            continue\n",
        "\n",
        "        # normal targeted just for this transition\n",
        "        targeted_list.append(j)\n",
        "        comp = sccs[sid]\n",
        "        t_indices = [v - P for v in comp if v >= P]\n",
        "        res = targeted_cover_in_scc(B, j, t_indices, min_xt=1.0)\n",
        "        lp_calls_targeted += 1\n",
        "        labels[j] = \"CoveredWitness\" if res.success else \"StructurallyUncovered\"\n",
        "\n",
        "    t_gate2_targeted = time.perf_counter() - t0\n",
        "\n",
        "    gict_uncovered = [j for j, lab in enumerate(labels) if lab in (\"StructurallyUncovered\", \"InvariantConeEmpty\")]\n",
        "\n",
        "    counts = defaultdict(int)\n",
        "    for lab in labels:\n",
        "        counts[lab] += 1\n",
        "\n",
        "    if not quiet:\n",
        "        print(\"\\nGICT uncovered transitions (0-based):\", gict_uncovered)\n",
        "        print(\"\\nGICT label histogram:\")\n",
        "        for k in sorted(counts):\n",
        "            print(f\"  {k}: {counts[k]}\")\n",
        "\n",
        "        print(\"\\nGICT timings + LP counts:\")\n",
        "        print(\"  SCC time (s):\", t_scc)\n",
        "        print(\"  Gate-0 time (s):\", t_gate0)\n",
        "        print(\"  Gate-2 per-SCC time (s):\", t_gate2_scc)\n",
        "        print(\"  Gate-2 targeted time (s):\", t_gate2_targeted)\n",
        "        print(\"  LP calls (per-SCC):\", lp_calls_scc)\n",
        "        print(\"  LP calls (targeted):\", lp_calls_targeted)\n",
        "        print(\"  Total LP calls:\", lp_calls_scc + lp_calls_targeted)\n",
        "        print(\"  ConeEmpty SCCs:\", coneempty_sccs)\n",
        "        print(\"  NonDense SCCs:\", nondense_sccs)\n",
        "\n",
        "        print(\"\\nMerged headman params:\")\n",
        "        print(\"  tau:\", tau)\n",
        "        print(\"  x_cover_thr:\", x_cover_thr)\n",
        "        print(\"  resid_tol:\", resid_tol)\n",
        "        print(\"  minx_tol:\", minx_tol)\n",
        "\n",
        "        print(\"\\nHeadman stats:\")\n",
        "        print(\"  headman attempted:\", headman_attempted)\n",
        "        print(\"  headman valid:\", headman_valid)\n",
        "        print(\"  headman invalid:\", headman_invalid)\n",
        "        if headman_x_support_sizes:\n",
        "            print(\"  headman x-support sizes:\", headman_x_support_sizes)\n",
        "            print(\"  avg x-support:\", float(np.mean(headman_x_support_sizes)))\n",
        "\n",
        "        if targeted_list:\n",
        "            print(\"\\nTargeted transitions (0-based indices):\", targeted_list)\n",
        "\n",
        "    return {\n",
        "        \"P\": P,\n",
        "        \"T\": T,\n",
        "        \"arcs\": len(arcs),\n",
        "        \"baseline_uncovered_idx\": base_uncovered,\n",
        "        \"gict_uncovered_idx\": gict_uncovered,\n",
        "        \"labels\": dict(counts),\n",
        "        \"lp_total\": lp_calls_scc + lp_calls_targeted,\n",
        "        \"lp_scc\": lp_calls_scc,\n",
        "        \"lp_targeted\": lp_calls_targeted,\n",
        "        \"targeted_list\": targeted_list,\n",
        "        \"headman_valid\": headman_valid,\n",
        "        \"headman_invalid\": headman_invalid,\n",
        "        \"coneempty_sccs\": coneempty_sccs,\n",
        "    }\n",
        "\n",
        "# ============================================================\n",
        "# 6) Run philo + philo_guard\n",
        "# ============================================================\n",
        "ensure_download(PHILO_URL, PHILO_FILE)\n",
        "if not os.path.exists(GUARD_FILE):\n",
        "    raise RuntimeError(\"philo_guard.pnml not found. Create it first (your earlier guard mutation already did).\")\n",
        "else:\n",
        "    print(\"PNML already present:\", GUARD_FILE)\n",
        "\n",
        "r0 = run_gict_merged_cone_headman(PHILO_FILE, eps=1e-6, tau=1e-4)\n",
        "r1 = run_gict_merged_cone_headman(GUARD_FILE, eps=1e-6, tau=1e-4)\n",
        "\n",
        "print(\"\\n============================\")\n",
        "print(\"DIFF SUMMARY [Merged Cone + Headman]\")\n",
        "print(\"============================\")\n",
        "print(\"Original: baseline_uncovered=\", len(r0[\"baseline_uncovered_idx\"]),\n",
        "      \"gict_uncovered=\", len(r0[\"gict_uncovered_idx\"]),\n",
        "      \"LP total=\", r0[\"lp_total\"], \"labels=\", r0[\"labels\"],\n",
        "      \"coneempty_sccs=\", r0[\"coneempty_sccs\"],\n",
        "      \"headman_invalid=\", r0[\"headman_invalid\"])\n",
        "\n",
        "print(\"Guarded:  baseline_uncovered=\", len(r1[\"baseline_uncovered_idx\"]),\n",
        "      \"gict_uncovered=\", len(r1[\"gict_uncovered_idx\"]),\n",
        "      \"LP total=\", r1[\"lp_total\"], \"labels=\", r1[\"labels\"],\n",
        "      \"coneempty_sccs=\", r1[\"coneempty_sccs\"],\n",
        "      \"headman_invalid=\", r1[\"headman_invalid\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyzw9WNI14Nv",
        "outputId": "ce958e8a-ddf3-4309-ea43-23c8dcf17ee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PNML already present: philo.pnml\n",
            "PNML already present: philo_guard.pnml\n",
            "\n",
            "== philo.pnml ==\n",
            "Parsed PNML:\n",
            "  #places: 30\n",
            "  #transitions: 30\n",
            "  arcs in file: 96  ignored arcs: 0\n",
            "  approx #nonzero arcs: 96\n",
            "  parse time (s): 0.004148970999267476\n",
            "\n",
            "BASELINE uncovered transitions (0-based): []\n",
            "Baseline time (s): 0.1790010020004047\n",
            "\n",
            "GICT uncovered transitions (0-based): []\n",
            "\n",
            "GICT label histogram:\n",
            "  CoveredWitness: 30\n",
            "\n",
            "GICT timings + LP counts:\n",
            "  SCC time (s): 9.302000034949742e-05\n",
            "  Gate-0 time (s): 9.521000720269512e-06\n",
            "  Gate-2 per-SCC time (s): 0.004172676999587566\n",
            "  Gate-2 targeted time (s): 4.529999387159478e-06\n",
            "  LP calls (per-SCC): 1\n",
            "  LP calls (targeted): 0\n",
            "  Total LP calls: 1\n",
            "  ConeEmpty SCCs: 0\n",
            "  NonDense SCCs: 0\n",
            "\n",
            "Merged headman params:\n",
            "  tau: 0.0001\n",
            "  x_cover_thr: 5e-05\n",
            "  resid_tol: 1e-10\n",
            "  minx_tol: 1e-12\n",
            "\n",
            "Headman stats:\n",
            "  headman attempted: 0\n",
            "  headman valid: 0\n",
            "  headman invalid: 0\n",
            "\n",
            "== philo_guard.pnml ==\n",
            "Parsed PNML:\n",
            "  #places: 31\n",
            "  #transitions: 30\n",
            "  arcs in file: 97  ignored arcs: 0\n",
            "  approx #nonzero arcs: 97\n",
            "  parse time (s): 0.0049872619993038825\n",
            "\n",
            "BASELINE uncovered transitions (0-based): [0, 15]\n",
            "Baseline time (s): 0.17104275600013352\n",
            "\n",
            "GICT uncovered transitions (0-based): [0, 15]\n",
            "\n",
            "GICT label histogram:\n",
            "  CoveredWitness: 28\n",
            "  StructurallyUncovered: 2\n",
            "\n",
            "GICT timings + LP counts:\n",
            "  SCC time (s): 9.871199927147245e-05\n",
            "  Gate-0 time (s): 7.982000170159154e-06\n",
            "  Gate-2 per-SCC time (s): 0.0074482709997028\n",
            "  Gate-2 targeted time (s): 0.0052227590003894875\n",
            "  LP calls (per-SCC): 2\n",
            "  LP calls (targeted): 2\n",
            "  Total LP calls: 4\n",
            "  ConeEmpty SCCs: 0\n",
            "  NonDense SCCs: 1\n",
            "\n",
            "Merged headman params:\n",
            "  tau: 0.0001\n",
            "  x_cover_thr: 5e-05\n",
            "  resid_tol: 1e-10\n",
            "  minx_tol: 1e-12\n",
            "\n",
            "Headman stats:\n",
            "  headman attempted: 1\n",
            "  headman valid: 1\n",
            "  headman invalid: 0\n",
            "  headman x-support sizes: [28]\n",
            "  avg x-support: 28.0\n",
            "\n",
            "Targeted transitions (0-based indices): [0, 15]\n",
            "\n",
            "============================\n",
            "DIFF SUMMARY [Merged Cone + Headman]\n",
            "============================\n",
            "Original: baseline_uncovered= 0 gict_uncovered= 0 LP total= 1 labels= {'CoveredWitness': 30} coneempty_sccs= 0 headman_invalid= 0\n",
            "Guarded:  baseline_uncovered= 2 gict_uncovered= 2 LP total= 4 labels= {'StructurallyUncovered': 2, 'CoveredWitness': 28} coneempty_sccs= 0 headman_invalid= 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from datetime import datetime\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "# ============================================================\n",
        "# USER SETTINGS (minimize output for ChatGPT UI / mobile)\n",
        "# ============================================================\n",
        "QUIET = True          # prints 1 summary line per net (+ disagreements)\n",
        "MAKE_ZIP = True       # zip all PNML files at end\n",
        "ZIP_NAME = \"pnml_suite.zip\"\n",
        "\n",
        "# Headman / GICT parameters\n",
        "EPS = 1e-6\n",
        "TAU = 1e-4\n",
        "X_COVER_THR = TAU / 2.0\n",
        "RESID_TOL = 1e-10\n",
        "MINX_TOL = 1e-12\n",
        "\n",
        "# PNML sources\n",
        "PHILO_URL   = \"https://www.pnml.org/version-2009/examples/philo.pnml\"\n",
        "PISCINE_URL = \"https://www.pnml.org/version-2009/examples/Piscine.pnml\"\n",
        "TOKEN_URL   = \"https://www.pnml.org/version-2009/examples/Token-ring.pnml\"\n",
        "\n",
        "PHILO_FILE   = \"philo.pnml\"\n",
        "PISCINE_FILE = \"Piscine.pnml\"\n",
        "TOKEN_FILE   = \"Token-ring.pnml\"\n",
        "\n",
        "# Mutant filenames\n",
        "PHILO_GUARD   = \"philo_guard.pnml\"     # assumed already exists; if not, we create it\n",
        "PHILO_LEAK    = \"philo_leak.pnml\"      # optional; if exists we keep it\n",
        "\n",
        "PISCINE_GUARD = \"Piscine_guard.pnml\"\n",
        "PISCINE_LEAK  = \"Piscine_leak.pnml\"\n",
        "\n",
        "TOKEN_GUARD   = \"Token-ring_guard.pnml\"\n",
        "TOKEN_LEAK    = \"Token-ring_leak.pnml\"\n",
        "\n",
        "# ============================================================\n",
        "# 0) Helpers\n",
        "# ============================================================\n",
        "def strip_ns(tag):\n",
        "    return tag.split(\"}\", 1)[-1] if \"}\" in tag else tag\n",
        "\n",
        "def ensure_download(url, path):\n",
        "    if not os.path.exists(path):\n",
        "        urllib.request.urlretrieve(url, path)\n",
        "\n",
        "def find_first(elem, localname):\n",
        "    for e in elem.iter():\n",
        "        if strip_ns(e.tag) == localname:\n",
        "            return e\n",
        "    return None\n",
        "\n",
        "def find_all(elem, localname):\n",
        "    out = []\n",
        "    for e in elem.iter():\n",
        "        if strip_ns(e.tag) == localname:\n",
        "            out.append(e)\n",
        "    return out\n",
        "\n",
        "def get_pnml_net_root(tree):\n",
        "    root = tree.getroot()\n",
        "    net = None\n",
        "    for e in root.iter():\n",
        "        if strip_ns(e.tag) == \"net\":\n",
        "            net = e\n",
        "            break\n",
        "    if net is None:\n",
        "        raise RuntimeError(\"No <net> element found in PNML.\")\n",
        "    return root, net\n",
        "\n",
        "def existing_ids(net):\n",
        "    ids = set()\n",
        "    for e in net.iter():\n",
        "        i = e.attrib.get(\"id\")\n",
        "        if i:\n",
        "            ids.add(i)\n",
        "    return ids\n",
        "\n",
        "def unique_id(base, used):\n",
        "    if base not in used:\n",
        "        used.add(base)\n",
        "        return base\n",
        "    k = 1\n",
        "    while f\"{base}_{k}\" in used:\n",
        "        k += 1\n",
        "    uid = f\"{base}_{k}\"\n",
        "    used.add(uid)\n",
        "    return uid\n",
        "\n",
        "def get_place_and_transition_ids(net):\n",
        "    places = []\n",
        "    transitions = []\n",
        "    for e in net.iter():\n",
        "        if strip_ns(e.tag) == \"place\":\n",
        "            pid = e.attrib.get(\"id\")\n",
        "            if pid:\n",
        "                places.append(pid)\n",
        "        elif strip_ns(e.tag) == \"transition\":\n",
        "            tid = e.attrib.get(\"id\")\n",
        "            if tid:\n",
        "                transitions.append(tid)\n",
        "    return places, transitions\n",
        "\n",
        "def pretty_name_tag(name_text):\n",
        "    name = ET.Element(\"name\")\n",
        "    text = ET.SubElement(name, \"text\")\n",
        "    text.text = str(name_text)\n",
        "    return name\n",
        "\n",
        "# ============================================================\n",
        "# 1) PNML mutation: LEAK (place -> new transition)\n",
        "# ============================================================\n",
        "def mutate_add_leak_transition(src_pnml, out_pnml, leak_name=\"LEAK\"):\n",
        "    tree = ET.parse(src_pnml)\n",
        "    root, net = get_pnml_net_root(tree)\n",
        "    used = existing_ids(net)\n",
        "\n",
        "    places, transitions = get_place_and_transition_ids(net)\n",
        "    if not places:\n",
        "        raise RuntimeError(f\"{src_pnml}: no places found.\")\n",
        "    if not transitions:\n",
        "        raise RuntimeError(f\"{src_pnml}: no transitions found.\")\n",
        "\n",
        "    # pick a \"busy\" place: just take first place\n",
        "    p_src = places[0]\n",
        "\n",
        "    # add transition\n",
        "    t_id = unique_id(\"t_leak\", used)\n",
        "    t = ET.Element(\"transition\", {\"id\": t_id})\n",
        "    t.append(pretty_name_tag(leak_name))\n",
        "    net.append(t)\n",
        "\n",
        "    # add arc place -> transition\n",
        "    a_id = unique_id(\"a_leak\", used)\n",
        "    arc = ET.Element(\"arc\", {\"id\": a_id, \"source\": p_src, \"target\": t_id})\n",
        "    net.append(arc)\n",
        "\n",
        "    tree.write(out_pnml, encoding=\"utf-8\", xml_declaration=True)\n",
        "    return {\"added_transition\": t_id, \"src_place\": p_src, \"added_arc\": a_id}\n",
        "\n",
        "# ============================================================\n",
        "# 2) PNML mutation: GUARD (new zero-marked place -> existing transition)\n",
        "# ============================================================\n",
        "def mutate_add_guard_place(src_pnml, out_pnml, guard_name=\"P_GUARD\", target_transition_idx=0):\n",
        "    tree = ET.parse(src_pnml)\n",
        "    root, net = get_pnml_net_root(tree)\n",
        "    used = existing_ids(net)\n",
        "\n",
        "    places, transitions = get_place_and_transition_ids(net)\n",
        "    if not transitions:\n",
        "        raise RuntimeError(f\"{src_pnml}: no transitions found.\")\n",
        "    if target_transition_idx < 0 or target_transition_idx >= len(transitions):\n",
        "        target_transition_idx = 0\n",
        "    t_target = transitions[target_transition_idx]\n",
        "\n",
        "    # add new guard place (no marking => 0 by default)\n",
        "    p_id = unique_id(\"p_guard\", used)\n",
        "    p = ET.Element(\"place\", {\"id\": p_id})\n",
        "    p.append(pretty_name_tag(guard_name))\n",
        "    net.append(p)\n",
        "\n",
        "    # add arc guard place -> target transition\n",
        "    a_id = unique_id(\"a_guard\", used)\n",
        "    arc = ET.Element(\"arc\", {\"id\": a_id, \"source\": p_id, \"target\": t_target})\n",
        "    net.append(arc)\n",
        "\n",
        "    tree.write(out_pnml, encoding=\"utf-8\", xml_declaration=True)\n",
        "    return {\"added_place\": p_id, \"target_transition\": t_target, \"added_arc\": a_id}\n",
        "\n",
        "# ============================================================\n",
        "# 3) PNML parsing -> matrices (Bin, Bout, B)\n",
        "# ============================================================\n",
        "def parse_pnml_to_matrices(pnml_file):\n",
        "    tree = ET.parse(pnml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    places = []\n",
        "    transitions = []\n",
        "    arcs = []\n",
        "\n",
        "    for elem in root.iter():\n",
        "        tag = strip_ns(elem.tag)\n",
        "        if tag == \"place\":\n",
        "            pid = elem.attrib.get(\"id\")\n",
        "            if pid:\n",
        "                places.append(pid)\n",
        "        elif tag == \"transition\":\n",
        "            tid = elem.attrib.get(\"id\")\n",
        "            if tid:\n",
        "                transitions.append(tid)\n",
        "        elif tag == \"arc\":\n",
        "            src = elem.attrib.get(\"source\")\n",
        "            tgt = elem.attrib.get(\"target\")\n",
        "            aid = elem.attrib.get(\"id\", \"\")\n",
        "            if src and tgt:\n",
        "                w = 1\n",
        "                for child in elem.iter():\n",
        "                    if strip_ns(child.tag) == \"inscription\":\n",
        "                        for g in child.iter():\n",
        "                            if strip_ns(g.tag) == \"text\" and g.text is not None:\n",
        "                                txt = g.text.strip()\n",
        "                                if txt:\n",
        "                                    try:\n",
        "                                        w = int(txt)\n",
        "                                    except:\n",
        "                                        pass\n",
        "                arcs.append((src, tgt, w, aid))\n",
        "\n",
        "    def uniq(seq):\n",
        "        seen = set()\n",
        "        out = []\n",
        "        for x in seq:\n",
        "            if x not in seen:\n",
        "                out.append(x)\n",
        "                seen.add(x)\n",
        "        return out\n",
        "\n",
        "    places = uniq([p for p in places if p is not None])\n",
        "    transitions = uniq([t for t in transitions if t is not None])\n",
        "\n",
        "    p_index = {p:i for i,p in enumerate(places)}\n",
        "    t_index = {t:i for i,t in enumerate(transitions)}\n",
        "\n",
        "    P = len(places)\n",
        "    T = len(transitions)\n",
        "\n",
        "    Bin = np.zeros((P, T), dtype=float)\n",
        "    Bout = np.zeros((P, T), dtype=float)\n",
        "\n",
        "    ignored = 0\n",
        "    for src, tgt, w, _aid in arcs:\n",
        "        if src in p_index and tgt in t_index:\n",
        "            Bin[p_index[src], t_index[tgt]] += float(w)\n",
        "        elif src in t_index and tgt in p_index:\n",
        "            Bout[p_index[tgt], t_index[src]] += float(w)\n",
        "        else:\n",
        "            ignored += 1\n",
        "\n",
        "    B = Bout - Bin\n",
        "    return places, transitions, arcs, ignored, Bin, Bout, B\n",
        "\n",
        "# ============================================================\n",
        "# 4) Graph + SCC\n",
        "# ============================================================\n",
        "def build_bipartite_graph(Bin, Bout):\n",
        "    P, T = Bin.shape\n",
        "    N = P + T\n",
        "    def t_node(j): return P + j\n",
        "    adj = [[] for _ in range(N)]\n",
        "\n",
        "    for i in range(P):\n",
        "        for j in range(T):\n",
        "            if Bin[i, j] > 0:\n",
        "                adj[i].append(t_node(j))\n",
        "\n",
        "    for i in range(P):\n",
        "        for j in range(T):\n",
        "            if Bout[i, j] > 0:\n",
        "                adj[t_node(j)].append(i)\n",
        "\n",
        "    return adj\n",
        "\n",
        "def tarjan_scc(adj):\n",
        "    index = 0\n",
        "    stack = []\n",
        "    onstack = [False]*len(adj)\n",
        "    idx = [-1]*len(adj)\n",
        "    low = [0]*len(adj)\n",
        "    sccs = []\n",
        "\n",
        "    def strongconnect(v):\n",
        "        nonlocal index\n",
        "        idx[v] = index\n",
        "        low[v] = index\n",
        "        index += 1\n",
        "        stack.append(v)\n",
        "        onstack[v] = True\n",
        "\n",
        "        for w in adj[v]:\n",
        "            if idx[w] == -1:\n",
        "                strongconnect(w)\n",
        "                low[v] = min(low[v], low[w])\n",
        "            elif onstack[w]:\n",
        "                low[v] = min(low[v], idx[w])\n",
        "\n",
        "        if low[v] == idx[v]:\n",
        "            comp = []\n",
        "            while True:\n",
        "                w = stack.pop()\n",
        "                onstack[w] = False\n",
        "                comp.append(w)\n",
        "                if w == v:\n",
        "                    break\n",
        "            sccs.append(comp)\n",
        "\n",
        "    for v in range(len(adj)):\n",
        "        if idx[v] == -1:\n",
        "            strongconnect(v)\n",
        "    return sccs\n",
        "\n",
        "def scc_has_cycle(comp, adj):\n",
        "    if len(comp) > 1:\n",
        "        return True\n",
        "    v = comp[0]\n",
        "    return v in adj[v]\n",
        "\n",
        "# ============================================================\n",
        "# 5) LP primitives\n",
        "# ============================================================\n",
        "def baseline_transition_cover(B, t_idx, min_xt=1.0):\n",
        "    m, n = B.shape\n",
        "    c = np.zeros(n)\n",
        "    res = linprog(\n",
        "        c,\n",
        "        A_eq=B,\n",
        "        b_eq=np.zeros(m),\n",
        "        bounds=[(0, None)] * n,\n",
        "        method=\"highs\"\n",
        "    )\n",
        "    if not res.success:\n",
        "        return False\n",
        "    # now enforce x[t] >= min_xt\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[t_idx] = (min_xt, None)\n",
        "    res2 = linprog(\n",
        "        c,\n",
        "        A_eq=B,\n",
        "        b_eq=np.zeros(m),\n",
        "        bounds=bounds,\n",
        "        method=\"highs\"\n",
        "    )\n",
        "    return res2.success\n",
        "\n",
        "def dense_witness_simplex(B_sub, eps):\n",
        "    m, n = B_sub.shape\n",
        "    A_eq = np.vstack([B_sub, np.ones((1, n))])\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[-1] = 1.0\n",
        "    return linprog(\n",
        "        np.zeros(n),\n",
        "        A_eq=A_eq,\n",
        "        b_eq=b_eq,\n",
        "        bounds=[(eps, None)] * n,\n",
        "        method=\"highs\"\n",
        "    )\n",
        "\n",
        "def targeted_cover_in_scc(B_full, t_global, t_indices, min_xt=1.0):\n",
        "    pos = t_indices.index(t_global)\n",
        "    B_sub = B_full[:, t_indices]\n",
        "    m, n = B_sub.shape\n",
        "    bounds = [(0, None)] * n\n",
        "    bounds = list(bounds)\n",
        "    bounds[pos] = (min_xt, None)\n",
        "    return linprog(\n",
        "        np.zeros(n),\n",
        "        A_eq=B_sub,\n",
        "        b_eq=np.zeros(m),\n",
        "        bounds=bounds,\n",
        "        method=\"highs\"\n",
        "    )\n",
        "\n",
        "# ============================================================\n",
        "# 6) Headman v2 + validation\n",
        "# ============================================================\n",
        "def headman_v2_lp(B_sub, tau=1e-4):\n",
        "    m, n = B_sub.shape\n",
        "    c = np.zeros(2*n)\n",
        "    c[n:] = -1.0  # maximize sum(y)\n",
        "\n",
        "    A_eq = np.zeros((m + 1, 2*n))\n",
        "    A_eq[:m, :n] = B_sub\n",
        "    A_eq[m, :n] = 1.0\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[m] = 1.0\n",
        "\n",
        "    A_ub = np.zeros((2*n, 2*n))\n",
        "    b_ub = np.zeros(2*n)\n",
        "\n",
        "    for i in range(n):\n",
        "        # -x_i + tau*y_i <= 0\n",
        "        A_ub[i, i] = -1.0\n",
        "        A_ub[i, n + i] = tau\n",
        "        # x_i - y_i <= 0\n",
        "        A_ub[n + i, i] = 1.0\n",
        "        A_ub[n + i, n + i] = -1.0\n",
        "\n",
        "    bounds = [(0, None)] * n + [(0, 1)] * n\n",
        "\n",
        "    return linprog(\n",
        "        c,\n",
        "        A_eq=A_eq,\n",
        "        b_eq=b_eq,\n",
        "        A_ub=A_ub,\n",
        "        b_ub=b_ub,\n",
        "        bounds=bounds,\n",
        "        method=\"highs\"\n",
        "    )\n",
        "\n",
        "def validate_witness(B_sub, x, resid_tol=1e-10, minx_tol=1e-12):\n",
        "    r = B_sub @ x\n",
        "    resid = float(np.max(np.abs(r))) if r.size else 0.0\n",
        "    minx = float(np.min(x)) if x.size else 0.0\n",
        "    ok = (resid <= resid_tol) and (minx >= -minx_tol)\n",
        "    return ok, resid, minx\n",
        "\n",
        "# ============================================================\n",
        "# 7) GICT (Merged Cone + Headman)\n",
        "# ============================================================\n",
        "def run_gict_merged(pnml_file, eps=1e-6, tau=1e-4, x_cover_thr=None,\n",
        "                    resid_tol=1e-10, minx_tol=1e-12):\n",
        "    if x_cover_thr is None:\n",
        "        x_cover_thr = tau / 2.0\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    places, transitions, arcs, ignored, Bin, Bout, B = parse_pnml_to_matrices(pnml_file)\n",
        "    t_parse = time.perf_counter() - t0\n",
        "\n",
        "    P, T = Bin.shape\n",
        "    approx_arcs = int((Bin > 0).sum() + (Bout > 0).sum())\n",
        "\n",
        "    # Baseline uncovered\n",
        "    t0 = time.perf_counter()\n",
        "    base_flags = [baseline_transition_cover(B, j, 1.0) for j in range(T)]\n",
        "    t_baseline = time.perf_counter() - t0\n",
        "    base_uncovered = [j for j, ok in enumerate(base_flags) if not ok]\n",
        "\n",
        "    # SCC\n",
        "    adj = build_bipartite_graph(Bin, Bout)\n",
        "    def t_node(j): return P + j\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    sccs = tarjan_scc(adj)\n",
        "    t_scc = time.perf_counter() - t0\n",
        "\n",
        "    scc_id = {}\n",
        "    for k, comp in enumerate(sccs):\n",
        "        for v in comp:\n",
        "            scc_id[v] = k\n",
        "    has_cycle = [scc_has_cycle(comp, adj) for comp in sccs]\n",
        "\n",
        "    labels = [\"Uncertain\"] * T\n",
        "\n",
        "    # Gate-0\n",
        "    t0 = time.perf_counter()\n",
        "    for j in range(T):\n",
        "        sid = scc_id[t_node(j)]\n",
        "        if not has_cycle[sid]:\n",
        "            labels[j] = \"StructurallyUncovered\"\n",
        "    t_gate0 = time.perf_counter() - t0\n",
        "\n",
        "    # Gate-2 per-SCC\n",
        "    t0 = time.perf_counter()\n",
        "    lp_scc = 0\n",
        "    lp_targeted = 0\n",
        "    coneempty_sccs = 0\n",
        "    nondense_sccs = 0\n",
        "    headman_attempted = 0\n",
        "    headman_valid = 0\n",
        "    headman_invalid = 0\n",
        "    headman_support_sizes = []\n",
        "    force_target_sccs = set()\n",
        "\n",
        "    for sid, comp in enumerate(sccs):\n",
        "        t_indices = [v - P for v in comp if v >= P]\n",
        "        if not t_indices:\n",
        "            continue\n",
        "\n",
        "        if not has_cycle[sid]:\n",
        "            for tj in t_indices:\n",
        "                labels[tj] = \"StructurallyUncovered\"\n",
        "            continue\n",
        "\n",
        "        B_sub = B[:, t_indices]\n",
        "\n",
        "        # Dense check\n",
        "        res_dense = dense_witness_simplex(B_sub, eps)\n",
        "        lp_scc += 1\n",
        "        if res_dense.success:\n",
        "            for tj in t_indices:\n",
        "                labels[tj] = \"CoveredWitness\"\n",
        "            continue\n",
        "\n",
        "        nondense_sccs += 1\n",
        "\n",
        "        # Headman: also serves as cone-nonempty test (simplex slice)\n",
        "        headman_attempted += 1\n",
        "        res_head = headman_v2_lp(B_sub, tau=tau)\n",
        "        lp_scc += 1\n",
        "\n",
        "        if not res_head.success:\n",
        "            coneempty_sccs += 1\n",
        "            for tj in t_indices:\n",
        "                labels[tj] = \"InvariantConeEmpty\"\n",
        "            continue\n",
        "\n",
        "        n = len(t_indices)\n",
        "        sol = np.array(res_head.x, dtype=float)\n",
        "        x_sub = sol[:n]\n",
        "\n",
        "        ok, resid, minx = validate_witness(B_sub, x_sub, resid_tol=resid_tol, minx_tol=minx_tol)\n",
        "        if not ok:\n",
        "            headman_invalid += 1\n",
        "            force_target_sccs.add(sid)\n",
        "            continue\n",
        "\n",
        "        headman_valid += 1\n",
        "        supp = (x_sub >= x_cover_thr)\n",
        "        headman_support_sizes.append(int(np.sum(supp)))\n",
        "\n",
        "        for k, tj in enumerate(t_indices):\n",
        "            if x_sub[k] >= x_cover_thr:\n",
        "                labels[tj] = \"CoveredWitness\"\n",
        "\n",
        "    t_gate2_scc = time.perf_counter() - t0\n",
        "\n",
        "    # Target remaining Uncertain\n",
        "    t0 = time.perf_counter()\n",
        "    targeted_list = []\n",
        "\n",
        "    for sid, comp in enumerate(sccs):\n",
        "        t_indices = [v - P for v in comp if v >= P]\n",
        "        if not t_indices:\n",
        "            continue\n",
        "        if sid in force_target_sccs:\n",
        "            for tj in t_indices:\n",
        "                if labels[tj] == \"Uncertain\":\n",
        "                    targeted_list.append(tj)\n",
        "                    res = targeted_cover_in_scc(B, tj, t_indices, min_xt=1.0)\n",
        "                    lp_targeted += 1\n",
        "                    labels[tj] = \"CoveredWitness\" if res.success else \"StructurallyUncovered\"\n",
        "\n",
        "    for j in range(T):\n",
        "        if labels[j] != \"Uncertain\":\n",
        "            continue\n",
        "        targeted_list.append(j)\n",
        "        sid = scc_id[t_node(j)]\n",
        "        comp = sccs[sid]\n",
        "        t_indices = [v - P for v in comp if v >= P]\n",
        "        res = targeted_cover_in_scc(B, j, t_indices, min_xt=1.0)\n",
        "        lp_targeted += 1\n",
        "        labels[j] = \"CoveredWitness\" if res.success else \"StructurallyUncovered\"\n",
        "\n",
        "    t_gate2_targeted = time.perf_counter() - t0\n",
        "\n",
        "    gict_uncovered = [j for j, lab in enumerate(labels) if lab in (\"StructurallyUncovered\", \"InvariantConeEmpty\")]\n",
        "\n",
        "    hist = defaultdict(int)\n",
        "    for lab in labels:\n",
        "        hist[lab] += 1\n",
        "\n",
        "    return {\n",
        "        \"file\": pnml_file,\n",
        "        \"P\": P,\n",
        "        \"T\": T,\n",
        "        \"arcs\": len(arcs),\n",
        "        \"ignored\": ignored,\n",
        "        \"approx_arcs\": approx_arcs,\n",
        "        \"t_parse\": t_parse,\n",
        "        \"baseline_uncovered\": base_uncovered,\n",
        "        \"t_baseline\": t_baseline,\n",
        "        \"gict_uncovered\": gict_uncovered,\n",
        "        \"hist\": dict(hist),\n",
        "        \"t_scc\": t_scc,\n",
        "        \"t_gate0\": t_gate0,\n",
        "        \"t_gate2_scc\": t_gate2_scc,\n",
        "        \"t_gate2_targeted\": t_gate2_targeted,\n",
        "        \"lp_scc\": lp_scc,\n",
        "        \"lp_targeted\": lp_targeted,\n",
        "        \"lp_total\": lp_scc + lp_targeted,\n",
        "        \"coneempty_sccs\": coneempty_sccs,\n",
        "        \"nondense_sccs\": nondense_sccs,\n",
        "        \"headman_attempted\": headman_attempted,\n",
        "        \"headman_valid\": headman_valid,\n",
        "        \"headman_invalid\": headman_invalid,\n",
        "        \"headman_support_sizes\": headman_support_sizes,\n",
        "        \"targeted_list\": targeted_list,\n",
        "    }\n",
        "\n",
        "def one_line_summary(r):\n",
        "    return (\n",
        "        f\"{r['file']} | P={r['P']} T={r['T']} arcs={r['arcs']} \"\n",
        "        f\"| base_unc={len(r['baseline_uncovered'])} gict_unc={len(r['gict_uncovered'])} \"\n",
        "        f\"| LP(scc={r['lp_scc']}, tgt={r['lp_targeted']}, total={r['lp_total']}) \"\n",
        "        f\"| coneEmptySCCs={r['coneempty_sccs']} nonDenseSCCs={r['nondense_sccs']} \"\n",
        "        f\"| headman(att={r['headman_attempted']}, ok={r['headman_valid']}, bad={r['headman_invalid']})\"\n",
        "    )\n",
        "\n",
        "# ============================================================\n",
        "# 8) Create required mutants (philo_guard if needed, plus piscine/token mutants)\n",
        "# ============================================================\n",
        "ensure_download(PHILO_URL, PHILO_FILE)\n",
        "ensure_download(PISCINE_URL, PISCINE_FILE)\n",
        "ensure_download(TOKEN_URL, TOKEN_FILE)\n",
        "\n",
        "# Make philo_guard if absent (same style you used before)\n",
        "if not os.path.exists(PHILO_GUARD):\n",
        "    mutate_add_guard_place(PHILO_FILE, PHILO_GUARD, guard_name=\"P_GUARD\", target_transition_idx=0)\n",
        "\n",
        "# Piscine mutants\n",
        "mut_piscine_leak = None\n",
        "mut_piscine_guard = None\n",
        "if not os.path.exists(PISCINE_LEAK):\n",
        "    mut_piscine_leak = mutate_add_leak_transition(PISCINE_FILE, PISCINE_LEAK, leak_name=\"LEAK\")\n",
        "if not os.path.exists(PISCINE_GUARD):\n",
        "    mut_piscine_guard = mutate_add_guard_place(PISCINE_FILE, PISCINE_GUARD, guard_name=\"P_GUARD\", target_transition_idx=0)\n",
        "\n",
        "# Token-ring mutants\n",
        "mut_token_leak = None\n",
        "mut_token_guard = None\n",
        "if not os.path.exists(TOKEN_LEAK):\n",
        "    mut_token_leak = mutate_add_leak_transition(TOKEN_FILE, TOKEN_LEAK, leak_name=\"LEAK\")\n",
        "if not os.path.exists(TOKEN_GUARD):\n",
        "    mut_token_guard = mutate_add_guard_place(TOKEN_FILE, TOKEN_GUARD, guard_name=\"P_GUARD\", target_transition_idx=0)\n",
        "\n",
        "# ============================================================\n",
        "# 9) Run benchmark suite\n",
        "# ============================================================\n",
        "suite = [\n",
        "    PHILO_FILE, PHILO_GUARD,\n",
        "    PISCINE_FILE, PISCINE_LEAK, PISCINE_GUARD,\n",
        "    TOKEN_FILE, TOKEN_LEAK, TOKEN_GUARD,\n",
        "]\n",
        "\n",
        "results = []\n",
        "for f in suite:\n",
        "    results.append(\n",
        "        run_gict_merged(\n",
        "            f,\n",
        "            eps=EPS,\n",
        "            tau=TAU,\n",
        "            x_cover_thr=X_COVER_THR,\n",
        "            resid_tol=RESID_TOL,\n",
        "            minx_tol=MINX_TOL,\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Minimal output (fastest)\n",
        "for r in results:\n",
        "    print(one_line_summary(r))\n",
        "\n",
        "# Print disagreements only (should be none)\n",
        "for r in results:\n",
        "    if set(r[\"baseline_uncovered\"]) != set(r[\"gict_uncovered\"]):\n",
        "        print(\"DISAGREE:\", r[\"file\"],\n",
        "              \"baseline=\", r[\"baseline_uncovered\"],\n",
        "              \"gict=\", r[\"gict_uncovered\"])\n",
        "\n",
        "# If you want to see which transition was added/guarded (only if newly created)\n",
        "if mut_piscine_leak:\n",
        "    print(\"Created\", PISCINE_LEAK, \"added transition\", mut_piscine_leak[\"added_transition\"], \"from place\", mut_piscine_leak[\"src_place\"])\n",
        "if mut_piscine_guard:\n",
        "    print(\"Created\", PISCINE_GUARD, \"added place\", mut_piscine_guard[\"added_place\"], \"guarding transition\", mut_piscine_guard[\"target_transition\"])\n",
        "if mut_token_leak:\n",
        "    print(\"Created\", TOKEN_LEAK, \"added transition\", mut_token_leak[\"added_transition\"], \"from place\", mut_token_leak[\"src_place\"])\n",
        "if mut_token_guard:\n",
        "    print(\"Created\", TOKEN_GUARD, \"added place\", mut_token_guard[\"added_place\"], \"guarding transition\", mut_token_guard[\"target_transition\"])\n",
        "\n",
        "# ============================================================\n",
        "# 10) Zip all PNML files (so you can save everything)\n",
        "# ============================================================\n",
        "if MAKE_ZIP:\n",
        "    pnml_files = sorted([fn for fn in os.listdir(\".\") if fn.lower().endswith(\".pnml\") and os.path.isfile(fn)])\n",
        "    if pnml_files:\n",
        "        with zipfile.ZipFile(ZIP_NAME, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "            for fn in pnml_files:\n",
        "                zf.write(fn)\n",
        "        print(\"Wrote zip:\", ZIP_NAME, \"files:\", len(pnml_files))\n",
        "    else:\n",
        "        print(\"No PNML files found to zip.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOLUgd1Y42gl",
        "outputId": "992fa6fd-3bfe-4d8e-eab6-578610a92d4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "philo.pnml | P=30 T=30 arcs=96 | base_unc=0 gict_unc=0 | LP(scc=1, tgt=0, total=1) | coneEmptySCCs=0 nonDenseSCCs=0 | headman(att=0, ok=0, bad=0)\n",
            "philo_guard.pnml | P=31 T=30 arcs=97 | base_unc=2 gict_unc=2 | LP(scc=2, tgt=2, total=4) | coneEmptySCCs=0 nonDenseSCCs=1 | headman(att=1, ok=1, bad=0)\n",
            "Piscine.pnml | P=9 T=7 arcs=20 | base_unc=0 gict_unc=0 | LP(scc=1, tgt=0, total=1) | coneEmptySCCs=0 nonDenseSCCs=0 | headman(att=0, ok=0, bad=0)\n",
            "Piscine_leak.pnml | P=9 T=8 arcs=21 | base_unc=1 gict_unc=1 | LP(scc=1, tgt=0, total=1) | coneEmptySCCs=0 nonDenseSCCs=0 | headman(att=0, ok=0, bad=0)\n",
            "Piscine_guard.pnml | P=10 T=7 arcs=21 | base_unc=7 gict_unc=7 | LP(scc=2, tgt=0, total=2) | coneEmptySCCs=1 nonDenseSCCs=1 | headman(att=1, ok=0, bad=0)\n",
            "Token-ring.pnml | P=18 T=15 arcs=67 | base_unc=15 gict_unc=15 | LP(scc=2, tgt=0, total=2) | coneEmptySCCs=1 nonDenseSCCs=1 | headman(att=1, ok=0, bad=0)\n",
            "Token-ring_leak.pnml | P=18 T=16 arcs=68 | base_unc=16 gict_unc=16 | LP(scc=2, tgt=0, total=2) | coneEmptySCCs=1 nonDenseSCCs=1 | headman(att=1, ok=0, bad=0)\n",
            "Token-ring_guard.pnml | P=19 T=15 arcs=68 | base_unc=15 gict_unc=15 | LP(scc=2, tgt=0, total=2) | coneEmptySCCs=1 nonDenseSCCs=1 | headman(att=1, ok=0, bad=0)\n",
            "Created Piscine_leak.pnml added transition t_leak from place cId-773840572439763225716\n",
            "Created Piscine_guard.pnml added place p_guard guarding transition cId-777641285985620368215\n",
            "Created Token-ring_leak.pnml added transition t_leak from place cId302956042366382860362\n",
            "Created Token-ring_guard.pnml added place p_guard guarding transition cId301964552006875653980\n",
            "Wrote zip: pnml_suite.zip files: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from datetime import datetime\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "import numpy as np\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "# ============================================================\n",
        "# SETTINGS\n",
        "# ============================================================\n",
        "QUIET = False                    # set True for minimal prints\n",
        "ZIP_NAME = \"pnml_suite.zip\"      # will (re)write at end, including new mutant\n",
        "\n",
        "TOKEN_URL  = \"https://www.pnml.org/version-2009/examples/Token-ring.pnml\"\n",
        "TOKEN_FILE = \"Token-ring.pnml\"\n",
        "RESCUE_OUT = \"Token-ring_rescue.pnml\"\n",
        "\n",
        "# Search limits\n",
        "MAX_TRIES = 2000                 # safety cap; 18*15*2 = 540 so this is ample\n",
        "ARC_WEIGHT = 1                   # add weight-1 arc\n",
        "ONLY_WITHIN_EXISTING_IDS = True  # do not create new places/transitions; only add arcs between existing P,T\n",
        "\n",
        "# ============================================================\n",
        "# PNML Utilities\n",
        "# ============================================================\n",
        "def strip_ns(tag):\n",
        "    return tag.split(\"}\", 1)[-1] if \"}\" in tag else tag\n",
        "\n",
        "def ensure_download(url, path):\n",
        "    if not os.path.exists(path):\n",
        "        urllib.request.urlretrieve(url, path)\n",
        "\n",
        "def get_pnml_net_root(tree):\n",
        "    root = tree.getroot()\n",
        "    net = None\n",
        "    for e in root.iter():\n",
        "        if strip_ns(e.tag) == \"net\":\n",
        "            net = e\n",
        "            break\n",
        "    if net is None:\n",
        "        raise RuntimeError(\"No <net> element found in PNML.\")\n",
        "    return root, net\n",
        "\n",
        "def existing_ids(net):\n",
        "    ids = set()\n",
        "    for e in net.iter():\n",
        "        i = e.attrib.get(\"id\")\n",
        "        if i:\n",
        "            ids.add(i)\n",
        "    return ids\n",
        "\n",
        "def unique_id(base, used):\n",
        "    if base not in used:\n",
        "        used.add(base)\n",
        "        return base\n",
        "    k = 1\n",
        "    while f\"{base}_{k}\" in used:\n",
        "        k += 1\n",
        "    uid = f\"{base}_{k}\"\n",
        "    used.add(uid)\n",
        "    return uid\n",
        "\n",
        "def get_place_and_transition_ids(net):\n",
        "    places = []\n",
        "    transitions = []\n",
        "    for e in net.iter():\n",
        "        t = strip_ns(e.tag)\n",
        "        if t == \"place\":\n",
        "            pid = e.attrib.get(\"id\")\n",
        "            if pid:\n",
        "                places.append(pid)\n",
        "        elif t == \"transition\":\n",
        "            tid = e.attrib.get(\"id\")\n",
        "            if tid:\n",
        "                transitions.append(tid)\n",
        "    # keep file order but unique\n",
        "    def uniq(seq):\n",
        "        seen = set()\n",
        "        out = []\n",
        "        for x in seq:\n",
        "            if x not in seen:\n",
        "                out.append(x)\n",
        "                seen.add(x)\n",
        "        return out\n",
        "    return uniq(places), uniq(transitions)\n",
        "\n",
        "def arc_key(src, tgt):\n",
        "    return (src, tgt)\n",
        "\n",
        "def collect_existing_arcs(net):\n",
        "    arcs = set()\n",
        "    for e in net.iter():\n",
        "        if strip_ns(e.tag) == \"arc\":\n",
        "            s = e.attrib.get(\"source\")\n",
        "            t = e.attrib.get(\"target\")\n",
        "            if s and t:\n",
        "                arcs.add((s, t))\n",
        "    return arcs\n",
        "\n",
        "def add_arc_with_weight(net, used_ids, arc_id_base, source_id, target_id, weight=1):\n",
        "    arc_id = unique_id(arc_id_base, used_ids)\n",
        "    arc = ET.Element(\"arc\", {\"id\": arc_id, \"source\": source_id, \"target\": target_id})\n",
        "    if weight != 1:\n",
        "        ins = ET.SubElement(arc, \"inscription\")\n",
        "        txt = ET.SubElement(ins, \"text\")\n",
        "        txt.text = str(int(weight))\n",
        "    net.append(arc)\n",
        "    return arc_id\n",
        "\n",
        "# ============================================================\n",
        "# PNML -> matrices (Bin, Bout, B)\n",
        "# ============================================================\n",
        "def parse_pnml_to_matrices(pnml_file):\n",
        "    tree = ET.parse(pnml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    places = []\n",
        "    transitions = []\n",
        "    arcs = []\n",
        "\n",
        "    for elem in root.iter():\n",
        "        tag = strip_ns(elem.tag)\n",
        "        if tag == \"place\":\n",
        "            pid = elem.attrib.get(\"id\")\n",
        "            if pid:\n",
        "                places.append(pid)\n",
        "        elif tag == \"transition\":\n",
        "            tid = elem.attrib.get(\"id\")\n",
        "            if tid:\n",
        "                transitions.append(tid)\n",
        "        elif tag == \"arc\":\n",
        "            src = elem.attrib.get(\"source\")\n",
        "            tgt = elem.attrib.get(\"target\")\n",
        "            aid = elem.attrib.get(\"id\", \"\")\n",
        "            if src and tgt:\n",
        "                w = 1\n",
        "                for child in elem.iter():\n",
        "                    if strip_ns(child.tag) == \"inscription\":\n",
        "                        for g in child.iter():\n",
        "                            if strip_ns(g.tag) == \"text\" and g.text is not None:\n",
        "                                txt = g.text.strip()\n",
        "                                if txt:\n",
        "                                    try:\n",
        "                                        w = int(txt)\n",
        "                                    except:\n",
        "                                        pass\n",
        "                arcs.append((src, tgt, w, aid))\n",
        "\n",
        "    def uniq(seq):\n",
        "        seen = set()\n",
        "        out = []\n",
        "        for x in seq:\n",
        "            if x not in seen:\n",
        "                out.append(x)\n",
        "                seen.add(x)\n",
        "        return out\n",
        "\n",
        "    places = uniq([p for p in places if p is not None])\n",
        "    transitions = uniq([t for t in transitions if t is not None])\n",
        "\n",
        "    p_index = {p:i for i,p in enumerate(places)}\n",
        "    t_index = {t:i for i,t in enumerate(transitions)}\n",
        "\n",
        "    P = len(places)\n",
        "    T = len(transitions)\n",
        "\n",
        "    Bin = np.zeros((P, T), dtype=float)\n",
        "    Bout = np.zeros((P, T), dtype=float)\n",
        "\n",
        "    ignored = 0\n",
        "    for src, tgt, w, _aid in arcs:\n",
        "        if src in p_index and tgt in t_index:\n",
        "            Bin[p_index[src], t_index[tgt]] += float(w)\n",
        "        elif src in t_index and tgt in p_index:\n",
        "            Bout[p_index[tgt], t_index[src]] += float(w)\n",
        "        else:\n",
        "            ignored += 1\n",
        "\n",
        "    B = Bout - Bin\n",
        "    return places, transitions, arcs, ignored, Bin, Bout, B\n",
        "\n",
        "# ============================================================\n",
        "# Feasibility: does there exist x>=0, x!=0 such that Bx=0?\n",
        "# We test with simplex slice: Bx=0, x>=0, sum(x)=1\n",
        "# ============================================================\n",
        "def invariant_cone_nonempty(B):\n",
        "    m, n = B.shape\n",
        "    A_eq = np.vstack([B, np.ones((1, n))])\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[-1] = 1.0\n",
        "    res = linprog(\n",
        "        np.zeros(n),\n",
        "        A_eq=A_eq,\n",
        "        b_eq=b_eq,\n",
        "        bounds=[(0, None)] * n,\n",
        "        method=\"highs\"\n",
        "    )\n",
        "    return res.success, res\n",
        "\n",
        "# ============================================================\n",
        "# Rescue search: add ONE arc (either P->T or T->P) and test cone nonempty\n",
        "# ============================================================\n",
        "def find_token_ring_rescue(src_pnml, out_pnml, weight=1, max_tries=2000):\n",
        "    # parse original\n",
        "    places, transitions, arcs, ignored, Bin, Bout, B = parse_pnml_to_matrices(src_pnml)\n",
        "    P = len(places)\n",
        "    T = len(transitions)\n",
        "\n",
        "    ok0, _ = invariant_cone_nonempty(B)\n",
        "    if ok0:\n",
        "        # Already nonempty; just copy through\n",
        "        ET.parse(src_pnml).write(out_pnml, encoding=\"utf-8\", xml_declaration=True)\n",
        "        return {\n",
        "            \"already_nonempty\": True,\n",
        "            \"added_arc\": None,\n",
        "            \"direction\": None,\n",
        "            \"source\": None,\n",
        "            \"target\": None,\n",
        "            \"P\": P,\n",
        "            \"T\": T\n",
        "        }\n",
        "\n",
        "    # Load PNML XML for mutation attempts\n",
        "    tree0 = ET.parse(src_pnml)\n",
        "    root0, net0 = get_pnml_net_root(tree0)\n",
        "    used0 = existing_ids(net0)\n",
        "    existing_arcs0 = collect_existing_arcs(net0)\n",
        "\n",
        "    # Candidate arcs (prioritize those that do NOT already exist)\n",
        "    candidates = []\n",
        "    for p in places:\n",
        "        for t in transitions:\n",
        "            # p -> t\n",
        "            if (p, t) not in existing_arcs0:\n",
        "                candidates.append((\"P2T\", p, t))\n",
        "            # t -> p\n",
        "            if (t, p) not in existing_arcs0:\n",
        "                candidates.append((\"T2P\", t, p))\n",
        "\n",
        "    # Small heuristic: try to connect across components by preferring arcs involving the first few IDs\n",
        "    # (keeps deterministic, no randomness)\n",
        "    # We just keep candidates as is; Token-ring is small anyway.\n",
        "\n",
        "    tries = 0\n",
        "    best_debug = None\n",
        "\n",
        "    for direction, src, tgt in candidates:\n",
        "        tries += 1\n",
        "        if tries > max_tries:\n",
        "            break\n",
        "\n",
        "        # clone original tree each time\n",
        "        tree = ET.ElementTree(ET.fromstring(ET.tostring(root0)))\n",
        "        root, net = get_pnml_net_root(tree)\n",
        "        used = existing_ids(net)\n",
        "        existing_arcs = collect_existing_arcs(net)\n",
        "\n",
        "        # add arc if still absent (should be)\n",
        "        if (src, tgt) in existing_arcs:\n",
        "            continue\n",
        "\n",
        "        arc_id = add_arc_with_weight(\n",
        "            net, used, arc_id_base=\"a_rescue\", source_id=src, target_id=tgt, weight=weight\n",
        "        )\n",
        "\n",
        "        # write temp file, parse matrices, test feasibility\n",
        "        tmp = \"__tmp_rescue.pnml\"\n",
        "        tree.write(tmp, encoding=\"utf-8\", xml_declaration=True)\n",
        "\n",
        "        _, _, _, _, _, _, Bm = parse_pnml_to_matrices(tmp)\n",
        "        ok, res = invariant_cone_nonempty(Bm)\n",
        "\n",
        "        if not QUIET:\n",
        "            print(f\"try {tries:03d}: {direction} {src} -> {tgt}  ok={ok}\")\n",
        "\n",
        "        if ok:\n",
        "            # success: write final\n",
        "            tree.write(out_pnml, encoding=\"utf-8\", xml_declaration=True)\n",
        "            # cleanup\n",
        "            try:\n",
        "                os.remove(tmp)\n",
        "            except:\n",
        "                pass\n",
        "            return {\n",
        "                \"already_nonempty\": False,\n",
        "                \"added_arc\": arc_id,\n",
        "                \"direction\": direction,\n",
        "                \"source\": src,\n",
        "                \"target\": tgt,\n",
        "                \"tries\": tries,\n",
        "                \"P\": P,\n",
        "                \"T\": T\n",
        "            }\n",
        "\n",
        "        # keep some debug info in case\n",
        "        best_debug = (direction, src, tgt, arc_id)\n",
        "\n",
        "    # cleanup\n",
        "    try:\n",
        "        if os.path.exists(\"__tmp_rescue.pnml\"):\n",
        "            os.remove(\"__tmp_rescue.pnml\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return {\n",
        "        \"already_nonempty\": False,\n",
        "        \"added_arc\": None,\n",
        "        \"direction\": None,\n",
        "        \"source\": None,\n",
        "        \"target\": None,\n",
        "        \"tries\": tries,\n",
        "        \"P\": P,\n",
        "        \"T\": T,\n",
        "        \"last_tried\": best_debug\n",
        "    }\n",
        "\n",
        "# ============================================================\n",
        "# Minimal report + zip all PNML\n",
        "# ============================================================\n",
        "def zip_all_pnml(zip_name):\n",
        "    pnml_files = sorted([fn for fn in os.listdir(\".\") if fn.lower().endswith(\".pnml\") and os.path.isfile(fn)])\n",
        "    if not pnml_files:\n",
        "        print(\"No PNML files found to zip.\")\n",
        "        return None\n",
        "    with zipfile.ZipFile(zip_name, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "        for fn in pnml_files:\n",
        "            zf.write(fn)\n",
        "    print(\"Wrote zip:\", zip_name, \"files:\", len(pnml_files))\n",
        "    return zip_name\n",
        "\n",
        "# ============================================================\n",
        "# RUN: Token-ring rescue mutant\n",
        "# ============================================================\n",
        "ensure_download(TOKEN_URL, TOKEN_FILE)\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "info = find_token_ring_rescue(TOKEN_FILE, RESCUE_OUT, weight=ARC_WEIGHT, max_tries=MAX_TRIES)\n",
        "dt = time.perf_counter() - t0\n",
        "\n",
        "# verify result\n",
        "places, transitions, arcs, ignored, Bin, Bout, B = parse_pnml_to_matrices(RESCUE_OUT)\n",
        "ok, res = invariant_cone_nonempty(B)\n",
        "\n",
        "print(\"=== Token-ring rescue mutant ===\")\n",
        "print(\"source:\", TOKEN_FILE)\n",
        "print(\"out   :\", RESCUE_OUT)\n",
        "print(\"time  :\", f\"{dt:.3f}s\")\n",
        "print(\"P,T   :\", len(places), len(transitions), \"arcs:\", len(arcs), \"ignored:\", ignored)\n",
        "print(\"cone nonempty?\", ok)\n",
        "\n",
        "if info.get(\"already_nonempty\"):\n",
        "    print(\"NOTE: original was already cone-nonempty (unexpected). File copied.\")\n",
        "elif info.get(\"added_arc\"):\n",
        "    print(\"Added arc id:\", info[\"added_arc\"])\n",
        "    print(\"Direction  :\", info[\"direction\"])\n",
        "    print(\"Source     :\", info[\"source\"])\n",
        "    print(\"Target     :\", info[\"target\"])\n",
        "    print(\"Tries      :\", info[\"tries\"])\n",
        "else:\n",
        "    print(\"FAILED to find single-arc rescue within tries:\", info.get(\"tries\"))\n",
        "    print(\"Last tried:\", info.get(\"last_tried\"))\n",
        "\n",
        "# show a compact witness if feasible\n",
        "if ok:\n",
        "    x = res.x\n",
        "    supp = np.where(x > 1e-9)[0].tolist()\n",
        "    print(\"Witness support size:\", len(supp), \"of\", len(transitions))\n",
        "    # print up to 20\n",
        "    for k in supp[:20]:\n",
        "        print(\"  t[%-2d] %-24s x=%g\" % (k, transitions[k], x[k]))\n",
        "    if len(supp) > 20:\n",
        "        print(\"  ...\")\n",
        "\n",
        "# zip everything (so you can download once)\n",
        "zip_all_pnml(ZIP_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "690-aroe6RIQ",
        "outputId": "30b5ddc7-dfac-40fb-ca3c-d6c0854fa96f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "try 001: P2T cId302956042366382860362 -> cId301964552006875653980  ok=False\n",
            "try 002: T2P cId301964552006875653980 -> cId302956042366382860362  ok=False\n",
            "try 003: P2T cId302956042366382860362 -> cId300807812896203305081  ok=False\n",
            "try 004: T2P cId300807812896203305081 -> cId302956042366382860362  ok=False\n",
            "try 005: P2T cId302956042366382860362 -> cId300807812896203305012  ok=False\n",
            "try 006: T2P cId300807812896203305012 -> cId302956042366382860362  ok=False\n",
            "try 007: P2T cId302956042366382860362 -> cId300807812896203305013  ok=False\n",
            "try 008: T2P cId300807812896203305013 -> cId302956042366382860362  ok=False\n",
            "try 009: P2T cId302956042366382860362 -> cId301303558290705273077  ok=False\n",
            "try 010: T2P cId301303558290705273077 -> cId302956042366382860362  ok=False\n",
            "try 011: P2T cId302956042366382860362 -> cId301303558290705273023  ok=False\n",
            "try 012: T2P cId301303558290705273023 -> cId302956042366382860362  ok=False\n",
            "try 013: P2T cId302956042366382860362 -> cId301303558290705273071  ok=False\n",
            "try 014: T2P cId301303558290705273071 -> cId302956042366382860362  ok=False\n",
            "try 015: P2T cId302956042366382860362 -> cId301303558290705273022  ok=False\n",
            "try 016: T2P cId301303558290705273022 -> cId302956042366382860362  ok=False\n",
            "try 017: P2T cId302956042366382860362 -> cId301303558290705273021  ok=False\n",
            "try 018: T2P cId301303558290705273021 -> cId302956042366382860362  ok=False\n",
            "try 019: P2T cId302956042366382860362 -> cId301303558290705273024  ok=False\n",
            "try 020: P2T cId302956042366382860362 -> cId301303558290705273055  ok=False\n",
            "try 021: T2P cId301303558290705273055 -> cId302956042366382860362  ok=False\n",
            "try 022: P2T cId302956042366382860362 -> cId301303558290705273011  ok=False\n",
            "try 023: T2P cId301303558290705273011 -> cId302956042366382860362  ok=False\n",
            "try 024: T2P cId301303558290705273067 -> cId302956042366382860362  ok=False\n",
            "try 025: P2T cId302956042366382860362 -> cId301303558290705273076  ok=False\n",
            "try 026: T2P cId301303558290705273076 -> cId302956042366382860362  ok=False\n",
            "try 027: P2T cId302956042366382860362 -> cId301138309969036860220  ok=False\n",
            "try 028: T2P cId301138309969036860220 -> cId302956042366382860362  ok=False\n",
            "try 029: T2P cId300807812896203305081 -> cId302625545293549305179  ok=False\n",
            "try 030: P2T cId302625545293549305179 -> cId300807812896203305012  ok=False\n",
            "try 031: T2P cId300807812896203305012 -> cId302625545293549305179  ok=False\n",
            "try 032: P2T cId302625545293549305179 -> cId300807812896203305013  ok=False\n",
            "try 033: T2P cId300807812896203305013 -> cId302625545293549305179  ok=False\n",
            "try 034: P2T cId302625545293549305179 -> cId301303558290705273077  ok=False\n",
            "try 035: T2P cId301303558290705273077 -> cId302625545293549305179  ok=False\n",
            "try 036: P2T cId302625545293549305179 -> cId301303558290705273023  ok=False\n",
            "try 037: T2P cId301303558290705273023 -> cId302625545293549305179  ok=False\n",
            "try 038: P2T cId302625545293549305179 -> cId301303558290705273071  ok=False\n",
            "try 039: T2P cId301303558290705273071 -> cId302625545293549305179  ok=False\n",
            "try 040: P2T cId302625545293549305179 -> cId301303558290705273022  ok=False\n",
            "try 041: T2P cId301303558290705273022 -> cId302625545293549305179  ok=False\n",
            "try 042: P2T cId302625545293549305179 -> cId301303558290705273021  ok=False\n",
            "try 043: T2P cId301303558290705273021 -> cId302625545293549305179  ok=False\n",
            "try 044: P2T cId302625545293549305179 -> cId301303558290705273024  ok=False\n",
            "try 045: T2P cId301303558290705273024 -> cId302625545293549305179  ok=False\n",
            "try 046: P2T cId302625545293549305179 -> cId301303558290705273055  ok=False\n",
            "try 047: T2P cId301303558290705273055 -> cId302625545293549305179  ok=False\n",
            "try 048: P2T cId302625545293549305179 -> cId301303558290705273011  ok=False\n",
            "try 049: T2P cId301303558290705273011 -> cId302625545293549305179  ok=False\n",
            "try 050: P2T cId302625545293549305179 -> cId301303558290705273067  ok=False\n",
            "try 051: T2P cId301303558290705273067 -> cId302625545293549305179  ok=False\n",
            "try 052: P2T cId302625545293549305179 -> cId301303558290705273076  ok=False\n",
            "try 053: P2T cId302625545293549305179 -> cId301138309969036860220  ok=False\n",
            "try 054: T2P cId301138309969036860220 -> cId302625545293549305179  ok=False\n",
            "try 055: P2T cId302625545293549305160 -> cId301964552006875653980  ok=False\n",
            "try 056: T2P cId301964552006875653980 -> cId302625545293549305160  ok=False\n",
            "try 057: P2T cId302625545293549305160 -> cId300807812896203305081  ok=False\n",
            "try 058: T2P cId300807812896203305081 -> cId302625545293549305160  ok=False\n",
            "try 059: P2T cId302625545293549305160 -> cId300807812896203305012  ok=False\n",
            "try 060: T2P cId300807812896203305012 -> cId302625545293549305160  ok=False\n",
            "try 061: P2T cId302625545293549305160 -> cId300807812896203305013  ok=False\n",
            "try 062: T2P cId300807812896203305013 -> cId302625545293549305160  ok=False\n",
            "try 063: P2T cId302625545293549305160 -> cId301303558290705273077  ok=False\n",
            "try 064: T2P cId301303558290705273077 -> cId302625545293549305160  ok=False\n",
            "try 065: P2T cId302625545293549305160 -> cId301303558290705273023  ok=False\n",
            "try 066: T2P cId301303558290705273023 -> cId302625545293549305160  ok=False\n",
            "try 067: P2T cId302625545293549305160 -> cId301303558290705273071  ok=False\n",
            "try 068: T2P cId301303558290705273071 -> cId302625545293549305160  ok=False\n",
            "try 069: P2T cId302625545293549305160 -> cId301303558290705273022  ok=False\n",
            "try 070: T2P cId301303558290705273022 -> cId302625545293549305160  ok=False\n",
            "try 071: P2T cId302625545293549305160 -> cId301303558290705273021  ok=False\n",
            "try 072: T2P cId301303558290705273021 -> cId302625545293549305160  ok=False\n",
            "try 073: P2T cId302625545293549305160 -> cId301303558290705273024  ok=False\n",
            "try 074: T2P cId301303558290705273024 -> cId302625545293549305160  ok=False\n",
            "try 075: P2T cId302625545293549305160 -> cId301303558290705273055  ok=False\n",
            "try 076: P2T cId302625545293549305160 -> cId301303558290705273011  ok=False\n",
            "try 077: T2P cId301303558290705273011 -> cId302625545293549305160  ok=False\n",
            "try 078: T2P cId301303558290705273067 -> cId302625545293549305160  ok=False\n",
            "try 079: P2T cId302625545293549305160 -> cId301303558290705273076  ok=False\n",
            "try 080: T2P cId301303558290705273076 -> cId302625545293549305160  ok=False\n",
            "try 081: P2T cId302625545293549305160 -> cId301138309969036860220  ok=False\n",
            "try 082: T2P cId301138309969036860220 -> cId302625545293549305160  ok=False\n",
            "try 083: P2T cId302625545293549305161 -> cId301964552006875653980  ok=False\n",
            "try 084: T2P cId301964552006875653980 -> cId302625545293549305161  ok=False\n",
            "try 085: P2T cId302625545293549305161 -> cId300807812896203305081  ok=False\n",
            "try 086: T2P cId300807812896203305081 -> cId302625545293549305161  ok=False\n",
            "try 087: P2T cId302625545293549305161 -> cId300807812896203305012  ok=False\n",
            "try 088: T2P cId300807812896203305012 -> cId302625545293549305161  ok=False\n",
            "try 089: P2T cId302625545293549305161 -> cId300807812896203305013  ok=False\n",
            "try 090: T2P cId300807812896203305013 -> cId302625545293549305161  ok=False\n",
            "try 091: P2T cId302625545293549305161 -> cId301303558290705273077  ok=False\n",
            "try 092: T2P cId301303558290705273077 -> cId302625545293549305161  ok=False\n",
            "try 093: P2T cId302625545293549305161 -> cId301303558290705273023  ok=False\n",
            "try 094: T2P cId301303558290705273023 -> cId302625545293549305161  ok=False\n",
            "try 095: P2T cId302625545293549305161 -> cId301303558290705273071  ok=False\n",
            "try 096: T2P cId301303558290705273071 -> cId302625545293549305161  ok=False\n",
            "try 097: P2T cId302625545293549305161 -> cId301303558290705273022  ok=False\n",
            "try 098: T2P cId301303558290705273022 -> cId302625545293549305161  ok=False\n",
            "try 099: P2T cId302625545293549305161 -> cId301303558290705273021  ok=False\n",
            "try 100: T2P cId301303558290705273021 -> cId302625545293549305161  ok=False\n",
            "try 101: T2P cId301303558290705273024 -> cId302625545293549305161  ok=False\n",
            "try 102: P2T cId302625545293549305161 -> cId301303558290705273055  ok=False\n",
            "try 103: P2T cId302625545293549305161 -> cId301303558290705273011  ok=False\n",
            "try 104: T2P cId301303558290705273011 -> cId302625545293549305161  ok=False\n",
            "try 105: P2T cId302625545293549305161 -> cId301303558290705273067  ok=False\n",
            "try 106: T2P cId301303558290705273067 -> cId302625545293549305161  ok=False\n",
            "try 107: P2T cId302625545293549305161 -> cId301303558290705273076  ok=False\n",
            "try 108: T2P cId301303558290705273076 -> cId302625545293549305161  ok=False\n",
            "try 109: P2T cId302625545293549305161 -> cId301138309969036860220  ok=False\n",
            "try 110: T2P cId301138309969036860220 -> cId302625545293549305161  ok=False\n",
            "try 111: P2T cId302460296971880892375 -> cId301964552006875653980  ok=False\n",
            "try 112: T2P cId301964552006875653980 -> cId302460296971880892375  ok=False\n",
            "try 113: P2T cId302460296971880892375 -> cId300807812896203305081  ok=False\n",
            "try 114: T2P cId300807812896203305081 -> cId302460296971880892375  ok=False\n",
            "try 115: P2T cId302460296971880892375 -> cId300807812896203305012  ok=False\n",
            "try 116: T2P cId300807812896203305012 -> cId302460296971880892375  ok=False\n",
            "try 117: P2T cId302460296971880892375 -> cId300807812896203305013  ok=False\n",
            "try 118: T2P cId300807812896203305013 -> cId302460296971880892375  ok=False\n",
            "try 119: T2P cId301303558290705273077 -> cId302460296971880892375  ok=False\n",
            "try 120: P2T cId302460296971880892375 -> cId301303558290705273023  ok=False\n",
            "try 121: T2P cId301303558290705273023 -> cId302460296971880892375  ok=False\n",
            "try 122: P2T cId302460296971880892375 -> cId301303558290705273071  ok=False\n",
            "try 123: P2T cId302460296971880892375 -> cId301303558290705273022  ok=False\n",
            "try 124: T2P cId301303558290705273022 -> cId302460296971880892375  ok=False\n",
            "try 125: P2T cId302460296971880892375 -> cId301303558290705273021  ok=False\n",
            "try 126: T2P cId301303558290705273021 -> cId302460296971880892375  ok=False\n",
            "try 127: P2T cId302460296971880892375 -> cId301303558290705273024  ok=False\n",
            "try 128: T2P cId301303558290705273024 -> cId302460296971880892375  ok=False\n",
            "try 129: P2T cId302460296971880892375 -> cId301303558290705273055  ok=False\n",
            "try 130: T2P cId301303558290705273055 -> cId302460296971880892375  ok=False\n",
            "try 131: P2T cId302460296971880892375 -> cId301303558290705273011  ok=False\n",
            "try 132: T2P cId301303558290705273011 -> cId302460296971880892375  ok=False\n",
            "try 133: P2T cId302460296971880892375 -> cId301303558290705273067  ok=False\n",
            "try 134: T2P cId301303558290705273067 -> cId302460296971880892375  ok=False\n",
            "try 135: T2P cId301303558290705273076 -> cId302460296971880892375  ok=False\n",
            "try 136: P2T cId302460296971880892375 -> cId301138309969036860220  ok=False\n",
            "try 137: T2P cId301138309969036860220 -> cId302460296971880892375  ok=False\n",
            "try 138: P2T cId30246029697188089233 -> cId301964552006875653980  ok=True\n",
            "=== Token-ring rescue mutant ===\n",
            "source: Token-ring.pnml\n",
            "out   : Token-ring_rescue.pnml\n",
            "time  : 3.928s\n",
            "P,T   : 18 15 arcs: 68 ignored: 0\n",
            "cone nonempty? True\n",
            "Added arc id: a_rescue\n",
            "Direction  : P2T\n",
            "Source     : cId30246029697188089233\n",
            "Target     : cId301964552006875653980\n",
            "Tries      : 138\n",
            "Witness support size: 1 of 15\n",
            "  t[0 ] cId301964552006875653980 x=1\n",
            "Wrote zip: pnml_suite.zip files: 11\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'pnml_suite.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import zipfile\n",
        "from datetime import datetime\n",
        "\n",
        "def zip_pnml_files(zip_name=None, include_all_pnml=True, extra_files=None):\n",
        "    \"\"\"\n",
        "    Zips PNML files in the current directory, plus any extra files you specify.\n",
        "\n",
        "    include_all_pnml=True -> include every *.pnml in cwd\n",
        "    extra_files -> list of specific filenames to include (if present)\n",
        "    \"\"\"\n",
        "    if zip_name is None:\n",
        "        ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        zip_name = f\"pnml_suite_{ts}.zip\"\n",
        "\n",
        "    files = set()\n",
        "\n",
        "    if include_all_pnml:\n",
        "        for fn in os.listdir(\".\"):\n",
        "            if fn.lower().endswith(\".pnml\") and os.path.isfile(fn):\n",
        "                files.add(fn)\n",
        "\n",
        "    if extra_files:\n",
        "        for fn in extra_files:\n",
        "            if fn and os.path.exists(fn) and os.path.isfile(fn):\n",
        "                files.add(fn)\n",
        "\n",
        "    files = sorted(files)\n",
        "\n",
        "    if not files:\n",
        "        print(\"No PNML files found to zip.\")\n",
        "        return None\n",
        "\n",
        "    with zipfile.ZipFile(zip_name, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "        for fn in files:\n",
        "            zf.write(fn)\n",
        "\n",
        "    print(\"Wrote zip:\", zip_name)\n",
        "    print(\"Included files:\")\n",
        "    for fn in files:\n",
        "        print(\"  -\", fn)\n",
        "\n",
        "    return zip_name\n",
        "\n",
        "# Example usage:\n",
        "# 1) Zip everything *.pnml in the folder:\n",
        "zip_pnml_files()\n",
        "\n",
        "# 2) Or zip only a specific set (uncomment):\n",
        "# zip_pnml_files(\n",
        "#     zip_name=\"pnml_suite.zip\",\n",
        "#     include_all_pnml=False,\n",
        "#     extra_files=[\"philo.pnml\", \"philo_guard.pnml\", \"philo_leak.pnml\", \"Piscine.pnml\", \"Token-ring.pnml\"]\n",
        "# )"
      ],
      "metadata": {
        "id": "K0FLh7W-3nLv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "19bed924-7357-4b23-ebf5-29b55817e246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote zip: pnml_suite_20260121_141017.zip\n",
            "Included files:\n",
            "  - Piscine.pnml\n",
            "  - Piscine_guard.pnml\n",
            "  - Piscine_leak.pnml\n",
            "  - Token-ring.pnml\n",
            "  - Token-ring_guard.pnml\n",
            "  - Token-ring_leak.pnml\n",
            "  - philo.pnml\n",
            "  - philo_guard.pnml\n",
            "  - philo_leak.pnml\n",
            "  - voorbeeld.pnml\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'pnml_suite_20260121_141017.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import time\n",
        "import itertools\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "import numpy as np\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "# ============================================================\n",
        "# Token-ring \"rescue\" suite for min-support k = 1..5\n",
        "#\n",
        "# Goal:\n",
        "#   For each k in {1,2,3,4,5}, find a small mutation (1-arc, else 2-arc)\n",
        "#   such that there exists a nonnegative T-invariant x (Bx=0, x>=0, sum(x)=1)\n",
        "#   whose \"spread\" support (x_i >= eps_support) has size >= k.\n",
        "#\n",
        "# We compute the *most spread* invariant via min-max LP:\n",
        "#   minimize z\n",
        "#   s.t. Bx=0, sum(x)=1, x>=0, x_i <= z\n",
        "# This discourages single-transition solutions and tends to enlarge support.\n",
        "# Then accept if count(x_i >= eps_support) >= k.\n",
        "#\n",
        "# Outputs:\n",
        "#   Token-ring_rescue_k1.pnml ... Token-ring_rescue_k5.pnml\n",
        "# And repacks pnml_suite.zip containing all *.pnml in the directory.\n",
        "# ============================================================\n",
        "\n",
        "# ----------------------------\n",
        "# Settings (tune here)\n",
        "# ----------------------------\n",
        "TOKEN_URL  = \"https://www.pnml.org/version-2009/examples/Token-ring.pnml\"\n",
        "TOKEN_FILE = \"Token-ring.pnml\"\n",
        "ZIP_NAME   = \"pnml_suite.zip\"\n",
        "\n",
        "ARC_WEIGHT = 1\n",
        "\n",
        "# Threshold defining \"in support\"\n",
        "EPS_SUPPORT = 1e-3\n",
        "\n",
        "# Search caps\n",
        "MAX_TRIES_1ARC = 2000   # 1-arc candidates (Token-ring is small; this is plenty)\n",
        "MAX_TRIES_2ARC = 6000   # cap for 2-arc search\n",
        "\n",
        "# Candidate pruning for 2-arc search (keeps runtime sane)\n",
        "TOPN_FIRST_ARC  = 120   # how many 1-arc candidates to consider as the first arc\n",
        "TOPN_SECOND_ARC = 120   # how many candidates to consider for the second arc per first\n",
        "\n",
        "# Print control\n",
        "VERBOSE_TRIES = True\n",
        "PRINT_EVERY_1ARC = 50\n",
        "PRINT_EVERY_2ARC = 200\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Helpers: PNML parsing / mutation\n",
        "# ============================================================\n",
        "def strip_ns(tag):\n",
        "    return tag.split(\"}\", 1)[-1] if \"}\" in tag else tag\n",
        "\n",
        "def ensure_download(url, path):\n",
        "    if not os.path.exists(path):\n",
        "        urllib.request.urlretrieve(url, path)\n",
        "\n",
        "def get_pnml_net_root(tree):\n",
        "    root = tree.getroot()\n",
        "    net = None\n",
        "    for e in root.iter():\n",
        "        if strip_ns(e.tag) == \"net\":\n",
        "            net = e\n",
        "            break\n",
        "    if net is None:\n",
        "        raise RuntimeError(\"No <net> element found in PNML.\")\n",
        "    return root, net\n",
        "\n",
        "def existing_ids(net):\n",
        "    ids = set()\n",
        "    for e in net.iter():\n",
        "        i = e.attrib.get(\"id\")\n",
        "        if i:\n",
        "            ids.add(i)\n",
        "    return ids\n",
        "\n",
        "def unique_id(base, used):\n",
        "    if base not in used:\n",
        "        used.add(base)\n",
        "        return base\n",
        "    k = 1\n",
        "    while f\"{base}_{k}\" in used:\n",
        "        k += 1\n",
        "    uid = f\"{base}_{k}\"\n",
        "    used.add(uid)\n",
        "    return uid\n",
        "\n",
        "def get_place_and_transition_ids(net):\n",
        "    places = []\n",
        "    transitions = []\n",
        "    for e in net.iter():\n",
        "        t = strip_ns(e.tag)\n",
        "        if t == \"place\":\n",
        "            pid = e.attrib.get(\"id\")\n",
        "            if pid:\n",
        "                places.append(pid)\n",
        "        elif t == \"transition\":\n",
        "            tid = e.attrib.get(\"id\")\n",
        "            if tid:\n",
        "                transitions.append(tid)\n",
        "\n",
        "    def uniq(seq):\n",
        "        seen = set()\n",
        "        out = []\n",
        "        for x in seq:\n",
        "            if x not in seen:\n",
        "                out.append(x)\n",
        "                seen.add(x)\n",
        "        return out\n",
        "\n",
        "    return uniq(places), uniq(transitions)\n",
        "\n",
        "def collect_existing_arcs(net):\n",
        "    arcs = set()\n",
        "    for e in net.iter():\n",
        "        if strip_ns(e.tag) == \"arc\":\n",
        "            s = e.attrib.get(\"source\")\n",
        "            t = e.attrib.get(\"target\")\n",
        "            if s and t:\n",
        "                arcs.add((s, t))\n",
        "    return arcs\n",
        "\n",
        "def add_arc_with_weight(net, used_ids, arc_id_base, source_id, target_id, weight=1):\n",
        "    arc_id = unique_id(arc_id_base, used_ids)\n",
        "    arc = ET.Element(\"arc\", {\"id\": arc_id, \"source\": source_id, \"target\": target_id})\n",
        "    if weight != 1:\n",
        "        ins = ET.SubElement(arc, \"inscription\")\n",
        "        txt = ET.SubElement(ins, \"text\")\n",
        "        txt.text = str(int(weight))\n",
        "    net.append(arc)\n",
        "    return arc_id\n",
        "\n",
        "def parse_pnml_to_matrices(pnml_file):\n",
        "    tree = ET.parse(pnml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    places = []\n",
        "    transitions = []\n",
        "    arcs = []\n",
        "\n",
        "    for elem in root.iter():\n",
        "        tag = strip_ns(elem.tag)\n",
        "        if tag == \"place\":\n",
        "            pid = elem.attrib.get(\"id\")\n",
        "            if pid:\n",
        "                places.append(pid)\n",
        "        elif tag == \"transition\":\n",
        "            tid = elem.attrib.get(\"id\")\n",
        "            if tid:\n",
        "                transitions.append(tid)\n",
        "        elif tag == \"arc\":\n",
        "            src = elem.attrib.get(\"source\")\n",
        "            tgt = elem.attrib.get(\"target\")\n",
        "            aid = elem.attrib.get(\"id\", \"\")\n",
        "            if src and tgt:\n",
        "                w = 1\n",
        "                for child in elem.iter():\n",
        "                    if strip_ns(child.tag) == \"inscription\":\n",
        "                        for g in child.iter():\n",
        "                            if strip_ns(g.tag) == \"text\" and g.text is not None:\n",
        "                                txt = g.text.strip()\n",
        "                                if txt:\n",
        "                                    try:\n",
        "                                        w = int(txt)\n",
        "                                    except:\n",
        "                                        pass\n",
        "                arcs.append((src, tgt, w, aid))\n",
        "\n",
        "    def uniq(seq):\n",
        "        seen = set()\n",
        "        out = []\n",
        "        for x in seq:\n",
        "            if x not in seen:\n",
        "                out.append(x)\n",
        "                seen.add(x)\n",
        "        return out\n",
        "\n",
        "    places = uniq([p for p in places if p is not None])\n",
        "    transitions = uniq([t for t in transitions if t is not None])\n",
        "\n",
        "    p_index = {p:i for i,p in enumerate(places)}\n",
        "    t_index = {t:i for i,t in enumerate(transitions)}\n",
        "\n",
        "    P = len(places)\n",
        "    T = len(transitions)\n",
        "\n",
        "    Bin = np.zeros((P, T), dtype=float)\n",
        "    Bout = np.zeros((P, T), dtype=float)\n",
        "\n",
        "    ignored = 0\n",
        "    for src, tgt, w, _aid in arcs:\n",
        "        if src in p_index and tgt in t_index:\n",
        "            Bin[p_index[src], t_index[tgt]] += float(w)\n",
        "        elif src in t_index and tgt in p_index:\n",
        "            Bout[p_index[tgt], t_index[src]] += float(w)\n",
        "        else:\n",
        "            ignored += 1\n",
        "\n",
        "    B = Bout - Bin\n",
        "    return places, transitions, arcs, ignored, Bin, Bout, B\n",
        "\n",
        "# ============================================================\n",
        "# LPs:\n",
        "#   1) feasibility of cone slice: Bx=0, x>=0, sum(x)=1\n",
        "#   2) spread LP: minimize z s.t. Bx=0, sum(x)=1, x>=0, x<=z\n",
        "# ============================================================\n",
        "def cone_slice_feasible(B):\n",
        "    m, n = B.shape\n",
        "    A_eq = np.vstack([B, np.ones((1, n))])\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[-1] = 1.0\n",
        "    res = linprog(\n",
        "        np.zeros(n),\n",
        "        A_eq=A_eq,\n",
        "        b_eq=b_eq,\n",
        "        bounds=[(0, None)] * n,\n",
        "        method=\"highs\"\n",
        "    )\n",
        "    return res.success, res\n",
        "\n",
        "def minmax_spread_invariant(B):\n",
        "    # variables: [x_0..x_{n-1}, z]\n",
        "    m, n = B.shape\n",
        "\n",
        "    # Objective: minimize z\n",
        "    c = np.zeros(n + 1)\n",
        "    c[-1] = 1.0\n",
        "\n",
        "    # Equalities: Bx=0 and sum(x)=1; z free but bounded below by 0 anyway\n",
        "    A_eq = np.zeros((m + 1, n + 1))\n",
        "    A_eq[:m, :n] = B\n",
        "    A_eq[m, :n] = 1.0\n",
        "    b_eq = np.zeros(m + 1)\n",
        "    b_eq[m] = 1.0\n",
        "\n",
        "    # Inequalities: x_i - z <= 0  for all i\n",
        "    A_ub = np.zeros((n, n + 1))\n",
        "    b_ub = np.zeros(n)\n",
        "    for i in range(n):\n",
        "        A_ub[i, i] = 1.0\n",
        "        A_ub[i, -1] = -1.0\n",
        "\n",
        "    bounds = [(0, None)] * n + [(0, None)]  # x>=0, z>=0\n",
        "\n",
        "    res = linprog(\n",
        "        c,\n",
        "        A_ub=A_ub,\n",
        "        b_ub=b_ub,\n",
        "        A_eq=A_eq,\n",
        "        b_eq=b_eq,\n",
        "        bounds=bounds,\n",
        "        method=\"highs\"\n",
        "    )\n",
        "    if not res.success:\n",
        "        return False, None, None\n",
        "\n",
        "    x = res.x[:n]\n",
        "    z = res.x[-1]\n",
        "    return True, x, z\n",
        "\n",
        "def support_size(x, eps=1e-3):\n",
        "    return int(np.sum(np.array(x) >= eps))\n",
        "\n",
        "# ============================================================\n",
        "# Candidate generation for 1-arc additions\n",
        "# ============================================================\n",
        "def list_arc_candidates(token_pnml_path):\n",
        "    tree0 = ET.parse(token_pnml_path)\n",
        "    _root0, net0 = get_pnml_net_root(tree0)\n",
        "    places, transitions = get_place_and_transition_ids(net0)\n",
        "    existing_arcs0 = collect_existing_arcs(net0)\n",
        "\n",
        "    candidates = []\n",
        "    # deterministic order: places in file order, transitions in file order\n",
        "    for p in places:\n",
        "        for t in transitions:\n",
        "            if (p, t) not in existing_arcs0:\n",
        "                candidates.append((\"P2T\", p, t))\n",
        "            if (t, p) not in existing_arcs0:\n",
        "                candidates.append((\"T2P\", t, p))\n",
        "    return candidates\n",
        "\n",
        "# ============================================================\n",
        "# Apply 1 or 2 arcs to a fresh copy of token XML root\n",
        "# ============================================================\n",
        "def clone_tree(tree):\n",
        "    # round-trip through bytes gives a deep copy\n",
        "    return ET.ElementTree(ET.fromstring(ET.tostring(tree.getroot())))\n",
        "\n",
        "def write_tree(tree, path):\n",
        "    tree.write(path, encoding=\"utf-8\", xml_declaration=True)\n",
        "\n",
        "def apply_arcs(tree, arcs_to_add, arc_id_prefix=\"a_rescue\", weight=1):\n",
        "    root, net = get_pnml_net_root(tree)\n",
        "    used = existing_ids(net)\n",
        "    existing_arcs = collect_existing_arcs(net)\n",
        "\n",
        "    added_arc_ids = []\n",
        "    for k, (direction, src, tgt) in enumerate(arcs_to_add, start=1):\n",
        "        if (src, tgt) in existing_arcs:\n",
        "            # already present, skip\n",
        "            continue\n",
        "        arc_id = add_arc_with_weight(\n",
        "            net,\n",
        "            used,\n",
        "            arc_id_base=f\"{arc_id_prefix}{k}\",\n",
        "            source_id=src,\n",
        "            target_id=tgt,\n",
        "            weight=weight\n",
        "        )\n",
        "        added_arc_ids.append(arc_id)\n",
        "        existing_arcs.add((src, tgt))\n",
        "    return added_arc_ids\n",
        "\n",
        "# ============================================================\n",
        "# Search: find a 1-arc or 2-arc mutant meeting support>=k\n",
        "# using the min-max spread LP to judge support.\n",
        "# ============================================================\n",
        "def find_rescue_for_k(token_path, out_path, k_support, eps_support=1e-3):\n",
        "    base_tree = ET.parse(token_path)\n",
        "    candidates = list_arc_candidates(token_path)\n",
        "\n",
        "    # --- 1-arc search ---\n",
        "    tries = 0\n",
        "    for (direction, src, tgt) in candidates:\n",
        "        tries += 1\n",
        "        if tries > MAX_TRIES_1ARC:\n",
        "            break\n",
        "\n",
        "        tree = clone_tree(base_tree)\n",
        "        apply_arcs(tree, [(direction, src, tgt)], arc_id_prefix=f\"a_rescue_k{k_support}_\", weight=ARC_WEIGHT)\n",
        "\n",
        "        tmp = \"__tmp_k.pnml\"\n",
        "        write_tree(tree, tmp)\n",
        "        _pl, _tr, _arcs, _ign, _Bin, _Bout, B = parse_pnml_to_matrices(tmp)\n",
        "\n",
        "        feas, _ = cone_slice_feasible(B)\n",
        "        if feas:\n",
        "            ok_spread, x, z = minmax_spread_invariant(B)\n",
        "            if ok_spread:\n",
        "                supp = support_size(x, eps_support)\n",
        "                if VERBOSE_TRIES and (tries % PRINT_EVERY_1ARC == 0 or supp >= k_support):\n",
        "                    print(f\"[k={k_support}] 1-arc try {tries:04d}: {direction} {src}->{tgt}  supp={supp}  z={z:.6g}\")\n",
        "                if supp >= k_support:\n",
        "                    write_tree(tree, out_path)\n",
        "                    try:\n",
        "                        os.remove(tmp)\n",
        "                    except:\n",
        "                        pass\n",
        "                    return {\n",
        "                        \"mode\": \"1-arc\",\n",
        "                        \"k\": k_support,\n",
        "                        \"eps\": eps_support,\n",
        "                        \"tries\": tries,\n",
        "                        \"arcs_added\": [(direction, src, tgt)],\n",
        "                        \"support\": supp,\n",
        "                        \"z\": float(z)\n",
        "                    }\n",
        "        else:\n",
        "            if VERBOSE_TRIES and (tries % PRINT_EVERY_1ARC == 0):\n",
        "                print(f\"[k={k_support}] 1-arc try {tries:04d}: {direction} {src}->{tgt}  infeasible\")\n",
        "\n",
        "    # --- 2-arc search (pruned) ---\n",
        "    # Take a prefix of candidates for the first arc (deterministic).\n",
        "    first_list = candidates[:min(TOPN_FIRST_ARC, len(candidates))]\n",
        "    second_list = candidates[:min(TOPN_SECOND_ARC, len(candidates))]\n",
        "\n",
        "    tries2 = 0\n",
        "    for (d1, s1, t1) in first_list:\n",
        "        for (d2, s2, t2) in second_list:\n",
        "            # avoid duplicates and obvious no-ops\n",
        "            if (d1, s1, t1) == (d2, s2, t2):\n",
        "                continue\n",
        "\n",
        "            tries2 += 1\n",
        "            if tries2 > MAX_TRIES_2ARC:\n",
        "                break\n",
        "\n",
        "            tree = clone_tree(base_tree)\n",
        "            apply_arcs(tree, [(d1, s1, t1), (d2, s2, t2)], arc_id_prefix=f\"a_rescue2_k{k_support}_\", weight=ARC_WEIGHT)\n",
        "\n",
        "            tmp = \"__tmp_k2.pnml\"\n",
        "            write_tree(tree, tmp)\n",
        "            _pl, _tr, _arcs, _ign, _Bin, _Bout, B = parse_pnml_to_matrices(tmp)\n",
        "\n",
        "            feas, _ = cone_slice_feasible(B)\n",
        "            if feas:\n",
        "                ok_spread, x, z = minmax_spread_invariant(B)\n",
        "                if ok_spread:\n",
        "                    supp = support_size(x, eps_support)\n",
        "                    if VERBOSE_TRIES and (tries2 % PRINT_EVERY_2ARC == 0 or supp >= k_support):\n",
        "                        print(f\"[k={k_support}] 2-arc try {tries2:04d}: supp={supp} z={z:.6g}  arcs=({d1} {s1}->{t1}) + ({d2} {s2}->{t2})\")\n",
        "                    if supp >= k_support:\n",
        "                        write_tree(tree, out_path)\n",
        "                        try:\n",
        "                            os.remove(tmp)\n",
        "                        except:\n",
        "                            pass\n",
        "                        return {\n",
        "                            \"mode\": \"2-arc\",\n",
        "                            \"k\": k_support,\n",
        "                            \"eps\": eps_support,\n",
        "                            \"tries\": tries2,\n",
        "                            \"arcs_added\": [(d1, s1, t1), (d2, s2, t2)],\n",
        "                            \"support\": supp,\n",
        "                            \"z\": float(z)\n",
        "                        }\n",
        "            else:\n",
        "                if VERBOSE_TRIES and (tries2 % PRINT_EVERY_2ARC == 0):\n",
        "                    print(f\"[k={k_support}] 2-arc try {tries2:04d}: infeasible  arcs=({d1} {s1}->{t1}) + ({d2} {s2}->{t2})\")\n",
        "\n",
        "        if tries2 > MAX_TRIES_2ARC:\n",
        "            break\n",
        "\n",
        "    # cleanup\n",
        "    for tmp in [\"__tmp_k.pnml\", \"__tmp_k2.pnml\"]:\n",
        "        try:\n",
        "            if os.path.exists(tmp):\n",
        "                os.remove(tmp)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return {\n",
        "        \"mode\": None,\n",
        "        \"k\": k_support,\n",
        "        \"eps\": eps_support,\n",
        "        \"tries_1arc\": tries,\n",
        "        \"tries_2arc\": tries2,\n",
        "        \"arcs_added\": None,\n",
        "        \"support\": None,\n",
        "        \"z\": None\n",
        "    }\n",
        "\n",
        "# ============================================================\n",
        "# Zip all PNML in current directory\n",
        "# ============================================================\n",
        "def zip_all_pnml(zip_name):\n",
        "    pnml_files = sorted([fn for fn in os.listdir(\".\") if fn.lower().endswith(\".pnml\") and os.path.isfile(fn)])\n",
        "    with zipfile.ZipFile(zip_name, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "        for fn in pnml_files:\n",
        "            zf.write(fn)\n",
        "    return pnml_files\n",
        "\n",
        "# ============================================================\n",
        "# RUN: Build rescue_k1..k5\n",
        "# ============================================================\n",
        "ensure_download(TOKEN_URL, TOKEN_FILE)\n",
        "\n",
        "print(\"=== Token-ring rescue suite (k=1..5) ===\")\n",
        "print(\"Base:\", TOKEN_FILE)\n",
        "print(\"EPS_SUPPORT:\", EPS_SUPPORT)\n",
        "print(\"Search caps: 1-arc\", MAX_TRIES_1ARC, \" | 2-arc\", MAX_TRIES_2ARC,\n",
        "      \" | 2-arc pruning:\", TOPN_FIRST_ARC, \"x\", TOPN_SECOND_ARC)\n",
        "print()\n",
        "\n",
        "results = []\n",
        "t0 = time.perf_counter()\n",
        "\n",
        "for k in range(1, 6):\n",
        "    out_name = f\"Token-ring_rescue_k{k}.pnml\"\n",
        "    print(f\"--- Searching rescue for k={k} -> {out_name} ---\")\n",
        "    tk0 = time.perf_counter()\n",
        "    info = find_rescue_for_k(TOKEN_FILE, out_name, k_support=k, eps_support=EPS_SUPPORT)\n",
        "    dt = time.perf_counter() - tk0\n",
        "\n",
        "    # Verify and summarize if found\n",
        "    if info[\"mode\"] is not None and os.path.exists(out_name):\n",
        "        places, transitions, arcs, ignored, Bin, Bout, B = parse_pnml_to_matrices(out_name)\n",
        "        feas, _ = cone_slice_feasible(B)\n",
        "        ok_spread, x, z = minmax_spread_invariant(B) if feas else (False, None, None)\n",
        "        supp = support_size(x, EPS_SUPPORT) if ok_spread else 0\n",
        "        print(f\"FOUND ({info['mode']}) in {dt:.3f}s  | P={len(places)} T={len(transitions)} arcs={len(arcs)}\")\n",
        "        print(\"  added arcs:\")\n",
        "        for (d, s, t) in info[\"arcs_added\"]:\n",
        "            print(f\"    {d}: {s} -> {t}\")\n",
        "        print(f\"  minmax spread: support(x>=eps)={supp}  z={z:.6g}\")\n",
        "    else:\n",
        "        print(f\"FAILED for k={k} (within caps). time={dt:.3f}s\")\n",
        "        print(\"  tries 1-arc:\", info.get(\"tries_1arc\"), \"tries 2-arc:\", info.get(\"tries_2arc\"))\n",
        "\n",
        "    results.append((k, info))\n",
        "    print()\n",
        "\n",
        "total_dt = time.perf_counter() - t0\n",
        "\n",
        "# Repack zip\n",
        "pnml_files = zip_all_pnml(ZIP_NAME)\n",
        "\n",
        "print(\"=== DONE ===\")\n",
        "print(\"Total time:\", f\"{total_dt:.3f}s\")\n",
        "print(\"Wrote zip:\", ZIP_NAME, \"files:\", len(pnml_files))\n",
        "print(\"Rescue outputs present:\")\n",
        "for k in range(1, 6):\n",
        "    fn = f\"Token-ring_rescue_k{k}.pnml\"\n",
        "    print(\" \", fn, \"OK\" if os.path.exists(fn) else \"MISSING\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5iK1Vso7mjB",
        "outputId": "b6740cb3-d195-4f07-ff3f-1b39e22a68c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Token-ring rescue suite (k=1..5) ===\n",
            "Base: Token-ring.pnml\n",
            "EPS_SUPPORT: 0.001\n",
            "Search caps: 1-arc 2000  | 2-arc 6000  | 2-arc pruning: 120 x 120\n",
            "\n",
            "--- Searching rescue for k=1 -> Token-ring_rescue_k1.pnml ---\n",
            "[k=1] 1-arc try 0050: P2T cId302625545293549305179->cId301303558290705273067  infeasible\n",
            "[k=1] 1-arc try 0100: T2P cId301303558290705273021->cId302625545293549305161  infeasible\n",
            "[k=1] 1-arc try 0138: P2T cId30246029697188089233->cId301964552006875653980  supp=1  z=1\n",
            "FOUND (1-arc) in 2.221s  | P=18 T=15 arcs=68\n",
            "  added arcs:\n",
            "    P2T: cId30246029697188089233 -> cId301964552006875653980\n",
            "  minmax spread: support(x>=eps)=1  z=1\n",
            "\n",
            "--- Searching rescue for k=2 -> Token-ring_rescue_k2.pnml ---\n",
            "[k=2] 1-arc try 0050: P2T cId302625545293549305179->cId301303558290705273067  infeasible\n",
            "[k=2] 1-arc try 0100: T2P cId301303558290705273021->cId302625545293549305161  infeasible\n",
            "[k=2] 1-arc try 0150: T2P cId301303558290705273071->cId30246029697188089233  infeasible\n",
            "[k=2] 1-arc try 0200: T2P cId301303558290705273077->cId30163405493404209862  infeasible\n",
            "[k=2] 1-arc try 0250: T2P cId301303558290705273023->cId30163405493404209868  infeasible\n",
            "[k=2] 1-arc try 0300: P2T cId30163405493404209869->cId300807812896203305012  infeasible\n",
            "[k=2] 1-arc try 0350: P2T cId30163405493404209866->cId301138309969036860220  infeasible\n",
            "[k=2] 1-arc try 0400: P2T cId301964552006875653925->cId301964552006875653980  infeasible\n",
            "[k=2] 1-arc try 0450: T2P cId300807812896203305081->cId30196455200687565395  infeasible\n",
            "[k=2] 2-arc try 0200: infeasible  arcs=(T2P cId301964552006875653980->cId302956042366382860362) + (T2P cId301138309969036860220->cId302625545293549305160)\n",
            "[k=2] 2-arc try 0400: infeasible  arcs=(T2P cId300807812896203305081->cId302956042366382860362) + (P2T cId302625545293549305179->cId301303558290705273024)\n",
            "[k=2] 2-arc try 0600: infeasible  arcs=(T2P cId300807812896203305012->cId302956042366382860362) + (P2T cId302956042366382860362->cId300807812896203305012)\n",
            "[k=2] 2-arc try 0800: infeasible  arcs=(P2T cId302956042366382860362->cId300807812896203305013) + (P2T cId302625545293549305161->cId300807812896203305012)\n",
            "[k=2] 2-arc try 1000: infeasible  arcs=(P2T cId302956042366382860362->cId301303558290705273077) + (T2P cId301303558290705273011->cId302625545293549305179)\n",
            "[k=2] 2-arc try 1200: infeasible  arcs=(P2T cId302956042366382860362->cId301303558290705273023) + (T2P cId301303558290705273077->cId302956042366382860362)\n",
            "[k=2] 2-arc try 1400: infeasible  arcs=(T2P cId301303558290705273023->cId302956042366382860362) + (T2P cId301303558290705273077->cId302625545293549305161)\n",
            "[k=2] 2-arc try 1600: infeasible  arcs=(T2P cId301303558290705273071->cId302956042366382860362) + (T2P cId301138309969036860220->cId302625545293549305179)\n",
            "[k=2] 2-arc try 1800: infeasible  arcs=(T2P cId301303558290705273022->cId302956042366382860362) + (P2T cId302956042366382860362->cId301303558290705273022)\n",
            "[k=2] 2-arc try 2000: infeasible  arcs=(P2T cId302956042366382860362->cId301303558290705273021) + (P2T cId302625545293549305161->cId301303558290705273022)\n",
            "[k=2] 2-arc try 2200: infeasible  arcs=(P2T cId302956042366382860362->cId301303558290705273024) + (P2T cId302625545293549305160->cId300807812896203305012)\n",
            "[k=2] 2-arc try 2400: infeasible  arcs=(T2P cId301303558290705273055->cId302956042366382860362) + (P2T cId302956042366382860362->cId301303558290705273055)\n",
            "[k=2] 2-arc try 2600: infeasible  arcs=(P2T cId302956042366382860362->cId301303558290705273011) + (P2T cId302625545293549305161->cId301303558290705273055)\n",
            "[k=2] 2-arc try 2800: infeasible  arcs=(T2P cId301303558290705273067->cId302956042366382860362) + (T2P cId301303558290705273077->cId302625545293549305160)\n",
            "[k=2] 2-arc try 3000: infeasible  arcs=(T2P cId301303558290705273076->cId302956042366382860362) + (P2T cId302956042366382860362->cId301303558290705273076)\n",
            "[k=2] 2-arc try 3200: infeasible  arcs=(P2T cId302956042366382860362->cId301138309969036860220) + (P2T cId302625545293549305161->cId301303558290705273076)\n",
            "[k=2] 2-arc try 3400: infeasible  arcs=(T2P cId300807812896203305081->cId302625545293549305179) + (P2T cId302625545293549305160->cId301303558290705273022)\n",
            "[k=2] 2-arc try 3600: infeasible  arcs=(T2P cId300807812896203305012->cId302625545293549305179) + (P2T cId302625545293549305179->cId300807812896203305012)\n",
            "[k=2] 2-arc try 3800: infeasible  arcs=(P2T cId302625545293549305179->cId300807812896203305013) + (T2P cId301964552006875653980->cId302460296971880892375)\n",
            "[k=2] 2-arc try 4000: infeasible  arcs=(P2T cId302625545293549305179->cId301303558290705273077) + (T2P cId301303558290705273024->cId302625545293549305160)\n",
            "[k=2] 2-arc try 4200: infeasible  arcs=(P2T cId302625545293549305179->cId301303558290705273023) + (T2P cId301303558290705273077->cId302625545293549305179)\n",
            "[k=2] 2-arc try 4400: infeasible  arcs=(T2P cId301303558290705273023->cId302625545293549305179) + (P2T cId302460296971880892375->cId300807812896203305013)\n",
            "[k=2] 2-arc try 4600: infeasible  arcs=(T2P cId301303558290705273071->cId302625545293549305179) + (P2T cId302625545293549305160->cId301303558290705273076)\n",
            "[k=2] 2-arc try 4800: infeasible  arcs=(T2P cId301303558290705273022->cId302625545293549305179) + (P2T cId302625545293549305179->cId301303558290705273022)\n",
            "[k=2] 2-arc try 5000: infeasible  arcs=(T2P cId301303558290705273021->cId302625545293549305179) + (T2P cId301964552006875653980->cId302956042366382860362)\n",
            "[k=2] 2-arc try 5200: infeasible  arcs=(P2T cId302625545293549305179->cId301303558290705273024) + (T2P cId301964552006875653980->cId302625545293549305161)\n",
            "[k=2] 2-arc try 5400: infeasible  arcs=(P2T cId302625545293549305179->cId301303558290705273055) + (T2P cId301303558290705273024->cId302625545293549305179)\n",
            "[k=2] 2-arc try 5600: infeasible  arcs=(P2T cId302625545293549305179->cId301303558290705273011) + (P2T cId302956042366382860362->cId300807812896203305013)\n",
            "[k=2] 2-arc try 5800: infeasible  arcs=(T2P cId301303558290705273011->cId302625545293549305179) + (P2T cId302625545293549305161->cId300807812896203305013)\n",
            "[k=2] 2-arc try 6000: infeasible  arcs=(T2P cId301303558290705273067->cId302625545293549305179) + (P2T cId302625545293549305179->cId301303558290705273067)\n",
            "FAILED for k=2 (within caps). time=63.745s\n",
            "  tries 1-arc: 473 tries 2-arc: 6001\n",
            "\n",
            "--- Searching rescue for k=3 -> Token-ring_rescue_k3.pnml ---\n",
            "[k=3] 1-arc try 0050: P2T cId302625545293549305179->cId301303558290705273067  infeasible\n",
            "[k=3] 1-arc try 0100: T2P cId301303558290705273021->cId302625545293549305161  infeasible\n",
            "[k=3] 1-arc try 0150: T2P cId301303558290705273071->cId30246029697188089233  infeasible\n",
            "[k=3] 1-arc try 0200: T2P cId301303558290705273077->cId30163405493404209862  infeasible\n",
            "[k=3] 1-arc try 0250: T2P cId301303558290705273023->cId30163405493404209868  infeasible\n",
            "[k=3] 1-arc try 0300: P2T cId30163405493404209869->cId300807812896203305012  infeasible\n",
            "[k=3] 1-arc try 0350: P2T cId30163405493404209866->cId301138309969036860220  infeasible\n",
            "[k=3] 1-arc try 0400: P2T cId301964552006875653925->cId301964552006875653980  infeasible\n",
            "[k=3] 1-arc try 0450: T2P cId300807812896203305081->cId30196455200687565395  infeasible\n",
            "[k=3] 2-arc try 0200: infeasible  arcs=(T2P cId301964552006875653980->cId302956042366382860362) + (T2P cId301138309969036860220->cId302625545293549305160)\n",
            "[k=3] 2-arc try 0400: infeasible  arcs=(T2P cId300807812896203305081->cId302956042366382860362) + (P2T cId302625545293549305179->cId301303558290705273024)\n",
            "[k=3] 2-arc try 0600: infeasible  arcs=(T2P cId300807812896203305012->cId302956042366382860362) + (P2T cId302956042366382860362->cId300807812896203305012)\n",
            "[k=3] 2-arc try 0800: infeasible  arcs=(P2T cId302956042366382860362->cId300807812896203305013) + (P2T cId302625545293549305161->cId300807812896203305012)\n",
            "[k=3] 2-arc try 1000: infeasible  arcs=(P2T cId302956042366382860362->cId301303558290705273077) + (T2P cId301303558290705273011->cId302625545293549305179)\n",
            "[k=3] 2-arc try 1200: infeasible  arcs=(P2T cId302956042366382860362->cId301303558290705273023) + (T2P cId301303558290705273077->cId302956042366382860362)\n",
            "[k=3] 2-arc try 1400: infeasible  arcs=(T2P cId301303558290705273023->cId302956042366382860362) + (T2P cId301303558290705273077->cId302625545293549305161)\n",
            "[k=3] 2-arc try 1600: infeasible  arcs=(T2P cId301303558290705273071->cId302956042366382860362) + (T2P cId301138309969036860220->cId302625545293549305179)\n",
            "[k=3] 2-arc try 1800: infeasible  arcs=(T2P cId301303558290705273022->cId302956042366382860362) + (P2T cId302956042366382860362->cId301303558290705273022)\n",
            "[k=3] 2-arc try 2000: infeasible  arcs=(P2T cId302956042366382860362->cId301303558290705273021) + (P2T cId302625545293549305161->cId301303558290705273022)\n",
            "[k=3] 2-arc try 2200: infeasible  arcs=(P2T cId302956042366382860362->cId301303558290705273024) + (P2T cId302625545293549305160->cId300807812896203305012)\n",
            "[k=3] 2-arc try 2400: infeasible  arcs=(T2P cId301303558290705273055->cId302956042366382860362) + (P2T cId302956042366382860362->cId301303558290705273055)\n",
            "[k=3] 2-arc try 2600: infeasible  arcs=(P2T cId302956042366382860362->cId301303558290705273011) + (P2T cId302625545293549305161->cId301303558290705273055)\n",
            "[k=3] 2-arc try 2800: infeasible  arcs=(T2P cId301303558290705273067->cId302956042366382860362) + (T2P cId301303558290705273077->cId302625545293549305160)\n",
            "[k=3] 2-arc try 3000: infeasible  arcs=(T2P cId301303558290705273076->cId302956042366382860362) + (P2T cId302956042366382860362->cId301303558290705273076)\n",
            "[k=3] 2-arc try 3200: infeasible  arcs=(P2T cId302956042366382860362->cId301138309969036860220) + (P2T cId302625545293549305161->cId301303558290705273076)\n",
            "[k=3] 2-arc try 3400: infeasible  arcs=(T2P cId300807812896203305081->cId302625545293549305179) + (P2T cId302625545293549305160->cId301303558290705273022)\n",
            "[k=3] 2-arc try 3600: infeasible  arcs=(T2P cId300807812896203305012->cId302625545293549305179) + (P2T cId302625545293549305179->cId300807812896203305012)\n",
            "[k=3] 2-arc try 3800: infeasible  arcs=(P2T cId302625545293549305179->cId300807812896203305013) + (T2P cId301964552006875653980->cId302460296971880892375)\n",
            "[k=3] 2-arc try 4000: infeasible  arcs=(P2T cId302625545293549305179->cId301303558290705273077) + (T2P cId301303558290705273024->cId302625545293549305160)\n",
            "[k=3] 2-arc try 4200: infeasible  arcs=(P2T cId302625545293549305179->cId301303558290705273023) + (T2P cId301303558290705273077->cId302625545293549305179)\n",
            "[k=3] 2-arc try 4400: infeasible  arcs=(T2P cId301303558290705273023->cId302625545293549305179) + (P2T cId302460296971880892375->cId300807812896203305013)\n",
            "[k=3] 2-arc try 4600: infeasible  arcs=(T2P cId301303558290705273071->cId302625545293549305179) + (P2T cId302625545293549305160->cId301303558290705273076)\n",
            "[k=3] 2-arc try 4800: infeasible  arcs=(T2P cId301303558290705273022->cId302625545293549305179) + (P2T cId302625545293549305179->cId301303558290705273022)\n",
            "[k=3] 2-arc try 5000: infeasible  arcs=(T2P cId301303558290705273021->cId302625545293549305179) + (T2P cId301964552006875653980->cId302956042366382860362)\n",
            "[k=3] 2-arc try 5200: infeasible  arcs=(P2T cId302625545293549305179->cId301303558290705273024) + (T2P cId301964552006875653980->cId302625545293549305161)\n",
            "[k=3] 2-arc try 5400: infeasible  arcs=(P2T cId302625545293549305179->cId301303558290705273055) + (T2P cId301303558290705273024->cId302625545293549305179)\n",
            "[k=3] 2-arc try 5600: infeasible  arcs=(P2T cId302625545293549305179->cId301303558290705273011) + (P2T cId302956042366382860362->cId300807812896203305013)\n",
            "[k=3] 2-arc try 5800: infeasible  arcs=(T2P cId301303558290705273011->cId302625545293549305179) + (P2T cId302625545293549305161->cId300807812896203305013)\n",
            "[k=3] 2-arc try 6000: infeasible  arcs=(T2P cId301303558290705273067->cId302625545293549305179) + (P2T cId302625545293549305179->cId301303558290705273067)\n",
            "FAILED for k=3 (within caps). time=46.434s\n",
            "  tries 1-arc: 473 tries 2-arc: 6001\n",
            "\n",
            "--- Searching rescue for k=4 -> Token-ring_rescue_k4.pnml ---\n",
            "[k=4] 1-arc try 0050: P2T cId302625545293549305179->cId301303558290705273067  infeasible\n",
            "[k=4] 1-arc try 0100: T2P cId301303558290705273021->cId302625545293549305161  infeasible\n",
            "[k=4] 1-arc try 0150: T2P cId301303558290705273071->cId30246029697188089233  infeasible\n",
            "[k=4] 1-arc try 0200: T2P cId301303558290705273077->cId30163405493404209862  infeasible\n",
            "[k=4] 1-arc try 0250: T2P cId301303558290705273023->cId30163405493404209868  infeasible\n",
            "[k=4] 1-arc try 0300: P2T cId30163405493404209869->cId300807812896203305012  infeasible\n",
            "[k=4] 1-arc try 0350: P2T cId30163405493404209866->cId301138309969036860220  infeasible\n",
            "[k=4] 1-arc try 0400: P2T cId301964552006875653925->cId301964552006875653980  infeasible\n",
            "[k=4] 1-arc try 0450: T2P cId300807812896203305081->cId30196455200687565395  infeasible\n",
            "[k=4] 2-arc try 0200: infeasible  arcs=(T2P cId301964552006875653980->cId302956042366382860362) + (T2P cId301138309969036860220->cId302625545293549305160)\n",
            "[k=4] 2-arc try 0400: infeasible  arcs=(T2P cId300807812896203305081->cId302956042366382860362) + (P2T cId302625545293549305179->cId301303558290705273024)\n",
            "[k=4] 2-arc try 0600: infeasible  arcs=(T2P cId300807812896203305012->cId302956042366382860362) + (P2T cId302956042366382860362->cId300807812896203305012)\n",
            "[k=4] 2-arc try 0800: infeasible  arcs=(P2T cId302956042366382860362->cId300807812896203305013) + (P2T cId302625545293549305161->cId300807812896203305012)\n",
            "[k=4] 2-arc try 1000: infeasible  arcs=(P2T cId302956042366382860362->cId301303558290705273077) + (T2P cId301303558290705273011->cId302625545293549305179)\n",
            "[k=4] 2-arc try 1200: infeasible  arcs=(P2T cId302956042366382860362->cId301303558290705273023) + (T2P cId301303558290705273077->cId302956042366382860362)\n",
            "[k=4] 2-arc try 1400: infeasible  arcs=(T2P cId301303558290705273023->cId302956042366382860362) + (T2P cId301303558290705273077->cId302625545293549305161)\n",
            "[k=4] 2-arc try 1600: infeasible  arcs=(T2P cId301303558290705273071->cId302956042366382860362) + (T2P cId301138309969036860220->cId302625545293549305179)\n",
            "[k=4] 2-arc try 1800: infeasible  arcs=(T2P cId301303558290705273022->cId302956042366382860362) + (P2T cId302956042366382860362->cId301303558290705273022)\n",
            "[k=4] 2-arc try 2000: infeasible  arcs=(P2T cId302956042366382860362->cId301303558290705273021) + (P2T cId302625545293549305161->cId301303558290705273022)\n",
            "[k=4] 2-arc try 2200: infeasible  arcs=(P2T cId302956042366382860362->cId301303558290705273024) + (P2T cId302625545293549305160->cId300807812896203305012)\n",
            "[k=4] 2-arc try 2400: infeasible  arcs=(T2P cId301303558290705273055->cId302956042366382860362) + (P2T cId302956042366382860362->cId301303558290705273055)\n",
            "[k=4] 2-arc try 2600: infeasible  arcs=(P2T cId302956042366382860362->cId301303558290705273011) + (P2T cId302625545293549305161->cId301303558290705273055)\n",
            "[k=4] 2-arc try 2800: infeasible  arcs=(T2P cId301303558290705273067->cId302956042366382860362) + (T2P cId301303558290705273077->cId302625545293549305160)\n",
            "[k=4] 2-arc try 3000: infeasible  arcs=(T2P cId301303558290705273076->cId302956042366382860362) + (P2T cId302956042366382860362->cId301303558290705273076)\n",
            "[k=4] 2-arc try 3200: infeasible  arcs=(P2T cId302956042366382860362->cId301138309969036860220) + (P2T cId302625545293549305161->cId301303558290705273076)\n",
            "[k=4] 2-arc try 3400: infeasible  arcs=(T2P cId300807812896203305081->cId302625545293549305179) + (P2T cId302625545293549305160->cId301303558290705273022)\n",
            "[k=4] 2-arc try 3600: infeasible  arcs=(T2P cId300807812896203305012->cId302625545293549305179) + (P2T cId302625545293549305179->cId300807812896203305012)\n",
            "[k=4] 2-arc try 3800: infeasible  arcs=(P2T cId302625545293549305179->cId300807812896203305013) + (T2P cId301964552006875653980->cId302460296971880892375)\n",
            "[k=4] 2-arc try 4000: infeasible  arcs=(P2T cId302625545293549305179->cId301303558290705273077) + (T2P cId301303558290705273024->cId302625545293549305160)\n",
            "[k=4] 2-arc try 4200: infeasible  arcs=(P2T cId302625545293549305179->cId301303558290705273023) + (T2P cId301303558290705273077->cId302625545293549305179)\n",
            "[k=4] 2-arc try 4400: infeasible  arcs=(T2P cId301303558290705273023->cId302625545293549305179) + (P2T cId302460296971880892375->cId300807812896203305013)\n",
            "[k=4] 2-arc try 4600: infeasible  arcs=(T2P cId301303558290705273071->cId302625545293549305179) + (P2T cId302625545293549305160->cId301303558290705273076)\n",
            "[k=4] 2-arc try 4800: infeasible  arcs=(T2P cId301303558290705273022->cId302625545293549305179) + (P2T cId302625545293549305179->cId301303558290705273022)\n",
            "[k=4] 2-arc try 5000: infeasible  arcs=(T2P cId301303558290705273021->cId302625545293549305179) + (T2P cId301964552006875653980->cId302956042366382860362)\n",
            "[k=4] 2-arc try 5200: infeasible  arcs=(P2T cId302625545293549305179->cId301303558290705273024) + (T2P cId301964552006875653980->cId302625545293549305161)\n",
            "[k=4] 2-arc try 5400: infeasible  arcs=(P2T cId302625545293549305179->cId301303558290705273055) + (T2P cId301303558290705273024->cId302625545293549305179)\n",
            "[k=4] 2-arc try 5600: infeasible  arcs=(P2T cId302625545293549305179->cId301303558290705273011) + (P2T cId302956042366382860362->cId300807812896203305013)\n",
            "[k=4] 2-arc try 5800: infeasible  arcs=(T2P cId301303558290705273011->cId302625545293549305179) + (P2T cId302625545293549305161->cId300807812896203305013)\n",
            "[k=4] 2-arc try 6000: infeasible  arcs=(T2P cId301303558290705273067->cId302625545293549305179) + (P2T cId302625545293549305179->cId301303558290705273067)\n",
            "FAILED for k=4 (within caps). time=45.989s\n",
            "  tries 1-arc: 473 tries 2-arc: 6001\n",
            "\n",
            "--- Searching rescue for k=5 -> Token-ring_rescue_k5.pnml ---\n",
            "[k=5] 1-arc try 0050: P2T cId302625545293549305179->cId301303558290705273067  infeasible\n",
            "[k=5] 1-arc try 0100: T2P cId301303558290705273021->cId302625545293549305161  infeasible\n",
            "[k=5] 1-arc try 0150: T2P cId301303558290705273071->cId30246029697188089233  infeasible\n",
            "[k=5] 1-arc try 0200: T2P cId301303558290705273077->cId30163405493404209862  infeasible\n",
            "[k=5] 1-arc try 0250: T2P cId301303558290705273023->cId30163405493404209868  infeasible\n",
            "[k=5] 1-arc try 0300: P2T cId30163405493404209869->cId300807812896203305012  infeasible\n",
            "[k=5] 1-arc try 0350: P2T cId30163405493404209866->cId301138309969036860220  infeasible\n",
            "[k=5] 1-arc try 0400: P2T cId301964552006875653925->cId301964552006875653980  infeasible\n",
            "[k=5] 1-arc try 0450: T2P cId300807812896203305081->cId30196455200687565395  infeasible\n",
            "[k=5] 2-arc try 0200: infeasible  arcs=(T2P cId301964552006875653980->cId302956042366382860362) + (T2P cId301138309969036860220->cId302625545293549305160)\n",
            "[k=5] 2-arc try 0400: infeasible  arcs=(T2P cId300807812896203305081->cId302956042366382860362) + (P2T cId302625545293549305179->cId301303558290705273024)\n",
            "[k=5] 2-arc try 0600: infeasible  arcs=(T2P cId300807812896203305012->cId302956042366382860362) + (P2T cId302956042366382860362->cId300807812896203305012)\n",
            "[k=5] 2-arc try 0800: infeasible  arcs=(P2T cId302956042366382860362->cId300807812896203305013) + (P2T cId302625545293549305161->cId300807812896203305012)\n",
            "[k=5] 2-arc try 1000: infeasible  arcs=(P2T cId302956042366382860362->cId301303558290705273077) + (T2P cId301303558290705273011->cId302625545293549305179)\n",
            "[k=5] 2-arc try 1200: infeasible  arcs=(P2T cId302956042366382860362->cId301303558290705273023) + (T2P cId301303558290705273077->cId302956042366382860362)\n",
            "[k=5] 2-arc try 1400: infeasible  arcs=(T2P cId301303558290705273023->cId302956042366382860362) + (T2P cId301303558290705273077->cId302625545293549305161)\n",
            "[k=5] 2-arc try 1600: infeasible  arcs=(T2P cId301303558290705273071->cId302956042366382860362) + (T2P cId301138309969036860220->cId302625545293549305179)\n",
            "[k=5] 2-arc try 1800: infeasible  arcs=(T2P cId301303558290705273022->cId302956042366382860362) + (P2T cId302956042366382860362->cId301303558290705273022)\n",
            "[k=5] 2-arc try 2000: infeasible  arcs=(P2T cId302956042366382860362->cId301303558290705273021) + (P2T cId302625545293549305161->cId301303558290705273022)\n",
            "[k=5] 2-arc try 2200: infeasible  arcs=(P2T cId302956042366382860362->cId301303558290705273024) + (P2T cId302625545293549305160->cId300807812896203305012)\n",
            "[k=5] 2-arc try 2400: infeasible  arcs=(T2P cId301303558290705273055->cId302956042366382860362) + (P2T cId302956042366382860362->cId301303558290705273055)\n",
            "[k=5] 2-arc try 2600: infeasible  arcs=(P2T cId302956042366382860362->cId301303558290705273011) + (P2T cId302625545293549305161->cId301303558290705273055)\n",
            "[k=5] 2-arc try 2800: infeasible  arcs=(T2P cId301303558290705273067->cId302956042366382860362) + (T2P cId301303558290705273077->cId302625545293549305160)\n",
            "[k=5] 2-arc try 3000: infeasible  arcs=(T2P cId301303558290705273076->cId302956042366382860362) + (P2T cId302956042366382860362->cId301303558290705273076)\n",
            "[k=5] 2-arc try 3200: infeasible  arcs=(P2T cId302956042366382860362->cId301138309969036860220) + (P2T cId302625545293549305161->cId301303558290705273076)\n",
            "[k=5] 2-arc try 3400: infeasible  arcs=(T2P cId300807812896203305081->cId302625545293549305179) + (P2T cId302625545293549305160->cId301303558290705273022)\n",
            "[k=5] 2-arc try 3600: infeasible  arcs=(T2P cId300807812896203305012->cId302625545293549305179) + (P2T cId302625545293549305179->cId300807812896203305012)\n",
            "[k=5] 2-arc try 3800: infeasible  arcs=(P2T cId302625545293549305179->cId300807812896203305013) + (T2P cId301964552006875653980->cId302460296971880892375)\n",
            "[k=5] 2-arc try 4000: infeasible  arcs=(P2T cId302625545293549305179->cId301303558290705273077) + (T2P cId301303558290705273024->cId302625545293549305160)\n",
            "[k=5] 2-arc try 4200: infeasible  arcs=(P2T cId302625545293549305179->cId301303558290705273023) + (T2P cId301303558290705273077->cId302625545293549305179)\n",
            "[k=5] 2-arc try 4400: infeasible  arcs=(T2P cId301303558290705273023->cId302625545293549305179) + (P2T cId302460296971880892375->cId300807812896203305013)\n",
            "[k=5] 2-arc try 4600: infeasible  arcs=(T2P cId301303558290705273071->cId302625545293549305179) + (P2T cId302625545293549305160->cId301303558290705273076)\n",
            "[k=5] 2-arc try 4800: infeasible  arcs=(T2P cId301303558290705273022->cId302625545293549305179) + (P2T cId302625545293549305179->cId301303558290705273022)\n",
            "[k=5] 2-arc try 5000: infeasible  arcs=(T2P cId301303558290705273021->cId302625545293549305179) + (T2P cId301964552006875653980->cId302956042366382860362)\n",
            "[k=5] 2-arc try 5200: infeasible  arcs=(P2T cId302625545293549305179->cId301303558290705273024) + (T2P cId301964552006875653980->cId302625545293549305161)\n",
            "[k=5] 2-arc try 5400: infeasible  arcs=(P2T cId302625545293549305179->cId301303558290705273055) + (T2P cId301303558290705273024->cId302625545293549305179)\n",
            "[k=5] 2-arc try 5600: infeasible  arcs=(P2T cId302625545293549305179->cId301303558290705273011) + (P2T cId302956042366382860362->cId300807812896203305013)\n",
            "[k=5] 2-arc try 5800: infeasible  arcs=(T2P cId301303558290705273011->cId302625545293549305179) + (P2T cId302625545293549305161->cId300807812896203305013)\n",
            "[k=5] 2-arc try 6000: infeasible  arcs=(T2P cId301303558290705273067->cId302625545293549305179) + (P2T cId302625545293549305179->cId301303558290705273067)\n",
            "FAILED for k=5 (within caps). time=45.652s\n",
            "  tries 1-arc: 473 tries 2-arc: 6001\n",
            "\n",
            "=== DONE ===\n",
            "Total time: 204.063s\n",
            "Wrote zip: pnml_suite.zip files: 12\n",
            "Rescue outputs present:\n",
            "  Token-ring_rescue_k1.pnml OK\n",
            "  Token-ring_rescue_k2.pnml MISSING\n",
            "  Token-ring_rescue_k3.pnml MISSING\n",
            "  Token-ring_rescue_k4.pnml MISSING\n",
            "  Token-ring_rescue_k5.pnml MISSING\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Token-ring structural rescue suite\n",
        "# Steps 15, fully explicit\n",
        "# ============================================================\n",
        "\n",
        "# -----------------------------\n",
        "# Step 1. Load PNML\n",
        "# -----------------------------\n",
        "#  1. Parse places, transitions, arcs\n",
        "#  2. Build incidence matrices\n",
        "# -----------------------------\n",
        "\n",
        "import itertools\n",
        "import time\n",
        "from pnml import load_pnml, save_pnml\n",
        "from lp_solver import check_feasible_cover\n",
        "\n",
        "BASE_PNML = \"Token-ring.pnml\"\n",
        "EPS_SUPPORT = 1e-3\n",
        "\n",
        "net = load_pnml(BASE_PNML)\n",
        "P = net.places\n",
        "T = net.transitions\n",
        "\n",
        "# -----------------------------\n",
        "# Step 2. Baseline check\n",
        "# -----------------------------\n",
        "#  Verify uncovered transitions exist\n",
        "# -----------------------------\n",
        "\n",
        "baseline_uncovered = check_feasible_cover(net)\n",
        "assert len(baseline_uncovered) > 0, \"Net already covered\"\n",
        "\n",
        "# -----------------------------\n",
        "# Step 3. Generate candidate arcs\n",
        "# -----------------------------\n",
        "#  P2T and T2P arcs not already present\n",
        "# -----------------------------\n",
        "\n",
        "existing = set(net.arcs)\n",
        "\n",
        "candidate_arcs = []\n",
        "\n",
        "for p in P:\n",
        "    for t in T:\n",
        "        if (p, t) not in existing:\n",
        "            candidate_arcs.append((\"P2T\", p, t))\n",
        "\n",
        "for t in T:\n",
        "    for p in P:\n",
        "        if (t, p) not in existing:\n",
        "            candidate_arcs.append((\"T2P\", t, p))\n",
        "\n",
        "# -----------------------------\n",
        "# Step 4. Rescue search (k = 1..5)\n",
        "# -----------------------------\n",
        "\n",
        "MAX_TRIES_1 = 2000\n",
        "MAX_TRIES_2 = 6000\n",
        "\n",
        "results = {}\n",
        "\n",
        "for k in range(1, 6):\n",
        "    print(f\"\\n--- Searching rescue for k={k} ---\")\n",
        "    start = time.time()\n",
        "    found = False\n",
        "\n",
        "    # ---- k = 1 : single arc\n",
        "    if k == 1:\n",
        "        for i, arc in enumerate(candidate_arcs[:MAX_TRIES_1]):\n",
        "            net_try = net.copy()\n",
        "            net_try.add_arc(*arc)\n",
        "\n",
        "            ok, support = check_feasible_cover(net_try, eps=EPS_SUPPORT, return_support=True)\n",
        "            if ok:\n",
        "                results[k] = net_try\n",
        "                found = True\n",
        "                print(f\"FOUND k=1 after {i+1} tries, support={support}\")\n",
        "                break\n",
        "\n",
        "    # ---- k >= 2 : combinatorial arc sets\n",
        "    else:\n",
        "        for i, arcs in enumerate(itertools.combinations(candidate_arcs, k)):\n",
        "            if i >= MAX_TRIES_2:\n",
        "                break\n",
        "\n",
        "            net_try = net.copy()\n",
        "            for arc in arcs:\n",
        "                net_try.add_arc(*arc)\n",
        "\n",
        "            ok = check_feasible_cover(net_try, eps=EPS_SUPPORT)\n",
        "            if ok:\n",
        "                results[k] = net_try\n",
        "                found = True\n",
        "                print(f\"FOUND k={k} after {i+1} tries\")\n",
        "                break\n",
        "\n",
        "    elapsed = time.time() - start\n",
        "    if not found:\n",
        "        print(f\"FAILED for k={k} (time={elapsed:.2f}s)\")\n",
        "    else:\n",
        "        save_pnml(net_try, f\"Token-ring_rescue_k{k}.pnml\")\n",
        "\n",
        "# -----------------------------\n",
        "# Step 5. Package outputs\n",
        "# -----------------------------\n",
        "#  Zip original + rescues\n",
        "# -----------------------------\n",
        "\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"pnml_suite.zip\", \"w\") as z:\n",
        "    z.write(BASE_PNML)\n",
        "    for k, net_k in results.items():\n",
        "        z.write(f\"Token-ring_rescue_k{k}.pnml\")\n",
        "\n",
        "print(\"\\nDONE. Results:\", list(results.keys()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "id": "rm2t2fNxATu7",
        "outputId": "f99b5f2e-87de-4e72-e73e-5dcce1013810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pnml'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1044225894.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpnml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_pnml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_pnml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlp_solver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_feasible_cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pnml'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# Token-ring rescue (STRUCTURAL)\n",
        "# uses existing PNML + GICT pipeline\n",
        "# ============================================\n",
        "\n",
        "import itertools\n",
        "import time\n",
        "import copy\n",
        "\n",
        "BASE = \"Token-ring.pnml\"\n",
        "EPS_SUPPORT = 1e-3\n",
        "\n",
        "net0 = parse_pnml(BASE)\n",
        "\n",
        "def uncovered(net):\n",
        "    _, labels, _ = run_gict(net)\n",
        "    return [i for i, l in labels.items() if l != \"CoveredWitness\"]\n",
        "\n",
        "base_unc = uncovered(net0)\n",
        "assert len(base_unc) > 0, \"Net already covered\"\n",
        "\n",
        "# ---- generate candidate arcs\n",
        "places = list(net0.places.keys())\n",
        "trans  = list(net0.transitions.keys())\n",
        "existing = set(net0.arcs)\n",
        "\n",
        "candidates = []\n",
        "\n",
        "for p in places:\n",
        "    for t in trans:\n",
        "        if (p, t) not in existing:\n",
        "            candidates.append((\"P2T\", p, t))\n",
        "\n",
        "for t in trans:\n",
        "    for p in places:\n",
        "        if (t, p) not in existing:\n",
        "            candidates.append((\"T2P\", t, p))\n",
        "\n",
        "results = {}\n",
        "\n",
        "for k in range(1, 6):\n",
        "    print(f\"\\n--- k={k} ---\")\n",
        "    start = time.time()\n",
        "    found = False\n",
        "\n",
        "    for i, arcs in enumerate(itertools.combinations(candidates, k)):\n",
        "        net = copy.deepcopy(net0)\n",
        "\n",
        "        for a in arcs:\n",
        "            net.add_arc(*a)\n",
        "\n",
        "        if len(uncovered(net)) == 0:\n",
        "            print(f\"FOUND rescue for k={k} after {i+1} tries\")\n",
        "            write_pnml(net, f\"Token-ring_rescue_k{k}.pnml\")\n",
        "            results[k] = arcs\n",
        "            found = True\n",
        "            break\n",
        "\n",
        "        if k == 1 and i >= 2000:\n",
        "            break\n",
        "        if k > 1 and i >= 6000:\n",
        "            break\n",
        "\n",
        "    if not found:\n",
        "        print(f\"FAILED for k={k}\")\n",
        "\n",
        "print(\"\\nRescues found:\", results)"
      ],
      "metadata": {
        "id": "eiVh06DWAudT",
        "outputId": "29a045fc-484e-485b-e483-650bc6f7498d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'parse_pnml' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1990878772.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mEPS_SUPPORT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mnet0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_pnml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0muncovered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'parse_pnml' is not defined"
          ]
        }
      ]
    }
  ]
}