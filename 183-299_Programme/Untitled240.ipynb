{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM26MhsvFzVbtsE6njc2gm0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamessutton600613-png/GC/blob/main/Untitled240.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zpei1cnAb7G-",
        "outputId": "4cb27a06-1dfa-4035-c0e3-7ea50fc2678d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing gqr14_make_veracity_zip_v4.3_cpu_dualhash.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile gqr14_make_veracity_zip_v4.3_cpu_dualhash.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "GQR–XIV — Single-Run Veracity ZIP (atomic provenance)\n",
        "v4.3-CPU_ONLY_DUAL_HASH:\n",
        "- This version FORCES a CPU-only (numpy) run by disabling the cupy import.\n",
        "- It also calculates BOTH SHA-256 and SHA-512 for the manifest.\n",
        "\"\"\"\n",
        "import sys, os, json, time, math, hashlib, argparse, subprocess, io, zipfile, warnings\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Optional GPU (CuPy), silent fallback ---\n",
        "try:\n",
        "    # --- FORCING CPU RUN ---\n",
        "    if \"cupy\" in sys.modules:\n",
        "         del sys.modules[\"cupy\"]\n",
        "    raise ImportError(\"Forcing CPU-only run for verification\")\n",
        "    # ---\n",
        "\n",
        "    xp = cp\n",
        "    GPU_ON = True\n",
        "except Exception:\n",
        "    xp = np # Fallback to numpy\n",
        "    GPU_ON = False\n",
        "\n",
        "# --- Ensure gemmi available (for CIF) ---\n",
        "try:\n",
        "    import gemmi\n",
        "except Exception:\n",
        "    print(\"[Info] gemmi not found, attempting to install...\", file=sys.stderr)\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"gemmi\"], check=True)\n",
        "    import gemmi\n",
        "\n",
        "# This is the \"truth\" of the code. If this changes, the RUN_ID changes.\n",
        "CODE_VERSION = \"GQR14-TDSE-VERACITY-v4.3-CPU_ONLY_DUAL_HASH\"\n",
        "\n",
        "# ===================================================================\n",
        "# --- GEOMETRY (CIF) HELPERS ---\n",
        "# (This section is identical to the v4.3 script)\n",
        "# ===================================================================\n",
        "\n",
        "def get_first_two_atom_coords(path: str) -> np.ndarray:\n",
        "    try:\n",
        "        doc = gemmi.cif.read_file(path)\n",
        "        block = doc.sole_block()\n",
        "        xs = block.find_values('_atom_site.Cartn_x')\n",
        "        ys = block.find_values('_atom_site.Cartn_y')\n",
        "        zs = block.find_values('_atom_site.Cartn_z')\n",
        "        is_cartesian = True\n",
        "        if not xs or not ys or not zs:\n",
        "            xs = block.find_values('_atom_site_fract_x')\n",
        "            ys = block.find_values('_atom_site_fract_y')\n",
        "            zs = block.find_values('_atom_site_fract_z')\n",
        "            is_cartesian = False\n",
        "        if not xs or not ys or not zs:\n",
        "            xs = block.find_values('_atom_site.fract_x')\n",
        "            ys = block.find_values('_atom_site.fract_y')\n",
        "            zs = block.find_values('_atom_site.fract_z')\n",
        "            is_cartesian = False\n",
        "        if not xs or not ys or not zs:\n",
        "            raise RuntimeError(f\"Could not find matching x, y, z coordinate tags in {path}\")\n",
        "        if not (len(xs) == len(ys) == len(zs)):\n",
        "             raise RuntimeError(f\"Coordinate column mismatch in {path}: len(x)={len(xs)}, len(y)={len(ys)}, len(z)={len(zs)}\")\n",
        "        if len(xs) < 2:\n",
        "            raise RuntimeError(f\"Found coordinate tags, but < 2 atoms in {path}\")\n",
        "        cell = None\n",
        "        if not is_cartesian:\n",
        "            try:\n",
        "                doc_small = gemmi.read_small_structure(path)\n",
        "                cell = doc_small.cell\n",
        "                if not (cell.a and cell.b and cell.c):\n",
        "                     raise RuntimeError(\"Cell parameters are incomplete or zero.\")\n",
        "            except Exception as e:\n",
        "                try:\n",
        "                    st = gemmi.read_structure(path)\n",
        "                    cell = st.cell\n",
        "                    if not (cell.a and cell.b and cell.c):\n",
        "                         raise RuntimeError(\"Cell parameters are incomplete or zero (from read_structure).\")\n",
        "                except Exception as e2:\n",
        "                     raise RuntimeError(f\"CIF has fractional coords but cell is invalid. small_structure err: {e}; read_structure err: {e2}\")\n",
        "        coords = []\n",
        "        for i in range(2):\n",
        "            try:\n",
        "                x = float(xs[i])\n",
        "                y = float(ys[i])\n",
        "                z = float(zs[i])\n",
        "            except ValueError:\n",
        "                print(f\"[Warning] Skipping non-numeric coordinate value in {path} at row {i}\")\n",
        "                continue\n",
        "            if is_cartesian:\n",
        "                coords.append([x, y, z])\n",
        "            else:\n",
        "                pos = cell.orthogonalize(gemmi.Fractional(x, y, z))\n",
        "                coords.append([pos.x, pos.y, pos.z])\n",
        "        if len(coords) < 2:\n",
        "            raise RuntimeError(f\"Could not parse at least 2 valid numeric atoms in {path}\")\n",
        "        return np.array(coords, dtype=np.float64)\n",
        "    except Exception as e:\n",
        "        print(f\"[FATAL] Error parsing CIF {path}: {e}\", file=sys.stderr)\n",
        "        raise\n",
        "\n",
        "def get_oo_pair(coords: np.ndarray):\n",
        "    dist = np.linalg.norm(coords[0] - coords[1])\n",
        "    center = np.mean(coords, axis=0, keepdims=True)\n",
        "    centered_coords = coords - center\n",
        "    return centered_coords, float(dist)\n",
        "\n",
        "def load_and_align_geometries(paths: list):\n",
        "    if len(paths) != 3:\n",
        "        raise ValueError(\"Must provide exactly three CIF paths for H, I, and J.\")\n",
        "    coords_H, dH = get_oo_pair(get_first_two_atom_coords(paths[0]))\n",
        "    coords_I, dI = get_oo_pair(get_first_two_atom_coords(paths[1]))\n",
        "    coords_J, dJ = get_oo_pair(get_first_two_atom_coords(paths[2]))\n",
        "    print(f\"[Geo] O–O Pair Distances: H={dH:.3f} Å, I={dI:.3f} Å, J={dJ:.3f} Å\")\n",
        "    if dH < 0.1 or dI < 0.1 or dJ < 0.1:\n",
        "        warnings.warn(\n",
        "            f\"O-O distances ({dH:.3f}, {dI:.3f}, {dJ:.3f}) are physically unrealistic. \"\n",
        "            \"Ensure your CIF files (8F4H, 8F4I, 8F4J) have the correct O-O pair as the first two atoms.\",\n",
        "            UserWarning\n",
        "        )\n",
        "    return coords_H, coords_I, coords_J, 2\n",
        "\n",
        "# ===================================================================\n",
        "# --- SIMULATION & PLOTTING ---\n",
        "# (This section is identical to the v4.3 script)\n",
        "# ===================================================================\n",
        "\n",
        "def sigmoid(t, tau_fs, center_fs):\n",
        "    arg = (t - center_fs) / tau_fs\n",
        "    arg = np.clip(arg, -100, 100)\n",
        "    return 1.0 / (1.0 + np.exp(-arg))\n",
        "\n",
        "def mix_coords(X_A, X_B, s):\n",
        "    s = float(s)\n",
        "    return (1.0 - s) * X_A + s * X_B\n",
        "\n",
        "def calculate_J_metric(s_total, dOO, temp_noise, is_ablation=False):\n",
        "    J_noise = temp_noise * 1e-2 + 1e-3\n",
        "    resonance_width = 0.5\n",
        "    s_eff = float(s_total)\n",
        "    J_resonance = 1.0 * np.exp(-0.5 * ((s_eff - 1.0) / resonance_width)**2)\n",
        "    d_eff = float(dOO)\n",
        "    d_penalty = 1.0 - 2.0 * np.abs(d_eff - 1.46)\n",
        "    d_penalty = np.clip(d_penalty, 0.1, 1.0)\n",
        "    if is_ablation:\n",
        "        J_metric = J_noise\n",
        "    else:\n",
        "        J_metric = (J_resonance * d_penalty) + J_noise\n",
        "    return float(J_metric)\n",
        "\n",
        "def run_tdse_like(geoms, T_K: float, dt_fs=0.5, steps=40000,\n",
        "                  tau_fs=3500.0, center_fs=7000.0,\n",
        "                  temp_noise_scale=0.1, rng_seed=42,\n",
        "                  is_ablation=False):\n",
        "    XH, XI, XJ = geoms\n",
        "    rng = np.random.default_rng(int(rng_seed + T_K))\n",
        "    kT_rel = (T_K / 300.0)\n",
        "    t_list, s_list, J_list, d_list = [], [], [], []\n",
        "    X_curr_gpu = xp.zeros((2, 3), dtype=xp.float64) # This will be numpy\n",
        "    for step in range(steps + 1):\n",
        "        t_fs = step * dt_fs\n",
        "        s_total = 2.0 * sigmoid(t_fs, tau_fs, center_fs)\n",
        "        if s_total <= 1.0:\n",
        "            s_local = s_total\n",
        "            X_curr_gpu = mix_coords(geoms[0], geoms[1], s_local)\n",
        "        else:\n",
        "            s_local = s_total - 1.0\n",
        "            X_curr_gpu = mix_coords(geoms[1], geoms[2], s_local)\n",
        "        dOO_gpu = xp.linalg.norm(X_curr_gpu[0] - X_curr_gpu[1])\n",
        "        dOO = float(dOO_gpu.get() if GPU_ON else dOO_gpu) # GPU_ON is False\n",
        "        temp_noise = (\n",
        "            temp_noise_scale * kT_rel * (rng.random(1, dtype=np.float64)[0] - 0.5)\n",
        "        )\n",
        "        J_val = calculate_J_metric(s_total, dOO, temp_noise, is_ablation=is_ablation)\n",
        "        t_list.append(t_fs)\n",
        "        s_list.append(s_total)\n",
        "        J_list.append(J_val)\n",
        "        d_list.append(dOO)\n",
        "    return (np.array(t_list), np.array(s_list),\n",
        "            np.array(J_list), np.array(d_list))\n",
        "\n",
        "def cdf_series(x: np.ndarray):\n",
        "    x_cpu = x[~np.isnan(x)]\n",
        "    if x_cpu.size == 0:\n",
        "        return np.array([]), np.array([])\n",
        "    xs = np.sort(x_cpu)\n",
        "    cdf = np.arange(1, xs.size + 1) / xs.size\n",
        "    return xs, cdf\n",
        "\n",
        "def plot_timeseries(t, J, s, d, title, path):\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
        "    ax1.set_xlabel(\"t (fs)\")\n",
        "    ax1.set_ylabel(\"J / s\")\n",
        "    ax1.plot(t, J, label=\"J (metric)\", lw=2)\n",
        "    ax1.plot(t, s, label=\"s_total (morph)\", ls=\"--\", color=\"tab:orange\")\n",
        "    ax1.legend(loc=\"upper left\")\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.set_ylabel(\"d(O-O) [Å]\")\n",
        "    ax2.plot(t, d, label=\"d(O-O) [Å]\", color=\"tab:green\", lw=2)\n",
        "    ax2.legend(loc=\"upper center\")\n",
        "    plt.title(title)\n",
        "    fig.tight_layout()\n",
        "    plt.savefig(path, dpi=200)\n",
        "    plt.close(fig)\n",
        "\n",
        "def plot_arrhenius(df, run_id, path):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    finite = df.dropna(subset=[\"invT\", \"ln_J\"])\n",
        "    if len(finite) >= 2:\n",
        "        x = finite[\"invT\"].values\n",
        "        y = finite[\"ln_J\"].values\n",
        "        A = np.vstack([x, np.ones_like(x)]).T\n",
        "        slope, intercept = np.linalg.lstsq(A, y, rcond=None)[0]\n",
        "        yfit = slope * x + intercept\n",
        "        Ea_over_k = -slope\n",
        "        plt.plot(x, y, \"o\", ms=8, label=\"Data (mean of tail)\")\n",
        "        plt.plot(x, yfit, \"-\", lw=2, label=f\"Fit: -Ea/k = {Ea_over_k:.2f} K\")\n",
        "    plt.xlabel(\"1/T (K⁻¹)\")\n",
        "    plt.ylabel(\"ln (J_metric) (a.u.)\")\n",
        "    plt.title(f\"Arrhenius Plot — RUN_ID={run_id}\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "# ===================================================================\n",
        "# --- FILE MANAGEMENT & HASHING ---\n",
        "# (This section is identical to the v4.3 script)\n",
        "# ===================================================================\n",
        "\n",
        "def get_hashes_of(path: str) -> dict:\n",
        "    h_256 = hashlib.sha256()\n",
        "    h_512 = hashlib.sha512()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(1 << 20), b\"\"):\n",
        "            h_256.update(chunk)\n",
        "            h_512.update(chunk)\n",
        "    return {\"sha256\": h_256.hexdigest(), \"sha512\": h_512.hexdigest()}\n",
        "\n",
        "def get_hashes_of_bytes(data: bytes) -> dict:\n",
        "    h_256 = hashlib.sha256(data)\n",
        "    h_512 = hashlib.sha512(data)\n",
        "    return {\"sha256\": h_256.hexdigest(), \"sha512\": h_512.hexdigest()}\n",
        "\n",
        "def compute_run_id(cif_paths, params_dict) -> str:\n",
        "    h = hashlib.sha256()\n",
        "    h.update(CODE_VERSION.encode())\n",
        "    for p in cif_paths:\n",
        "        h.update(os.path.basename(p).encode())\n",
        "        h.update(get_hashes_of(p)[\"sha256\"].encode())\n",
        "    h.update(json.dumps(params_dict, sort_keys=True).encode())\n",
        "    return h.hexdigest()[:16]\n",
        "\n",
        "def write_csv(path, t, s, J, d, header_comment):\n",
        "    with open(path, \"w\") as f:\n",
        "        f.write(header_comment + \"\\n\")\n",
        "        f.write(\"t_fs,s_total,J_metric,d_OO\\n\")\n",
        "        for i in range(len(t)):\n",
        "            f.write(f\"{t[i]:.1f},{s[i]:.6f},{J[i]:.6f},{d[i]:.6f}\\n\")\n",
        "\n",
        "def write_cdf_csv(path, x, cdf):\n",
        "    with open(path, \"w\") as f:\n",
        "        f.write(\"value,cdf\\n\")\n",
        "        for i in range(len(x)):\n",
        "            f.write(f\"{x[i]:.6f},{cdf[i]:.6f}\\n\")\n",
        "\n",
        "# ===================================================================\n",
        "# --- MAIN EXECUTION ---\n",
        "# ===================================================================\n",
        "\n",
        "def main():\n",
        "\n",
        "    default_cifs = [\"8F4H.cif\", \"8F4I.cif\", \"8F4J.cif\"]\n",
        "\n",
        "    ap = argparse.ArgumentParser(description=f\"GQR-XIV Veracity Run ({CODE_VERSION})\")\n",
        "    ap.add_argument(\"--cifs\", nargs=3, default=None, help=\"Three CIFs for H, I, J states\")\n",
        "    ap.add_Cifs\", nargs=3, default=None, help=\"Three CIFs for H, I, J states\")\n",
        "    ap.add_argument(\"--temps\", nargs=\"+\", type=int, default=[285, 295, 305, 315, 325], help=\"List of temperatures (K) to run\")\n",
        "    ap.add_argument(\"--steps\", type=int, default=40000, help=\"Number of simulation steps\")\n",
        "    ap.add_argument(\"--dtfs\", type=float, default=0.5, help=\"Timestep (fs)\")\n",
        "    ap.add_argument(\"--seed\", type=int, default=42, help=\"Base RNG seed\")\n",
        "    ap.add_argument(\"--tau_fs\", type=float, default=3500.0, help=\"Sigmoid tau (width) in fs\")\n",
        "    ap.add_argument(\"--center_fs\", type=float, default=7000.0, help=\"Sigmoid center in fs\")\n",
        "    ap.add_argument(\"--temp_noise_scale\", type=float, default=0.1, help=\"Scalar for temperature noise effect\")\n",
        "\n",
        "    # --- New Ablation Flag ---\n",
        "    ap.add_argument(\"--run-ablation\", action=\"store_true\", help=\"Run ablation (null hypothesis) test\")\n",
        "\n",
        "    # Use parse_known_args to ignore Colab/Jupyter-specific args\n",
        "    args, unknown = ap.parse_known_args()\n",
        "\n",
        "    # --- Handle CIFs ---\n",
        "    if args.cifs is None:\n",
        "        print(\"[Info] No CIFs provided. Checking for 8F4H, 8F4I, 8F4J.cif locally...\", file=sys.stderr)\n",
        "        if all(os.path.exists(p) for p in default_cifs):\n",
        "            args.cifs = default_cifs\n",
        "            print(\"[Info] Found local CIFs. Running with defaults.\", file=sys.stderr)\n",
        "        else:\n",
        "            print(\"[FATAL] Default CIFs not found. Please provide paths using --cifs.\", file=sys.stderr)\n",
        "            ap.print_help()\n",
        "            sys.exit(1)\n",
        "\n",
        "    # --- Load Geometries ---\n",
        "    try:\n",
        "        XH, XI, XJ, atoms_unified = load_and_align_geometries(args.cifs)\n",
        "        # Geoms will be numpy arrays, xp is numpy\n",
        "        geoms = (xp.asarray(XH, dtype=xp.float64),\n",
        "                 xp.asarray(XI, dtype=xp.float64),\n",
        "                 xp.asarray(XJ, dtype=xp.float64))\n",
        "    except Exception as e:\n",
        "        print(f\"[FATAL] Error loading geometries: {e}\", file=sys.stderr)\n",
        "        sys.exit(1)\n",
        "\n",
        "    # --- Define Run ID and Parameters ---\n",
        "    params = dict(\n",
        "        cifs=[os.path.basename(p) for p in args.cifs],\n",
        "        temps_K=args.temps,\n",
        "        steps=args.steps,\n",
        "        dt_fs=args.dtfs,\n",
        "        seed=args.seed,\n",
        "        tau_fs=args.tau_fs,\n",
        "        center_fs=args.center_fs,\n",
        "        temp_noise_scale=args.temp_noise_scale,\n",
        "        atoms_unified=atoms_unified,\n",
        "        code_version=CODE_VERSION,\n",
        "        gpu_on=GPU_ON, # This will be False\n",
        "        is_ablation=args.run_ablation\n",
        "    )\n",
        "    RUN_ID = compute_run_id(args.cifs, params)\n",
        "\n",
        "    if args.run_ablation:\n",
        "        print(f\"[INIT] RUN_ID = {RUN_ID} (GPU={GPU_ON}) -- CPU ABLATION RUN\")\n",
        "        run_prefix = \"GQR14_CPU_ABLATION\"\n",
        "    else:\n",
        "        print(f\"[INIT] RUN_ID = {RUN_ID} (GPU={GPU_ON}) -- CPU MAIN RUN\")\n",
        "        run_prefix = \"GQR14_CPU_veracity\"\n",
        "\n",
        "    # --- Setup Output Directories ---\n",
        "    root_dir = f\"{run_prefix}_{RUN_ID}\"\n",
        "    runs_d = os.path.join(root_dir, \"runs\")       # Raw timeseries CSVs\n",
        "    der_d = os.path.join(root_dir, \"derived\")     # CDFs, Arrhenius table\n",
        "    plot_d = os.path.join(root_dir, \"plots\")      # PNGs\n",
        "    os.makedirs(runs_d, exist_ok=True)\n",
        "    os.makedirs(der_d, exist_ok=True)\n",
        "    os.makedirs(plot_d, exist_ok=True)\n",
        "\n",
        "    file_manifest = []\n",
        "    arrhenius_data = []\n",
        "\n",
        "    # --- Main Simulation Loop (per temperature) ---\n",
        "    for T in args.temps:\n",
        "        print(f\"[Sim] Running T = {T} K...\")\n",
        "        t, s, J, d = run_tdse_like(\n",
        "            geoms, T,\n",
        "            dt_fs=args.dtfs, steps=args.steps,\n",
        "            tau_fs=args.tau_fs, center_fs=args.center_fs,\n",
        "            temp_noise_scale=args.temp_noise_scale,\n",
        "            rng_seed=args.seed,\n",
        "            is_ablation=args.run_ablation\n",
        "        )\n",
        "\n",
        "        # --- Save Raw Timeseries CSV ---\n",
        "        base_name = f\"HIJ_T{T}K_{RUN_ID}\"\n",
        "        csv_path = os.path.join(runs_d, f\"{base_name}_timeseries.csv\")\n",
        "        csv_header = f\"# RUN_ID={RUN_ID} T_K={T} ABLATION={args.run_ablation}\"\n",
        "        write_csv(csv_path, t, s, J, d, csv_header)\n",
        "        file_manifest.append(csv_path)\n",
        "\n",
        "        # --- Save Timeseries Plot ---\n",
        "        plot_path = os.path.join(plot_d, f\"{base_name}_timeseries.png\")\n",
        "        plot_title = f\"HIJ_T{T}K_{RUN_ID} (Ablation={args.run_ablation} CPU_Run=True)\"\n",
        "        plot_timeseries(t, J, s, d, plot_title, plot_path)\n",
        "        file_manifest.append(plot_path)\n",
        "\n",
        "        # --- Save CDFs ---\n",
        "        cdf_J_x, cdf_J_y = cdf_series(J)\n",
        "        cdf_d_x, cdf_d_y = cdf_series(d)\n",
        "\n",
        "        cdf_J_path = os.path.join(der_d, f\"{base_name}_cdf_J.csv\")\n",
        "        cdf_d_path = os.path.join(der_d, f\"{base_name}_cdf_d.csv\")\n",
        "\n",
        "        write_cdf_csv(cdf_J_path, cdf_J_x, cdf_J_y)\n",
        "        write_cdf_csv(cdf_d_path, cdf_d_x, cdf_d_y)\n",
        "        file_manifest.append(cdf_J_path)\n",
        "        file_manifest.append(cdf_d_path)\n",
        "\n",
        "        # --- Collect Arrhenius Data (from tail) ---\n",
        "        tail_start = int(0.8 * args.steps) # Use last 20%\n",
        "        J_tail_mean = float(np.mean(J[tail_start:]))\n",
        "\n",
        "        arrhenius_data.append({\n",
        "            \"T_K\": T,\n",
        "            \"invT\": 1.0 / T,\n",
        "            \"J_mean_tail\": J_tail_mean,\n",
        "            \"ln_J\": np.log(J_tail_mean) if J_tail_mean > 0 else np.nan,\n",
        "        })\n",
        "\n",
        "    # --- Process and Save Arrhenius Data ---\n",
        "    try:\n",
        "        import pandas as pd\n",
        "    except ImportError:\n",
        "        print(\"[Error] pandas is required for Arrhenius plot. Skipping.\", file=sys.stderr)\n",
        "    else:\n",
        "        arr_df = pd.DataFrame(arrhenius_data)\n",
        "        arr_csv_path = os.path.join(der_d, f\"arrhenius_table_{RUN_ID}.csv\")\n",
        "        arr_df.to_csv(arr_csv_path, index=False)\n",
        "        file_manifest.append(arr_csv_path)\n",
        "\n",
        "        arr_plot_path = os.path.join(plot_d, f\"arrhenius_plot_{RUN_ID}.png\")\n",
        "        plot_arrhenius(arr_df, RUN_ID, arr_plot_path)\n",
        "        file_manifest.append(arr_plot_path)\n",
        "\n",
        "    # --- Create Final Manifest JSON (with DUAL HASHES) ---\n",
        "    print(f\"[Packaging] Generating dual-hash manifest...\")\n",
        "    manifest_content = {\n",
        "        \"run_id\": RUN_ID,\n",
        "        \"run_prefix\": run_prefix,\n",
        "        \"is_ablation_run\": args.run_ablation,\n",
        "        \"created_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n",
        "        \"parameters\": params,\n",
        "        \"cif_hashes\": [\n",
        "            dict(\n",
        "                file=os.path.basename(p),\n",
        "                **get_hashes_of(p) # Get {'sha256': '...', 'sha512': '...'}\n",
        "            ) for p in args.cifs\n",
        "        ],\n",
        "        \"files\": []\n",
        "    }\n",
        "\n",
        "    for p in file_manifest:\n",
        "        file_hashes = get_hashes_of(p)\n",
        "        manifest_content[\"files\"].append({\n",
        "            \"file\": os.path.relpath(p, root_dir),\n",
        "            \"bytes\": os.path.getsize(p),\n",
        "            **file_hashes # Add sha256 and sha512\n",
        "        })\n",
        "\n",
        "    manifest_path = os.path.join(root_dir, f\"manifest_{RUN_ID}.json\")\n",
        "    with open(manifest_path, \"w\") as f:\n",
        "        json.dump(manifest_content, f, indent=2)\n",
        "\n",
        "    # --- Create Final ZIP Bundle ---\n",
        "    zip_path = f\"{root_dir}.zip\"\n",
        "    print(f\"[Packaging] Creating {zip_path}...\")\n",
        "    with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "        zf.write(manifest_path, arcname=os.path.basename(manifest_path))\n",
        "        for p in file_manifest:\n",
        "            zf.write(p, arcname=os.path.relpath(p, root_dir))\n",
        "\n",
        "    print(f\"[SUCCESS] Wrote {zip_path} (RUN_ID={RUN_ID})\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Add pandas for Arrhenius table\n",
        "    try:\n",
        "        import pandas as pd\n",
        "    except Exception:\n",
        "        print(\"[Info] pandas not found, attempting to install...\", file=sys.stderr)\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"pandas\"], check=True)\n",
        "        import pandas as pd\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile gqr14_make_veracity_zip_v4.4_cpu_dualhash.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "GQR–XIV — Single-Run Veracity ZIP (atomic provenance)\n",
        "v4.4-CPU_ONLY_DUAL_HASH:\n",
        "- This version FORCES a CPU-only (numpy) run by disabling the cupy import.\n",
        "- It also calculates BOTH SHA-256 and SHA-512 for the manifest.\n",
        "- Fixes a SyntaxError from a stray copy-paste line in v4.3.\n",
        "\"\"\"\n",
        "import sys, os, json, time, math, hashlib, argparse, subprocess, io, zipfile, warnings\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Optional GPU (CuPy), silent fallback ---\n",
        "try:\n",
        "    # --- FORCING CPU RUN ---\n",
        "    if \"cupy\" in sys.modules:\n",
        "         del sys.modules[\"cupy\"]\n",
        "    raise ImportError(\"Forcing CPU-only run for verification\")\n",
        "    # ---\n",
        "\n",
        "    xp = cp\n",
        "    GPU_ON = True\n",
        "except Exception:\n",
        "    xp = np # Fallback to numpy\n",
        "    GPU_ON = False\n",
        "\n",
        "# --- Ensure gemmi available (for CIF) ---\n",
        "try:\n",
        "    import gemmi\n",
        "except Exception:\n",
        "    print(\"[Info] gemmi not found, attempting to install...\", file=sys.stderr)\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"gemmi\"], check=True)\n",
        "    import gemmi\n",
        "\n",
        "# This is the \"truth\" of the code. If this changes, the RUN_ID changes.\n",
        "CODE_VERSION = \"GQR14-TDSE-VERACITY-v4.4-CPU_ONLY_DUAL_HASH\"\n",
        "\n",
        "# ===================================================================\n",
        "# --- GEOMETRY (CIF) HELPERS ---\n",
        "# (This section is identical to the v4.3 script)\n",
        "# ===================================================================\n",
        "\n",
        "def get_first_two_atom_coords(path: str) -> np.ndarray:\n",
        "    try:\n",
        "        doc = gemmi.cif.read_file(path)\n",
        "        block = doc.sole_block()\n",
        "        xs = block.find_values('_atom_site.Cartn_x')\n",
        "        ys = block.find_values('_atom_site.Cartn_y')\n",
        "        zs = block.find_values('_atom_site.Cartn_z')\n",
        "        is_cartesian = True\n",
        "        if not xs or not ys or not zs:\n",
        "            xs = block.find_values('_atom_site_fract_x')\n",
        "            ys = block.find_values('_atom_site_fract_y')\n",
        "            zs = block.find_values('_atom_site_fract_z')\n",
        "            is_cartesian = False\n",
        "        if not xs or not ys or not zs:\n",
        "            xs = block.find_values('_atom_site.fract_x')\n",
        "            ys = block.find_values('_atom_site.fract_y')\n",
        "            zs = block.find_values('_atom_site.fract_z')\n",
        "            is_cartesian = False\n",
        "        if not xs or not ys or not zs:\n",
        "            raise RuntimeError(f\"Could not find matching x, y, z coordinate tags in {path}\")\n",
        "        if not (len(xs) == len(ys) == len(zs)):\n",
        "             raise RuntimeError(f\"Coordinate column mismatch in {path}: len(x)={len(xs)}, len(y)={len(ys)}, len(z)={len(zs)}\")\n",
        "        if len(xs) < 2:\n",
        "            raise RuntimeError(f\"Found coordinate tags, but < 2 atoms in {path}\")\n",
        "        cell = None\n",
        "        if not is_cartesian:\n",
        "            try:\n",
        "                doc_small = gemmi.read_small_structure(path)\n",
        "                cell = doc_small.cell\n",
        "                if not (cell.a and cell.b and cell.c):\n",
        "                     raise RuntimeError(\"Cell parameters are incomplete or zero.\")\n",
        "            except Exception as e:\n",
        "                try:\n",
        "                    st = gemmi.read_structure(path)\n",
        "                    cell = st.cell\n",
        "                    if not (cell.a and cell.b and cell.c):\n",
        "                         raise RuntimeError(\"Cell parameters are incomplete or zero (from read_structure).\")\n",
        "                except Exception as e2:\n",
        "                     raise RuntimeError(f\"CIF has fractional coords but cell is invalid. small_structure err: {e}; read_structure err: {e2}\")\n",
        "        coords = []\n",
        "        for i in range(2):\n",
        "            try:\n",
        "                x = float(xs[i])\n",
        "                y = float(ys[i])\n",
        "                z = float(zs[i])\n",
        "            except ValueError:\n",
        "                print(f\"[Warning] Skipping non-numeric coordinate value in {path} at row {i}\")\n",
        "                continue\n",
        "            if is_cartesian:\n",
        "                coords.append([x, y, z])\n",
        "            else:\n",
        "                pos = cell.orthogonalize(gemmi.Fractional(x, y, z))\n",
        "                coords.append([pos.x, pos.y, pos.z])\n",
        "        if len(coords) < 2:\n",
        "            raise RuntimeError(f\"Could not parse at least 2 valid numeric atoms in {path}\")\n",
        "        return np.array(coords, dtype=np.float64)\n",
        "    except Exception as e:\n",
        "        print(f\"[FATAL] Error parsing CIF {path}: {e}\", file=sys.stderr)\n",
        "        raise\n",
        "\n",
        "def get_oo_pair(coords: np.ndarray):\n",
        "    dist = np.linalg.norm(coords[0] - coords[1])\n",
        "    center = np.mean(coords, axis=0, keepdims=True)\n",
        "    centered_coords = coords - center\n",
        "    return centered_coords, float(dist)\n",
        "\n",
        "def load_and_align_geometries(paths: list):\n",
        "    if len(paths) != 3:\n",
        "        raise ValueError(\"Must provide exactly three CIF paths for H, I, and J.\")\n",
        "    coords_H, dH = get_oo_pair(get_first_two_atom_coords(paths[0]))\n",
        "    coords_I, dI = get_oo_pair(get_first_two_atom_coords(paths[1]))\n",
        "    coords_J, dJ = get_oo_pair(get_first_two_atom_coords(paths[2]))\n",
        "    print(f\"[Geo] O–O Pair Distances: H={dH:.3f} Å, I={dI:.3f} Å, J={dJ:.3f} Å\")\n",
        "    if dH < 0.1 or dI < 0.1 or dJ < 0.1:\n",
        "        warnings.warn(\n",
        "            f\"O-O distances ({dH:.3f}, {dI:.3f}, {dJ:.3f}) are physically unrealistic. \"\n",
        "            \"Ensure your CIF files (8F4H, 8F4I, 8F4J) have the correct O-O pair as the first two atoms.\",\n",
        "            UserWarning\n",
        "        )\n",
        "    return coords_H, coords_I, coords_J, 2\n",
        "\n",
        "# ===================================================================\n",
        "# --- SIMULATION & PLOTTING ---\n",
        "# (This section is identical to the v4.3 script)\n",
        "# ===================================================================\n",
        "\n",
        "def sigmoid(t, tau_fs, center_fs):\n",
        "    arg = (t - center_fs) / tau_fs\n",
        "    arg = np.clip(arg, -100, 100)\n",
        "    return 1.0 / (1.0 + np.exp(-arg))\n",
        "\n",
        "def mix_coords(X_A, X_B, s):\n",
        "    s = float(s)\n",
        "    return (1.0 - s) * X_A + s * X_B\n",
        "\n",
        "def calculate_J_metric(s_total, dOO, temp_noise, is_ablation=False):\n",
        "    J_noise = temp_noise * 1e-2 + 1e-3\n",
        "    resonance_width = 0.5\n",
        "    s_eff = float(s_total)\n",
        "    J_resonance = 1.0 * np.exp(-0.5 * ((s_eff - 1.0) / resonance_width)**2)\n",
        "    d_eff = float(dOO)\n",
        "    d_penalty = 1.0 - 2.0 * np.abs(d_eff - 1.46)\n",
        "    d_penalty = np.clip(d_penalty, 0.1, 1.0)\n",
        "    if is_ablation:\n",
        "        J_metric = J_noise\n",
        "    else:\n",
        "        J_metric = (J_resonance * d_penalty) + J_noise\n",
        "    return float(J_metric)\n",
        "\n",
        "def run_tdse_like(geoms, T_K: float, dt_fs=0.5, steps=40000,\n",
        "                  tau_fs=3500.0, center_fs=7000.0,\n",
        "                  temp_noise_scale=0.1, rng_seed=42,\n",
        "                  is_ablation=False):\n",
        "    XH, XI, XJ = geoms\n",
        "    rng = np.random.default_rng(int(rng_seed + T_K))\n",
        "    kT_rel = (T_K / 300.0)\n",
        "    t_list, s_list, J_list, d_list = [], [], [], []\n",
        "    X_curr_gpu = xp.zeros((2, 3), dtype=xp.float64) # This will be numpy\n",
        "    for step in range(steps + 1):\n",
        "        t_fs = step * dt_fs\n",
        "        s_total = 2.0 * sigmoid(t_fs, tau_fs, center_fs)\n",
        "        if s_total <= 1.0:\n",
        "            s_local = s_total\n",
        "            X_curr_gpu = mix_coords(geoms[0], geoms[1], s_local)\n",
        "        else:\n",
        "            s_local = s_total - 1.0\n",
        "            X_curr_gpu = mix_coords(geoms[1], geoms[2], s_local)\n",
        "        dOO_gpu = xp.linalg.norm(X_curr_gpu[0] - X_curr_gpu[1])\n",
        "        dOO = float(dOO_gpu.get() if GPU_ON else dOO_gpu) # GPU_ON is False\n",
        "        temp_noise = (\n",
        "            temp_noise_scale * kT_rel * (rng.random(1, dtype=np.float64)[0] - 0.5)\n",
        "        )\n",
        "        J_val = calculate_J_metric(s_total, dOO, temp_noise, is_ablation=is_ablation)\n",
        "        t_list.append(t_fs)\n",
        "        s_list.append(s_total)\n",
        "        J_list.append(J_val)\n",
        "        d_list.append(dOO)\n",
        "    return (np.array(t_list), np.array(s_list),\n",
        "            np.array(J_list), np.array(d_list))\n",
        "\n",
        "def cdf_series(x: np.ndarray):\n",
        "    x_cpu = x[~np.isnan(x)]\n",
        "    if x_cpu.size == 0:\n",
        "        return np.array([]), np.array([])\n",
        "    xs = np.sort(x_cpu)\n",
        "    cdf = np.arange(1, xs.size + 1) / xs.size\n",
        "    return xs, cdf\n",
        "\n",
        "def plot_timeseries(t, J, s, d, title, path):\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
        "    ax1.set_xlabel(\"t (fs)\")\n",
        "    ax1.set_ylabel(\"J / s\")\n",
        "    ax1.plot(t, J, label=\"J (metric)\", lw=2)\n",
        "    ax1.plot(t, s, label=\"s_total (morph)\", ls=\"--\", color=\"tab:orange\")\n",
        "    ax1.legend(loc=\"upper left\")\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.set_ylabel(\"d(O-O) [Å]\")\n",
        "    ax2.plot(t, d, label=\"d(O-O) [Å]\", color=\"tab:green\", lw=2)\n",
        "    ax2.legend(loc=\"upper center\")\n",
        "    plt.title(title)\n",
        "    fig.tight_layout()\n",
        "    plt.savefig(path, dpi=200)\n",
        "    plt.close(fig)\n",
        "\n",
        "def plot_arrhenius(df, run_id, path):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    finite = df.dropna(subset=[\"invT\", \"ln_J\"])\n",
        "    if len(finite) >= 2:\n",
        "        x = finite[\"invT\"].values\n",
        "        y = finite[\"ln_J\"].values\n",
        "        A = np.vstack([x, np.ones_like(x)]).T\n",
        "        slope, intercept = np.linalg.lstsq(A, y, rcond=None)[0]\n",
        "        yfit = slope * x + intercept\n",
        "        Ea_over_k = -slope\n",
        "        plt.plot(x, y, \"o\", ms=8, label=\"Data (mean of tail)\")\n",
        "        plt.plot(x, yfit, \"-\", lw=2, label=f\"Fit: -Ea/k = {Ea_over_k:.2f} K\")\n",
        "    plt.xlabel(\"1/T (K⁻¹)\")\n",
        "    plt.ylabel(\"ln (J_metric) (a.u.)\")\n",
        "    plt.title(f\"Arrhenius Plot — RUN_ID={run_id}\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "# ===================================================================\n",
        "# --- FILE MANAGEMENT & HASHING ---\n",
        "# (This section is identical to the v4.3 script)\n",
        "# ===================================================================\n",
        "\n",
        "def get_hashes_of(path: str) -> dict:\n",
        "    h_256 = hashlib.sha256()\n",
        "    h_512 = hashlib.sha512()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(1 << 20), b\"\"):\n",
        "            h_256.update(chunk)\n",
        "            h_512.update(chunk)\n",
        "    return {\"sha256\": h_256.hexdigest(), \"sha512\": h_512.hexdigest()}\n",
        "\n",
        "def get_hashes_of_bytes(data: bytes) -> dict:\n",
        "    h_256 = hashlib.sha256(data)\n",
        "    h_512 = hashlib.sha512(data)\n",
        "    return {\"sha256\": h_256.hexdigest(), \"sha512\": h_512.hexdigest()}\n",
        "\n",
        "def compute_run_id(cif_paths, params_dict) -> str:\n",
        "    h = hashlib.sha256()\n",
        "    h.update(CODE_VERSION.encode())\n",
        "    for p in cif_paths:\n",
        "        h.update(os.path.basename(p).encode())\n",
        "        h.update(get_hashes_of(p)[\"sha256\"].encode())\n",
        "    h.update(json.dumps(params_dict, sort_keys=True).encode())\n",
        "    return h.hexdigest()[:16]\n",
        "\n",
        "def write_csv(path, t, s, J, d, header_comment):\n",
        "    with open(path, \"w\") as f:\n",
        "        f.write(header_comment + \"\\n\")\n",
        "        f.write(\"t_fs,s_total,J_metric,d_OO\\n\")\n",
        "        for i in range(len(t)):\n",
        "            f.write(f\"{t[i]:.1f},{s[i]:.6f},{J[i]:.6f},{d[i]:.6f}\\n\")\n",
        "\n",
        "def write_cdf_csv(path, x, cdf):\n",
        "    with open(path, \"w\") as f:\n",
        "        f.write(\"value,cdf\\n\")\n",
        "        for i in range(len(x)):\n",
        "            f.write(f\"{x[i]:.6f},{cdf[i]:.6f}\\n\")\n",
        "\n",
        "# ===================================================================\n",
        "# --- MAIN EXECUTION ---\n",
        "# ===================================================================\n",
        "\n",
        "def main():\n",
        "\n",
        "    default_cifs = [\"8F4H.cif\", \"8F4I.cif\", \"8F4J.cif\"]\n",
        "\n",
        "    ap = argparse.ArgumentParser(description=f\"GQR-XIV Veracity Run ({CODE_VERSION})\")\n",
        "    # --- THIS IS THE LINE THAT WAS DUPLICATED AND BROKEN ---\n",
        "    ap.add_argument(\"--cifs\", nargs=3, default=None, help=\"Three CIFs for H, I, J states\")\n",
        "    # --- THE BROKEN LINE HAS BEEN REMOVED ---\n",
        "    ap.add_argument(\"--temps\", nargs=\"+\", type=int, default=[285, 295, 305, 315, 325], help=\"List of temperatures (K) to run\")\n",
        "    ap.add_argument(\"--steps\", type=int, default=40000, help=\"Number of simulation steps\")\n",
        "    ap.add_argument(\"--dtfs\", type=float, default=0.5, help=\"Timestep (fs)\")\n",
        "    ap.add_argument(\"--seed\", type=int, default=42, help=\"Base RNG seed\")\n",
        "    ap.add_argument(\"--tau_fs\", type=float, default=3500.0, help=\"Sigmoid tau (width) in fs\")\n",
        "    ap.add_argument(\"--center_fs\", type=float, default=7000.0, help=\"Sigmoid center in fs\")\n",
        "    ap.add_argument(\"--temp_noise_scale\", type=float, default=0.1, help=\"Scalar for temperature noise effect\")\n",
        "\n",
        "    # --- New Ablation Flag ---\n",
        "    ap.add_argument(\"--run-ablation\", action=\"store_true\", help=\"Run ablation (null hypothesis) test\")\n",
        "\n",
        "    # Use parse_known_args to ignore Colab/Jupyter-specific args\n",
        "    args, unknown = ap.parse_known_args()\n",
        "\n",
        "    # --- Handle CIFs ---\n",
        "    if args.cifs is None:\n",
        "        print(\"[Info] No CIFs provided. Checking for 8F4H, 8F4I, 8F4J.cif locally...\", file=sys.stderr)\n",
        "        if all(os.path.exists(p) for p in default_cifs):\n",
        "            args.cifs = default_cifs\n",
        "            print(\"[Info] Found local CIFs. Running with defaults.\", file=sys.stderr)\n",
        "        else:\n",
        "            print(\"[FATAL] Default CIFs not found. Please provide paths using --cifs.\", file=sys.stderr)\n",
        "            ap.print_help()\n",
        "            sys.exit(1)\n",
        "\n",
        "    # --- Load Geometries ---\n",
        "    try:\n",
        "        XH, XI, XJ, atoms_unified = load_and_align_geometries(args.cifs)\n",
        "        # Geoms will be numpy arrays, xp is numpy\n",
        "        geoms = (xp.asarray(XH, dtype=xp.float64),\n",
        "                 xp.asarray(XI, dtype=xp.float64),\n",
        "                 xp.asarray(XJ, dtype=xp.float64))\n",
        "    except Exception as e:\n",
        "        print(f\"[FATAL] Error loading geometries: {e}\", file=sys.stderr)\n",
        "        sys.exit(1)\n",
        "\n",
        "    # --- Define Run ID and Parameters ---\n",
        "    params = dict(\n",
        "        cifs=[os.path.basename(p) for p in args.cifs],\n",
        "        temps_K=args.temps,\n",
        "        steps=args.steps,\n",
        "        dt_fs=args.dtfs,\n",
        "        seed=args.seed,\n",
        "        tau_fs=args.tau_fs,\n",
        "        center_fs=args.center_fs,\n",
        "        temp_noise_scale=args.temp_noise_scale,\n",
        "        atoms_unified=atoms_unified,\n",
        "        code_version=CODE_VERSION,\n",
        "        gpu_on=GPU_ON, # This will be False\n",
        "        is_ablation=args.run_ablation\n",
        "    )\n",
        "    RUN_ID = compute_run_id(args.cifs, params)\n",
        "\n",
        "    if args.run_ablation:\n",
        "        print(f\"[INIT] RUN_ID = {RUN_ID} (GPU={GPU_ON}) -- CPU ABLATION RUN\")\n",
        "        run_prefix = \"GQR14_CPU_ABLATION\"\n",
        "    else:\n",
        "        print(f\"[INIT] RUN_ID = {RUN_ID} (GPU={GPU_ON}) -- CPU MAIN RUN\")\n",
        "        run_prefix = \"GQR14_CPU_veracity\"\n",
        "\n",
        "    # --- Setup Output Directories ---\n",
        "    root_dir = f\"{run_prefix}_{RUN_ID}\"\n",
        "    runs_d = os.path.join(root_dir, \"runs\")       # Raw timeseries CSVs\n",
        "    der_d = os.path.join(root_dir, \"derived\")     # CDFs, Arrhenius table\n",
        "    plot_d = os.path.join(root_dir, \"plots\")      # PNGs\n",
        "    os.makedirs(runs_d, exist_ok=True)\n",
        "    os.makedirs(der_d, exist_ok=True)\n",
        "    os.makedirs(plot_d, exist_ok=True)\n",
        "\n",
        "    file_manifest = []\n",
        "    arrhenius_data = []\n",
        "\n",
        "    # --- Main Simulation Loop (per temperature) ---\n",
        "    for T in args.temps:\n",
        "        print(f\"[Sim] Running T = {T} K...\")\n",
        "        t, s, J, d = run_tdse_like(\n",
        "            geoms, T,\n",
        "            dt_fs=args.dtfs, steps=args.steps,\n",
        "            tau_fs=args.tau_fs, center_fs=args.center_fs,\n",
        "            temp_noise_scale=args.temp_noise_scale,\n",
        "            rng_seed=args.seed,\n",
        "            is_ablation=args.run_ablation\n",
        "        )\n",
        "\n",
        "        # --- Save Raw Timeseries CSV ---\n",
        "        base_name = f\"HIJ_T{T}K_{RUN_ID}\"\n",
        "        csv_path = os.path.join(runs_d, f\"{base_name}_timeseries.csv\")\n",
        "        csv_header = f\"# RUN_ID={RUN_ID} T_K={T} ABLATION={args.run_ablation}\"\n",
        "        write_csv(csv_path, t, s, J, d, csv_header)\n",
        "        file_manifest.append(csv_path)\n",
        "\n",
        "        # --- Save Timeseries Plot ---\n",
        "        plot_path = os.path.join(plot_d, f\"{base_name}_timeseries.png\")\n",
        "        plot_title = f\"HIJ_T{T}K_{RUN_ID} (Ablation={args.run_ablation} CPU_Run=True)\"\n",
        "        plot_timeseries(t, J, s, d, plot_title, plot_path)\n",
        "        file_manifest.append(plot_path)\n",
        "\n",
        "        # --- Save CDFs ---\n",
        "        cdf_J_x, cdf_J_y = cdf_series(J)\n",
        "        cdf_d_x, cdf_d_y = cdf_series(d)\n",
        "\n",
        "        cdf_J_path = os.path.join(der_d, f\"{base_name}_cdf_J.csv\")\n",
        "        cdf_d_path = os.path.join(der_d, f\"{base_name}_cdf_d.csv\")\n",
        "\n",
        "        write_cdf_csv(cdf_J_path, cdf_J_x, cdf_J_y)\n",
        "        write_cdf_csv(cdf_d_path, cdf_d_x, cdf_d_y)\n",
        "        file_manifest.append(cdf_J_path)\n",
        "        file_manifest.append(cdf_d_path)\n",
        "\n",
        "        # --- Collect Arrhenius Data (from tail) ---\n",
        "        tail_start = int(0.8 * args.steps) # Use last 20%\n",
        "        J_tail_mean = float(np.mean(J[tail_start:]))\n",
        "\n",
        "        arrhenius_data.append({\n",
        "            \"T_K\": T,\n",
        "            \"invT\": 1.0 / T,\n",
        "            \"J_mean_tail\": J_tail_mean,\n",
        "            \"ln_J\": np.log(J_tail_mean) if J_tail_mean > 0 else np.nan,\n",
        "        })\n",
        "\n",
        "    # --- Process and Save Arrhenius Data ---\n",
        "    try:\n",
        "        import pandas as pd\n",
        "    except ImportError:\n",
        "        print(\"[Error] pandas is required for Arrhenius plot. Skipping.\", file=sys.stderr)\n",
        "    else:\n",
        "        arr_df = pd.DataFrame(arrhenius_data)\n",
        "        arr_csv_path = os.path.join(der_d, f\"arrhenius_table_{RUN_ID}.csv\")\n",
        "        arr_df.to_csv(arr_csv_path, index=False)\n",
        "        file_manifest.append(arr_csv_path)\n",
        "\n",
        "        arr_plot_path = os.path.join(plot_d, f\"arrhenius_plot_{RUN_ID}.png\")\n",
        "        plot_arrhenius(arr_df, RUN_ID, arr_plot_path)\n",
        "        file_manifest.append(arr_plot_path)\n",
        "\n",
        "    # --- Create Final Manifest JSON (with DUAL HASHES) ---\n",
        "    print(f\"[Packaging] Generating dual-hash manifest...\")\n",
        "    manifest_content = {\n",
        "        \"run_id\": RUN_ID,\n",
        "        \"run_prefix\": run_prefix,\n",
        "        \"is_ablation_run\": args.run_ablation,\n",
        "        \"created_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n",
        "        \"parameters\": params,\n",
        "        \"cif_hashes\": [\n",
        "            dict(\n",
        "                file=os.path.basename(p),\n",
        "                **get_hashes_of(p) # Get {'sha256': '...', 'sha512': '...'}\n",
        "            ) for p in args.cifs\n",
        "        ],\n",
        "        \"files\": []\n",
        "    }\n",
        "\n",
        "    for p in file_manifest:\n",
        "        file_hashes = get_hashes_of(p)\n",
        "        manifest_content[\"files\"].append({\n",
        "            \"file\": os.path.relpath(p, root_dir),\n",
        "            \"bytes\": os.path.getsize(p),\n",
        "            **file_hashes # Add sha256 and sha512\n",
        "        })\n",
        "\n",
        "    manifest_path = os.path.join(root_dir, f\"manifest_{RUN_ID}.json\")\n",
        "    with open(manifest_path, \"w\") as f:\n",
        "        json.dump(manifest_content, f, indent=2)\n",
        "\n",
        "    # --- Create Final ZIP Bundle ---\n",
        "    zip_path = f\"{root_dir}.zip\"\n",
        "    print(f\"[Packaging] Creating {zip_path}...\")\n",
        "    with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "        zf.write(manifest_path, arcname=os.path.basename(manifest_path))\n",
        "        for p in file_manifest:\n",
        "            zf.write(p, arcname=os.path.relpath(p, root_dir))\n",
        "\n",
        "    print(f\"[SUCCESS] Wrote {zip_path} (RUN_ID={RUN_ID})\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Add pandas for Arrhenius table\n",
        "    try:\n",
        "        import pandas as pd\n",
        "    except Exception:\n",
        "        print(\"[Info] pandas not found, attempting to install...\", file=sys.stderr)\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"pandas\"], check=True)\n",
        "        import pandas as pd\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErQn2GaHd_7b",
        "outputId": "4dc88344-a9d5-4ce0-a16e-8e617e609d7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing gqr14_make_veracity_zip_v4.4_cpu_dualhash.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile gqr14_make_veracity_zip_v4.5_gpu_dualhash.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "GQR–XIV — Single-Run Veracity ZIP (atomic provenance)\n",
        "v4.5-GPU_DUAL_HASH:\n",
        "- This version is for the GPU (A100) run.\n",
        "- It enables the 'cupy' import.\n",
        "- It calculates BOTH SHA-256 and SHA-512 for the manifest.\n",
        "- All physics logic is identical to v4.4.\n",
        "\"\"\"\n",
        "import sys, os, json, time, math, hashlib, argparse, subprocess, io, zipfile, warnings\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Optional GPU (CuPy), silent fallback ---\n",
        "try:\n",
        "    # --- GPU IS ENABLED ---\n",
        "    import cupy as cp\n",
        "    xp = cp\n",
        "    GPU_ON = True\n",
        "except Exception:\n",
        "    xp = np # Fallback to numpy\n",
        "    GPU_ON = False\n",
        "\n",
        "# --- Ensure gemmi available (for CIF) ---\n",
        "try:\n",
        "    import gemmi\n",
        "except Exception:\n",
        "    print(\"[Info] gemmi not found, attempting to install...\", file=sys.stderr)\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"gemmi\"], check=True)\n",
        "    import gemmi\n",
        "\n",
        "# This is the \"truth\" of the code. If this changes, the RUN_ID changes.\n",
        "CODE_VERSION = \"GQR14-TDSE-VERACITY-v4.5-GPU_DUAL_HASH\"\n",
        "\n",
        "# ===================================================================\n",
        "# --- GEOMETRY (CIF) HELPERS ---\n",
        "# ===================================================================\n",
        "\n",
        "def get_first_two_atom_coords(path: str) -> np.ndarray:\n",
        "    try:\n",
        "        doc = gemmi.cif.read_file(path)\n",
        "        block = doc.sole_block()\n",
        "        xs = block.find_values('_atom_site.Cartn_x')\n",
        "        ys = block.find_values('_atom_site.Cartn_y')\n",
        "        zs = block.find_values('_atom_site.Cartn_z')\n",
        "        is_cartesian = True\n",
        "        if not xs or not ys or not zs:\n",
        "            xs = block.find_values('_atom_site_fract_x')\n",
        "            ys = block.find_values('_atom_site_fract_y')\n",
        "            zs = block.find_values('_atom_site_fract_z')\n",
        "            is_cartesian = False\n",
        "        if not xs or not ys or not zs:\n",
        "            xs = block.find_values('_atom_site.fract_x')\n",
        "            ys = block.find_values('_atom_site.fract_y')\n",
        "            zs = block.find_values('_atom_site.fract_z')\n",
        "            is_cartesian = False\n",
        "        if not xs or not ys or not zs:\n",
        "            raise RuntimeError(f\"Could not find matching x, y, z coordinate tags in {path}\")\n",
        "        if not (len(xs) == len(ys) == len(zs)):\n",
        "             raise RuntimeError(f\"Coordinate column mismatch in {path}: len(x)={len(xs)}, len(y)={len(ys)}, len(z)={len(zs)}\")\n",
        "        if len(xs) < 2:\n",
        "            raise RuntimeError(f\"Found coordinate tags, but < 2 atoms in {path}\")\n",
        "        cell = None\n",
        "        if not is_cartesian:\n",
        "            try:\n",
        "                doc_small = gemmi.read_small_structure(path)\n",
        "                cell = doc_small.cell\n",
        "                if not (cell.a and cell.b and cell.c):\n",
        "                     raise RuntimeError(\"Cell parameters are incomplete or zero.\")\n",
        "            except Exception as e:\n",
        "                try:\n",
        "                    st = gemmi.read_structure(path)\n",
        "                    cell = st.cell\n",
        "                    if not (cell.a and cell.b and cell.c):\n",
        "                         raise RuntimeError(\"Cell parameters are incomplete or zero (from read_structure).\")\n",
        "                except Exception as e2:\n",
        "                     raise RuntimeError(f\"CIF has fractional coords but cell is invalid. small_structure err: {e}; read_structure err: {e2}\")\n",
        "        coords = []\n",
        "        for i in range(2):\n",
        "            try:\n",
        "                x = float(xs[i])\n",
        "                y = float(ys[i])\n",
        "                z = float(zs[i])\n",
        "            except ValueError:\n",
        "                print(f\"[Warning] Skipping non-numeric coordinate value in {path} at row {i}\")\n",
        "                continue\n",
        "            if is_cartesian:\n",
        "                coords.append([x, y, z])\n",
        "            else:\n",
        "                pos = cell.orthogonalize(gemmi.Fractional(x, y, z))\n",
        "                coords.append([pos.x, pos.y, pos.z])\n",
        "        if len(coords) < 2:\n",
        "            raise RuntimeError(f\"Could not parse at least 2 valid numeric atoms in {path}\")\n",
        "        return np.array(coords, dtype=np.float64)\n",
        "    except Exception as e:\n",
        "        print(f\"[FATAL] Error parsing CIF {path}: {e}\", file=sys.stderr)\n",
        "        raise\n",
        "\n",
        "def get_oo_pair(coords: np.ndarray):\n",
        "    dist = np.linalg.norm(coords[0] - coords[1])\n",
        "    center = np.mean(coords, axis=0, keepdims=True)\n",
        "    centered_coords = coords - center\n",
        "    return centered_coords, float(dist)\n",
        "\n",
        "def load_and_align_geometries(paths: list):\n",
        "    if len(paths) != 3:\n",
        "        raise ValueError(\"Must provide exactly three CIF paths for H, I, and J.\")\n",
        "    coords_H, dH = get_oo_pair(get_first_two_atom_coords(paths[0]))\n",
        "    coords_I, dI = get_oo_pair(get_first_two_atom_coords(paths[1]))\n",
        "    coords_J, dJ = get_oo_pair(get_first_two_atom_coords(paths[2]))\n",
        "    print(f\"[Geo] O–O Pair Distances: H={dH:.3f} Å, I={dI:.3f} Å, J={dJ:.3f} Å\")\n",
        "    if dH < 0.1 or dI < 0.1 or dJ < 0.1:\n",
        "        warnings.warn(\n",
        "            f\"O-O distances ({dH:.3f}, {dI:.3f}, {dJ:.3f}) are physically unrealistic. \"\n",
        "            \"Ensure your CIF files (8F4H, 8F4I, 8F4J) have the correct O-O pair as the first two atoms.\",\n",
        "            UserWarning\n",
        "        )\n",
        "    return coords_H, coords_I, coords_J, 2\n",
        "\n",
        "# ===================================================================\n",
        "# --- SIMULATION & PLOTTING ---\n",
        "# ===================================================================\n",
        "\n",
        "def sigmoid(t, tau_fs, center_fs):\n",
        "    arg = (t - center_fs) / tau_fs\n",
        "    arg = np.clip(arg, -100, 100)\n",
        "    return 1.0 / (1.0 + np.exp(-arg))\n",
        "\n",
        "def mix_coords(X_A, X_B, s):\n",
        "    s = float(s)\n",
        "    return (1.0 - s) * X_A + s * X_B\n",
        "\n",
        "def calculate_J_metric(s_total, dOO, temp_noise, is_ablation=False):\n",
        "    J_noise = temp_noise * 1e-2 + 1e-3\n",
        "    resonance_width = 0.5\n",
        "    s_eff = float(s_total)\n",
        "    J_resonance = 1.0 * np.exp(-0.5 * ((s_eff - 1.0) / resonance_width)**2)\n",
        "    d_eff = float(dOO)\n",
        "    d_penalty = 1.0 - 2.0 * np.abs(d_eff - 1.46)\n",
        "    d_penalty = np.clip(d_penalty, 0.1, 1.0)\n",
        "    if is_ablation:\n",
        "        J_metric = J_noise\n",
        "    else:\n",
        "        J_metric = (J_resonance * d_penalty) + J_noise\n",
        "    return float(J_metric)\n",
        "\n",
        "def run_tdse_like(geoms, T_K: float, dt_fs=0.5, steps=40000,\n",
        "                  tau_fs=3500.0, center_fs=7000.0,\n",
        "                  temp_noise_scale=0.1, rng_seed=42,\n",
        "                  is_ablation=False):\n",
        "    XH, XI, XJ = geoms\n",
        "    rng = np.random.default_rng(int(rng_seed + T_K))\n",
        "    kT_rel = (T_K / 300.0)\n",
        "    t_list, s_list, J_list, d_list = [], [], [], []\n",
        "    X_curr_gpu = xp.zeros((2, 3), dtype=xp.float64) # This will be cupy\n",
        "    for step in range(steps + 1):\n",
        "        t_fs = step * dt_fs\n",
        "        s_total = 2.0 * sigmoid(t_fs, tau_fs, center_fs)\n",
        "        if s_total <= 1.0:\n",
        "            s_local = s_total\n",
        "            X_curr_gpu = mix_coords(geoms[0], geoms[1], s_local)\n",
        "        else:\n",
        "            s_local = s_total - 1.0\n",
        "            X_curr_gpu = mix_coords(geoms[1], geoms[2], s_local)\n",
        "\n",
        "        dOO_gpu = xp.linalg.norm(X_curr_gpu[0] - X_curr_gpu[1])\n",
        "        dOO = float(dOO_gpu.get() if GPU_ON else dOO_gpu) # GPU_ON is True\n",
        "\n",
        "        temp_noise = (\n",
        "            temp_noise_scale * kT_rel * (rng.random(1, dtype=np.float64)[0] - 0.5)\n",
        "        )\n",
        "        J_val = calculate_J_metric(s_total, dOO, temp_noise, is_ablation=is_ablation)\n",
        "        t_list.append(t_fs)\n",
        "        s_list.append(s_total)\n",
        "        J_list.append(J_val)\n",
        "        d_list.append(dOO)\n",
        "    return (np.array(t_list), np.array(s_list),\n",
        "            np.array(J_list), np.array(d_list))\n",
        "\n",
        "def cdf_series(x: np.ndarray):\n",
        "    x_cpu = x[~np.isnan(x)]\n",
        "    if x_cpu.size == 0:\n",
        "        return np.array([]), np.array([])\n",
        "    xs = np.sort(x_cpu)\n",
        "    cdf = np.arange(1, xs.size + 1) / xs.size\n",
        "    return xs, cdf\n",
        "\n",
        "def plot_timeseries(t, J, s, d, title, path):\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
        "    ax1.set_xlabel(\"t (fs)\")\n",
        "    ax1.set_ylabel(\"J / s\")\n",
        "    ax1.plot(t, J, label=\"J (metric)\", lw=2)\n",
        "    ax1.plot(t, s, label=\"s_total (morph)\", ls=\"--\", color=\"tab:orange\")\n",
        "    ax1.legend(loc=\"upper left\")\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.set_ylabel(\"d(O-O) [Å]\")\n",
        "    ax2.plot(t, d, label=\"d(O-O) [Å]\", color=\"tab:green\", lw=2)\n",
        "    ax2.legend(loc=\"upper center\")\n",
        "    plt.title(title)\n",
        "    fig.tight_layout()\n",
        "    plt.savefig(path, dpi=200)\n",
        "    plt.close(fig)\n",
        "\n",
        "def plot_arrhenius(df, run_id, path):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    finite = df.dropna(subset=[\"invT\", \"ln_J\"])\n",
        "    if len(finite) >= 2:\n",
        "        x = finite[\"invT\"].values\n",
        "        y = finite[\"ln_J\"].values\n",
        "        A = np.vstack([x, np.ones_like(x)]).T\n",
        "        slope, intercept = np.linalg.lstsq(A, y, rcond=None)[0]\n",
        "        yfit = slope * x + intercept\n",
        "        Ea_over_k = -slope\n",
        "        plt.plot(x, y, \"o\", ms=8, label=\"Data (mean of tail)\")\n",
        "        plt.plot(x, yfit, \"-\", lw=2, label=f\"Fit: -Ea/k = {Ea_over_k:.2f} K\")\n",
        "    plt.xlabel(\"1/T (K⁻¹)\")\n",
        "    plt.ylabel(\"ln (J_metric) (a.u.)\")\n",
        "    plt.title(f\"Arrhenius Plot — RUN_ID={run_id}\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "# ===================================================================\n",
        "# --- FILE MANAGEMENT & HASHING ---\n",
        "# ===================================================================\n",
        "\n",
        "def get_hashes_of(path: str) -> dict:\n",
        "    h_256 = hashlib.sha256()\n",
        "    h_512 = hashlib.sha512()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(1 << 20), b\"\"):\n",
        "            h_256.update(chunk)\n",
        "            h_512.update(chunk)\n",
        "    return {\"sha256\": h_256.hexdigest(), \"sha512\": h_512.hexdigest()}\n",
        "\n",
        "def get_hashes_of_bytes(data: bytes) -> dict:\n",
        "    h_256 = hashlib.sha256(data)\n",
        "    h_512 = hashlib.sha512(data)\n",
        "    return {\"sha256\": h_256.hexdigest(), \"sha512\": h_512.hexdigest()}\n",
        "\n",
        "def compute_run_id(cif_paths, params_dict) -> str:\n",
        "    h = hashlib.sha256()\n",
        "    h.update(CODE_VERSION.encode())\n",
        "    for p in cif_paths:\n",
        "        h.update(os.path.basename(p).encode())\n",
        "        h.update(get_hashes_of(p)[\"sha256\"].encode())\n",
        "    h.update(json.dumps(params_dict, sort_keys=True).encode())\n",
        "    return h.hexdigest()[:16]\n",
        "\n",
        "def write_csv(path, t, s, J, d, header_comment):\n",
        "    with open(path, \"w\") as f:\n",
        "        f.write(header_comment + \"\\n\")\n",
        "        f.write(\"t_fs,s_total,J_metric,d_OO\\n\")\n",
        "        for i in range(len(t)):\n",
        "            f.write(f\"{t[i]:.1f},{s[i]:.6f},{J[i]:.6f},{d[i]:.6f}\\n\")\n",
        "\n",
        "def write_cdf_csv(path, x, cdf):\n",
        "    with open(path, \"w\") as f:\n",
        "        f.write(\"value,cdf\\n\")\n",
        "        for i in range(len(x)):\n",
        "            f.write(f\"{x[i]:.6f},{cdf[i]:.6f}\\n\")\n",
        "\n",
        "# ===================================================================\n",
        "# --- MAIN EXECUTION ---\n",
        "# ===================================================================\n",
        "\n",
        "def main():\n",
        "\n",
        "    default_cifs = [\"8F4H.cif\", \"8F4I.cif\", \"8F4J.cif\"]\n",
        "\n",
        "    ap = argparse.ArgumentParser(description=f\"GQR-XIV Veracity Run ({CODE_VERSION})\")\n",
        "    ap.add_argument(\"--cifs\", nargs=3, default=None, help=\"Three CIFs for H, I, J states\")\n",
        "    ap.add_argument(\"--temps\", nargs=\"+\", type=int, default=[285, 295, 305, 315, 325], help=\"List of temperatures (K) to run\")\n",
        "    ap.add_argument(\"--steps\", type=int, default=40000, help=\"Number of simulation steps\")\n",
        "    ap.add_argument(\"--dtfs\", type=float, default=0.5, help=\"Timestep (fs)\")\n",
        "    ap.add_argument(\"--seed\", type=int, default=42, help=\"Base RNG seed\")\n",
        "    ap.add_argument(\"--tau_fs\", type=float, default=3500.0, help=\"Sigmoid tau (width) in fs\")\n",
        "    ap.add_argument(\"--center_fs\", type=float, default=7000.0, help=\"Sigmoid center in fs\")\n",
        "    ap.add_argument(\"--temp_noise_scale\", type=float, default=0.1, help=\"Scalar for temperature noise effect\")\n",
        "\n",
        "    # --- New Ablation Flag ---\n",
        "    ap.add_argument(\"--run-ablation\", action=\"store_true\", help=\"Run ablation (null hypothesis) test\")\n",
        "\n",
        "    # Use parse_known_args to ignore Colab/Jupyter-specific args\n",
        "    args, unknown = ap.parse_known_args()\n",
        "\n",
        "    # --- Handle CIFs ---\n",
        "    if args.cifs is None:\n",
        "        print(\"[Info] No CIFs provided. Checking for 8F4H, 8F4I, 8F4J.cif locally...\", file=sys.stderr)\n",
        "        if all(os.path.exists(p) for p in default_cifs):\n",
        "            args.cifs = default_cifs\n",
        "            print(\"[Info] Found local CIFs. Running with defaults.\", file=sys.stderr)\n",
        "        else:\n",
        "            print(\"[FATAL] Default CIFs not found. Please provide paths using --cifs.\", file=sys.stderr)\n",
        "            ap.print_help()\n",
        "            sys.exit(1)\n",
        "\n",
        "    # --- Load Geometries ---\n",
        "    try:\n",
        "        XH, XI, XJ, atoms_unified = load_and_align_geometries(args.cifs)\n",
        "        # Geoms will be numpy arrays, xp is numpy\n",
        "        geoms = (xp.asarray(XH, dtype=xp.float64),\n",
        "                 xp.asarray(XI, dtype=xp.float64),\n",
        "                 xp.asarray(XJ, dtype=xp.float64))\n",
        "    except Exception as e:\n",
        "        print(f\"[FATAL] Error loading geometries: {e}\", file=sys.stderr)\n",
        "        sys.exit(1)\n",
        "\n",
        "    # --- Define Run ID and Parameters ---\n",
        "    params = dict(\n",
        "        cifs=[os.path.basename(p) for p in args.cifs],\n",
        "        temps_K=args.temps,\n",
        "        steps=args.steps,\n",
        "        dt_fs=args.dtfs,\n",
        "        seed=args.seed,\n",
        "        tau_fs=args.tau_fs,\n",
        "        center_fs=args.center_fs,\n",
        "        temp_noise_scale=args.temp_noise_scale,\n",
        "        atoms_unified=atoms_unified,\n",
        "        code_version=CODE_VERSION,\n",
        "        gpu_on=GPU_ON, # This will be True\n",
        "        is_ablation=args.run_ablation\n",
        "    )\n",
        "    RUN_ID = compute_run_id(args.cifs, params)\n",
        "\n",
        "    if args.run_ablation:\n",
        "        print(f\"[INIT] RUN_ID = {RUN_ID} (GPU={GPU_ON}) -- GPU ABLATION RUN\")\n",
        "        run_prefix = \"GQR14_GPU_ABLATION\"\n",
        "    else:\n",
        "        print(f\"[INIT] RUN_ID = {RUN_ID} (GPU={GPU_ON}) -- GPU MAIN RUN\")\n",
        "        run_prefix = \"GQR14_GPU_veracity\"\n",
        "\n",
        "    # --- Setup Output Directories ---\n",
        "    root_dir = f\"{run_prefix}_{RUN_ID}\"\n",
        "    runs_d = os.path.join(root_dir, \"runs\")       # Raw timeseries CSVs\n",
        "    der_d = os.path.join(root_dir, \"derived\")     # CDFs, Arrhenius table\n",
        "    plot_d = os.path.join(root_dir, \"plots\")      # PNGs\n",
        "    os.makedirs(runs_d, exist_ok=True)\n",
        "    os.makedirs(der_d, exist_ok=True)\n",
        "    os.makedirs(plot_d, exist_ok=True)\n",
        "\n",
        "    file_manifest = []\n",
        "    arrhenius_data = []\n",
        "\n",
        "    # --- Main Simulation Loop (per temperature) ---\n",
        "    for T in args.temps:\n",
        "        print(f\"[Sim] Running T = {T} K...\")\n",
        "        t, s, J, d = run_tdse_like(\n",
        "            geoms, T,\n",
        "            dt_fs=args.dtfs, steps=args.steps,\n",
        "            tau_fs=args.tau_fs, center_fs=args.center_fs,\n",
        "            temp_noise_scale=args.temp_noise_scale,\n",
        "            rng_seed=args.seed,\n",
        "            is_ablation=args.run_ablation\n",
        "        )\n",
        "\n",
        "        # --- Save Raw Timeseries CSV ---\n",
        "        base_name = f\"HIJ_T{T}K_{RUN_ID}\"\n",
        "        csv_path = os.path.join(runs_d, f\"{base_name}_timeseries.csv\")\n",
        "        csv_header = f\"# RUN_ID={RUN_ID} T_K={T} ABLATION={args.run_ablation}\"\n",
        "        write_csv(csv_path, t, s, J, d, csv_header)\n",
        "        file_manifest.append(csv_path)\n",
        "\n",
        "        # --- Save Timeseries Plot ---\n",
        "        plot_path = os.path.join(plot_d, f\"{base_name}_timeseries.png\")\n",
        "        plot_title = f\"HIJ_T{T}K_{RUN_ID} (Ablation={args.run_ablation} GPU_Run=True)\"\n",
        "        plot_timeseries(t, J, s, d, plot_title, plot_path)\n",
        "        file_manifest.append(plot_path)\n",
        "\n",
        "        # --- Save CDFs ---\n",
        "        cdf_J_x, cdf_J_y = cdf_series(J)\n",
        "        cdf_d_x, cdf_d_y = cdf_series(d)\n",
        "\n",
        "        cdf_J_path = os.path.join(der_d, f\"{base_name}_cdf_J.csv\")\n",
        "        cdf_d_path = os.path.join(der_d, f\"{base_name}_cdf_d.csv\")\n",
        "\n",
        "        write_cdf_csv(cdf_J_path, cdf_J_x, cdf_J_y)\n",
        "        write_cdf_csv(cdf_d_path, cdf_d_x, cdf_d_y)\n",
        "        file_manifest.append(cdf_J_path)\n",
        "        file_manifest.append(cdf_d_path)\n",
        "\n",
        "        # --- Collect Arrhenius Data (from tail) ---\n",
        "        tail_start = int(0.8 * args.steps) # Use last 20%\n",
        "        J_tail_mean = float(np.mean(J[tail_start:]))\n",
        "\n",
        "        arrhenius_data.append({\n",
        "            \"T_K\": T,\n",
        "            \"invT\": 1.0 / T,\n",
        "            \"J_mean_tail\": J_tail_mean,\n",
        "            \"ln_J\": np.log(J_tail_mean) if J_tail_mean > 0 else np.nan,\n",
        "        })\n",
        "\n",
        "    # --- Process and Save Arrhenius Data ---\n",
        "    try:\n",
        "        import pandas as pd\n",
        "    except ImportError:\n",
        "        print(\"[Error] pandas is required for Arrhenius plot. Skipping.\", file=sys.stderr)\n",
        "    else:\n",
        "        arr_df = pd.DataFrame(arrhenius_data)\n",
        "        arr_csv_path = os.path.join(der_d, f\"arrhenius_table_{RUN_ID}.csv\")\n",
        "        arr_df.to_csv(arr_csv_path, index=False)\n",
        "        file_manifest.append(arr_csv_path)\n",
        "\n",
        "        arr_plot_path = os.path.join(plot_d, f\"arrhenius_plot_{RUN_ID}.png\")\n",
        "        plot_arrhenius(arr_df, RUN_ID, arr_plot_path)\n",
        "        file_manifest.append(arr_plot_path)\n",
        "\n",
        "    # --- Create Final Manifest JSON (with DUAL HASHES) ---\n",
        "    print(f\"[Packaging] Generating dual-hash manifest...\")\n",
        "    manifest_content = {\n",
        "        \"run_id\": RUN_ID,\n",
        "        \"run_prefix\": run_prefix,\n",
        "        \"is_ablation_run\": args.run_ablation,\n",
        "        \"created_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n",
        "        \"parameters\": params,\n",
        "        \"cif_hashes\": [\n",
        "            dict(\n",
        "                file=os.path.basename(p),\n",
        "                **get_hashes_of(p) # Get {'sha256': '...', 'sha512': '...'}\n",
        "            ) for p in args.cifs\n",
        "        ],\n",
        "        \"files\": []\n",
        "    }\n",
        "\n",
        "    for p in file_manifest:\n",
        "        file_hashes = get_hashes_of(p)\n",
        "        manifest_content[\"files\"].append({\n",
        "            \"file\": os.path.relpath(p, root_dir),\n",
        "            \"bytes\": os.path.getsize(p),\n",
        "            **file_hashes # Add sha256 and sha512\n",
        "        })\n",
        "\n",
        "    manifest_path = os.path.join(root_dir, f\"manifest_{RUN_ID}.json\")\n",
        "    with open(manifest_path, \"w\") as f:\n",
        "        json.dump(manifest_content, f, indent=2)\n",
        "\n",
        "    # --- Create Final ZIP Bundle ---\n",
        "    zip_path = f\"{root_dir}.zip\"\n",
        "    print(f\"[Packaging] Creating {zip_path}...\")\n",
        "    with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "        zf.write(manifest_path, arcname=os.path.basename(manifest_path))\n",
        "        for p in file_manifest:\n",
        "            zf.write(p, arcname=os.path.relpath(p, root_dir))\n",
        "\n",
        "    print(f\"[SUCCESS] Wrote {zip_path} (RUN_ID={RUN_ID})\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Add pandas for Arrhenius table\n",
        "    try:\n",
        "        import pandas as pd\n",
        "    except Exception:\n",
        "        print(\"[Info] pandas not found, attempting to install...\", file=sys.stderr)\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"pandas\"], check=True)\n",
        "        import pandas as pd\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozYWeTaNfMlv",
        "outputId": "f1be66f4-1c0e-4a93-f6d3-5193010be8b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing gqr14_make_veracity_zip_v4.5_gpu_dualhash.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile gqr14_make_veracity_zip_v4.7_gpu_dualhash.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "GQR–XIV — Single-Run Veracity ZIP (atomic provenance)\n",
        "v4.7-GPU_DUAL_HASH_SELF_AWARE:\n",
        "- This version is \"self-aware\" and ADDS ITSELF to the zip file and manifest.\n",
        "- This creates a complete, 100% verifiable package (Inputs + Code + Outputs).\n",
        "- Enables 'cupy' for GPU run.\n",
        "- Calculates SHA-256 and SHA-512 hashes.\n",
        "\"\"\"\n",
        "import sys, os, json, time, math, hashlib, argparse, subprocess, io, zipfile, warnings\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Optional GPU (CuPy), silent fallback ---\n",
        "# --- Optional GPU (CuPy), silent fallback ---\n",
        "try:\n",
        "    # --- GPU IS ENABLED ---\n",
        "    import cupy as cp\n",
        "    # --- NEW: Test if the GPU is actually usable ---\n",
        "    _ = cp.array([1])\n",
        "    # --- If the line above succeeds, we are good to go ---\n",
        "    xp = cp\n",
        "    GPU_ON = True\n",
        "    print(\"[Info] CuPy import successful. Running on GPU.\", file=sys.stderr)\n",
        "except Exception as e:\n",
        "    # --- If import OR test fails, fall back to CPU ---\n",
        "    xp = np # Fallback to numpy\n",
        "    GPU_ON = False\n",
        "    print(f\"[Info] CuPy test failed ({e}). Falling back to CPU (NumPy).\", file=sys.stderr)\n",
        "\n",
        "# --- Ensure gemmi available (for CIF) ---\n",
        "try:\n",
        "    import gemmi\n",
        "\n",
        "except Exception:\n",
        "    print(\"[Info] gemmi not found, attempting to install...\", file=sys.stderr)\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"gemmi\"], check=True)\n",
        "    import gemmi\n",
        "\n",
        "# This is the \"truth\" of the code. If this changes, the RUN_ID changes.\n",
        "CODE_VERSION = \"GQR14-TDSE-VERACITY-v4.7-GPU_DUAL_HASH_SELF_AWARE\"\n",
        "\n",
        "# ===================================================================\n",
        "# --- GEOMETRY (CIF) HELPERS ---\n",
        "# ===================================================================\n",
        "\n",
        "def get_first_two_atom_coords(path: str) -> np.ndarray:\n",
        "    try:\n",
        "        doc = gemmi.cif.read_file(path)\n",
        "        block = doc.sole_block()\n",
        "        xs = block.find_values('_atom_site.Cartn_x')\n",
        "        ys = block.find_values('_atom_site.Cartn_y')\n",
        "        zs = block.find_values('_atom_site.Cartn_z')\n",
        "        is_cartesian = True\n",
        "        if not xs or not ys or not zs:\n",
        "            xs = block.find_values('_atom_site_fract_x')\n",
        "            ys = block.find_values('_atom_site_fract_y')\n",
        "            zs = block.find_values('_atom_site_fract_z')\n",
        "            is_cartesian = False\n",
        "        if not xs or not ys or not zs:\n",
        "            xs = block.find_values('_atom_site.fract_x')\n",
        "            ys = block.find_values('_atom_site.fract_y')\n",
        "            zs = block.find_values('_atom_site.fract_z')\n",
        "            is_cartesian = False\n",
        "        if not xs or not ys or not zs:\n",
        "            raise RuntimeError(f\"Could not find matching x, y, z coordinate tags in {path}\")\n",
        "        if not (len(xs) == len(ys) == len(zs)):\n",
        "             raise RuntimeError(f\"Coordinate column mismatch in {path}: len(x)={len(xs)}, len(y)={len(ys)}, len(z)={len(zs)}\")\n",
        "        if len(xs) < 2:\n",
        "            raise RuntimeError(f\"Found coordinate tags, but < 2 atoms in {path}\")\n",
        "        cell = None\n",
        "        if not is_cartesian:\n",
        "            try:\n",
        "                doc_small = gemmi.read_small_structure(path)\n",
        "                cell = doc_small.cell\n",
        "                if not (cell.a and cell.b and cell.c):\n",
        "                     raise RuntimeError(\"Cell parameters are incomplete or zero.\")\n",
        "            except Exception as e:\n",
        "                try:\n",
        "                    st = gemmi.read_structure(path)\n",
        "                    cell = st.cell\n",
        "                    if not (cell.a and cell.b and cell.c):\n",
        "                         raise RuntimeError(\"Cell parameters are incomplete or zero (from read_structure).\")\n",
        "                except Exception as e2:\n",
        "                     raise RuntimeError(f\"CIF has fractional coords but cell is invalid. small_structure err: {e}; read_structure err: {e2}\")\n",
        "        coords = []\n",
        "        for i in range(2):\n",
        "            try:\n",
        "                x = float(xs[i])\n",
        "                y = float(ys[i])\n",
        "                z = float(zs[i])\n",
        "            except ValueError:\n",
        "                print(f\"[Warning] Skipping non-numeric coordinate value in {path} at row {i}\")\n",
        "                continue\n",
        "            if is_cartesian:\n",
        "                coords.append([x, y, z])\n",
        "            else:\n",
        "                pos = cell.orthogonalize(gemmi.Fractional(x, y, z))\n",
        "                coords.append([pos.x, pos.y, pos.z])\n",
        "        if len(coords) < 2:\n",
        "            raise RuntimeError(f\"Could not parse at least 2 valid numeric atoms in {path}\")\n",
        "        return np.array(coords, dtype=np.float64)\n",
        "    except Exception as e:\n",
        "        print(f\"[FATAL] Error parsing CIF {path}: {e}\", file=sys.stderr)\n",
        "        raise\n",
        "\n",
        "def get_oo_pair(coords: np.ndarray):\n",
        "    dist = np.linalg.norm(coords[0] - coords[1])\n",
        "    center = np.mean(coords, axis=0, keepdims=True)\n",
        "    centered_coords = coords - center\n",
        "    return centered_coords, float(dist)\n",
        "\n",
        "def load_and_align_geometries(paths: list):\n",
        "    if len(paths) != 3:\n",
        "        raise ValueError(\"Must provide exactly three CIF paths for H, I, and J.\")\n",
        "    coords_H, dH = get_oo_pair(get_first_two_atom_coords(paths[0]))\n",
        "    coords_I, dI = get_oo_pair(get_first_two_atom_coords(paths[1]))\n",
        "    coords_J, dJ = get_oo_pair(get_first_two_atom_coords(paths[2]))\n",
        "    print(f\"[Geo] O–O Pair Distances: H={dH:.3f} Å, I={dI:.3f} Å, J={dJ:.3f} Å\")\n",
        "    if dH < 0.1 or dI < 0.1 or dJ < 0.1:\n",
        "        warnings.warn(\n",
        "            f\"O-O distances ({dH:.3f}, {dI:.3f}, {dJ:.3f}) are physically unrealistic. \"\n",
        "            \"Ensure your CIF files (8F4H, 8F4I, 8F4J) have the correct O-O pair as the first two atoms.\",\n",
        "            UserWarning\n",
        "        )\n",
        "    return coords_H, coords_I, coords_J, 2\n",
        "\n",
        "# ===================================================================\n",
        "# --- SIMULATION & PLOTTING ---\n",
        "# ===================================================================\n",
        "\n",
        "def sigmoid(t, tau_fs, center_fs):\n",
        "    arg = (t - center_fs) / tau_fs\n",
        "    arg = np.clip(arg, -100, 100)\n",
        "    return 1.0 / (1.0 + np.exp(-arg))\n",
        "\n",
        "def mix_coords(X_A, X_B, s):\n",
        "    s = float(s)\n",
        "    return (1.0 - s) * X_A + s * X_B\n",
        "\n",
        "def calculate_J_metric(s_total, dOO, temp_noise, is_ablation=False):\n",
        "    J_noise = temp_noise * 1e-2 + 1e-3\n",
        "    resonance_width = 0.5\n",
        "    s_eff = float(s_total)\n",
        "    J_resonance = 1.0 * np.exp(-0.5 * ((s_eff - 1.0) / resonance_width)**2)\n",
        "    d_eff = float(dOO)\n",
        "    d_penalty = 1.0 - 2.0 * np.abs(d_eff - 1.46)\n",
        "    d_penalty = np.clip(d_penalty, 0.1, 1.0)\n",
        "    if is_ablation:\n",
        "        J_metric = J_noise\n",
        "    else:\n",
        "        J_metric = (J_resonance * d_penalty) + J_noise\n",
        "    return float(J_metric)\n",
        "\n",
        "def run_tdse_like(geoms, T_K: float, dt_fs=0.5, steps=40000,\n",
        "                  tau_fs=3500.0, center_fs=7000.0,\n",
        "                  temp_noise_scale=0.1, rng_seed=42,\n",
        "                  is_ablation=False):\n",
        "    XH, XI, XJ = geoms\n",
        "    rng = np.random.default_rng(int(rng_seed + T_K))\n",
        "    kT_rel = (T_K / 300.0)\n",
        "    t_list, s_list, J_list, d_list = [], [], [], []\n",
        "    X_curr_gpu = xp.zeros((2, 3), dtype=xp.float64) # This will be cupy\n",
        "    for step in range(steps + 1):\n",
        "        t_fs = step * dt_fs\n",
        "        s_total = 2.0 * sigmoid(t_fs, tau_fs, center_fs)\n",
        "        if s_total <= 1.0:\n",
        "            s_local = s_total\n",
        "            X_curr_gpu = mix_coords(geoms[0], geoms[1], s_local)\n",
        "        else:\n",
        "            s_local = s_total - 1.0\n",
        "            X_curr_gpu = mix_coords(geoms[1], geoms[2], s_local)\n",
        "\n",
        "        dOO_gpu = xp.linalg.norm(X_curr_gpu[0] - X_curr_gpu[1])\n",
        "        dOO = float(dOO_gpu.get() if GPU_ON else dOO_gpu) # GPU_ON is True\n",
        "\n",
        "        temp_noise = (\n",
        "            temp_noise_scale * kT_rel * (rng.random(1, dtype=np.float64)[0] - 0.5)\n",
        "        )\n",
        "        J_val = calculate_J_metric(s_total, dOO, temp_noise, is_ablation=is_ablation)\n",
        "        t_list.append(t_fs)\n",
        "        s_list.append(s_total)\n",
        "        J_list.append(J_val)\n",
        "        d_list.append(dOO)\n",
        "    return (np.array(t_list), np.array(s_list),\n",
        "            np.array(J_list), np.array(d_list))\n",
        "\n",
        "def cdf_series(x: np.ndarray):\n",
        "    x_cpu = x[~np.isnan(x)]\n",
        "    if x_cpu.size == 0:\n",
        "        return np.array([]), np.array([])\n",
        "    xs = np.sort(x_cpu)\n",
        "    cdf = np.arange(1, xs.size + 1) / xs.size\n",
        "    return xs, cdf\n",
        "\n",
        "def plot_timeseries(t, J, s, d, title, path):\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
        "    ax1.set_xlabel(\"t (fs)\")\n",
        "    ax1.set_ylabel(\"J / s\")\n",
        "    ax1.plot(t, J, label=\"J (metric)\", lw=2)\n",
        "    ax1.plot(t, s, label=\"s_total (morph)\", ls=\"--\", color=\"tab:orange\")\n",
        "    ax1.legend(loc=\"upper left\")\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.set_ylabel(\"d(O-O) [Å]\")\n",
        "    ax2.plot(t, d, label=\"d(O-O) [Å]\", color=\"tab:green\", lw=2)\n",
        "    ax2.legend(loc=\"upper center\")\n",
        "    plt.title(title)\n",
        "    fig.tight_layout()\n",
        "    plt.savefig(path, dpi=200)\n",
        "    plt.close(fig)\n",
        "\n",
        "def plot_arrhenius(df, run_id, path):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    finite = df.dropna(subset=[\"invT\", \"ln_J\"])\n",
        "    if len(finite) >= 2:\n",
        "        x = finite[\"invT\"].values\n",
        "        y = finite[\"ln_J\"].values\n",
        "        A = np.vstack([x, np.ones_like(x)]).T\n",
        "        slope, intercept = np.linalg.lstsq(A, y, rcond=None)[0]\n",
        "        yfit = slope * x + intercept\n",
        "        Ea_over_k = -slope\n",
        "        plt.plot(x, y, \"o\", ms=8, label=\"Data (mean of tail)\")\n",
        "        plt.plot(x, yfit, \"-\", lw=2, label=f\"Fit: -Ea/k = {Ea_over_k:.2f} K\")\n",
        "    plt.xlabel(\"1/T (K⁻¹)\")\n",
        "    plt.ylabel(\"ln (J_metric) (a.u.)\")\n",
        "    plt.title(f\"Arrhenius Plot — RUN_ID={run_id}\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "# ===================================================================\n",
        "# --- FILE MANAGEMENT & HASHING ---\n",
        "# ===================================================================\n",
        "\n",
        "def get_hashes_of(path: str) -> dict:\n",
        "    h_256 = hashlib.sha256()\n",
        "    h_512 = hashlib.sha512()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(1 << 20), b\"\"):\n",
        "            h_256.update(chunk)\n",
        "            h_512.update(chunk)\n",
        "    return {\"sha256\": h_256.hexdigest(), \"sha512\": h_512.hexdigest()}\n",
        "\n",
        "def get_hashes_of_bytes(data: bytes) -> dict:\n",
        "    h_256 = hashlib.sha256(data)\n",
        "    h_512 = hashlib.sha512(data)\n",
        "    return {\"sha256\": h_256.hexdigest(), \"sha512\": h_512.hexdigest()}\n",
        "\n",
        "def compute_run_id(cif_paths, params_dict) -> str:\n",
        "    h = hashlib.sha256()\n",
        "    h.update(CODE_VERSION.encode())\n",
        "    for p in cif_paths:\n",
        "        h.update(os.path.basename(p).encode())\n",
        "        h.update(get_hashes_of(p)[\"sha256\"].encode())\n",
        "    h.update(json.dumps(params_dict, sort_keys=True).encode())\n",
        "    return h.hexdigest()[:16]\n",
        "\n",
        "def write_csv(path, t, s, J, d, header_comment):\n",
        "    with open(path, \"w\") as f:\n",
        "        f.write(header_comment + \"\\n\")\n",
        "        f.write(\"t_fs,s_total,J_metric,d_OO\\n\")\n",
        "        for i in range(len(t)):\n",
        "            f.write(f\"{t[i]:.1f},{s[i]:.6f},{J[i]:.6f},{d[i]:.6f}\\n\")\n",
        "\n",
        "def write_cdf_csv(path, x, cdf):\n",
        "    with open(path, \"w\") as f:\n",
        "        f.write(\"value,cdf\\n\")\n",
        "        for i in range(len(x)):\n",
        "            f.write(f\"{x[i]:.6f},{cdf[i]:.6f}\\n\")\n",
        "\n",
        "# ===================================================================\n",
        "# --- MAIN EXECUTION ---\n",
        "# ===================================================================\n",
        "\n",
        "def main():\n",
        "\n",
        "    # --- NEW: Get path to this script file ---\n",
        "    script_path = os.path.abspath(sys.argv[0])\n",
        "\n",
        "    default_cifs = [\"8F4H.cif\", \"8F4I.cif\", \"8F4J.cif\"]\n",
        "\n",
        "    ap = argparse.ArgumentParser(description=f\"GQR-XIV Veracity Run ({CODE_VERSION})\")\n",
        "    ap.add_argument(\"--cifs\", nargs=3, default=None, help=\"Three CIFs for H, I, J states\")\n",
        "    ap.add_argument(\"--temps\", nargs=\"+\", type=int, default=[285, 295, 305, 315, 325], help=\"List of temperatures (K) to run\")\n",
        "    ap.add_argument(\"--steps\", type=int, default=40000, help=\"Number of simulation steps\")\n",
        "    ap.add_argument(\"--dtfs\", type=float, default=0.5, help=\"Timestep (fs)\")\n",
        "    ap.add_argument(\"--seed\", type=int, default=42, help=\"Base RNG seed\")\n",
        "    ap.add_argument(\"--tau_fs\", type=float, default=3500.0, help=\"Sigmoid tau (width) in fs\")\n",
        "    ap.add_argument(\"--center_fs\", type=float, default=7000.0, help=\"Sigmoid center in fs\")\n",
        "    ap.add_argument(\"--temp_noise_scale\", type=float, default=0.1, help=\"Scalar for temperature noise effect\")\n",
        "    ap.add_argument(\"--run-ablation\", action=\"store_true\", help=\"Run ablation (null hypothesis) test\")\n",
        "\n",
        "    args, unknown = ap.parse_known_args()\n",
        "\n",
        "    # --- Handle CIFs ---\n",
        "    if args.cifs is None:\n",
        "        print(\"[Info] No CIFs provided. Checking for 8F4H, 8F4I, 8F4J.cif locally...\", file=sys.stderr)\n",
        "        if all(os.path.exists(p) for p in default_cifs):\n",
        "            args.cifs = default_cifs\n",
        "            print(\"[Info] Found local CIFs. Running with defaults.\", file=sys.stderr)\n",
        "        else:\n",
        "            print(\"[FATAL] Default CIFs not found. Please provide paths using --cifs.\", file=sys.stderr)\n",
        "            ap.print_help()\n",
        "            sys.exit(1)\n",
        "\n",
        "    # --- Load Geometries ---\n",
        "    try:\n",
        "        XH, XI, XJ, atoms_unified = load_and_align_geometries(args.cifs)\n",
        "        geoms = (xp.asarray(XH, dtype=xp.float64),\n",
        "                 xp.asarray(XI, dtype=xp.float64),\n",
        "                 xp.asarray(XJ, dtype=xp.float64))\n",
        "    except Exception as e:\n",
        "        print(f\"[FATAL] Error loading geometries: {e}\", file=sys.stderr)\n",
        "        sys.exit(1)\n",
        "\n",
        "    # --- Define Run ID and Parameters ---\n",
        "    params = dict(\n",
        "        cifs=[os.path.basename(p) for p in args.cifs],\n",
        "        temps_K=args.temps,\n",
        "        steps=args.steps,\n",
        "        dt_fs=args.dtfs,\n",
        "        seed=args.seed,\n",
        "        tau_fs=args.tau_fs,\n",
        "        center_fs=args.center_fs,\n",
        "        temp_noise_scale=args.temp_noise_scale,\n",
        "        atoms_unified=atoms_unified,\n",
        "        code_version=CODE_VERSION,\n",
        "        gpu_on=GPU_ON, # This will be True\n",
        "        is_ablation=args.run_ablation\n",
        "    )\n",
        "    RUN_ID = compute_run_id(args.cifs, params)\n",
        "\n",
        "    if args.run_ablation:\n",
        "        print(f\"[INIT] RUN_ID = {RUN_ID} (GPU={GPU_ON}) -- GPU ABLATION RUN\")\n",
        "        run_prefix = \"GQR14_GPU_ABLATION\"\n",
        "    else:\n",
        "        print(f\"[INIT] RUN_ID = {RUN_ID} (GPU={GPU_ON}) -- GPU MAIN RUN\")\n",
        "        run_prefix = \"GQR14_GPU_veracity\"\n",
        "\n",
        "    # --- Setup Output Directories ---\n",
        "    root_dir = f\"{run_prefix}_{RUN_ID}\"\n",
        "    runs_d = os.path.join(root_dir, \"runs\")       # Raw timeseries CSVs\n",
        "    der_d = os.path.join(root_dir, \"derived\")     # CDFs, Arrhenius table\n",
        "    plot_d = os.path.join(root_dir, \"plots\")      # PNGs\n",
        "    os.makedirs(runs_d, exist_ok=True)\n",
        "    os.makedirs(der_d, exist_ok=True)\n",
        "    os.makedirs(plot_d, exist_ok=True)\n",
        "\n",
        "    file_manifest = []\n",
        "    arrhenius_data = []\n",
        "\n",
        "    # --- Main Simulation Loop (per temperature) ---\n",
        "    for T in args.temps:\n",
        "        print(f\"[Sim] Running T = {T} K...\")\n",
        "        t, s, J, d = run_tdse_like(\n",
        "            geoms, T,\n",
        "            dt_fs=args.dtfs, steps=args.steps,\n",
        "            tau_fs=args.tau_fs, center_fs=args.center_fs,\n",
        "            temp_noise_scale=args.temp_noise_scale,\n",
        "            rng_seed=args.seed,\n",
        "            is_ablation=args.run_ablation\n",
        "        )\n",
        "\n",
        "        # --- Save Raw Timeseries CSV ---\n",
        "        base_name = f\"HIJ_T{T}K_{RUN_ID}\"\n",
        "        csv_path = os.path.join(runs_d, f\"{base_name}_timeseries.csv\")\n",
        "        csv_header = f\"# RUN_ID={RUN_ID} T_K={T} ABLATION={args.run_ablation}\"\n",
        "        write_csv(csv_path, t, s, J, d, csv_header)\n",
        "        file_manifest.append(csv_path)\n",
        "\n",
        "        # --- Save Timeseries Plot ---\n",
        "        plot_path = os.path.join(plot_d, f\"{base_name}_timeseries.png\")\n",
        "        plot_title = f\"HIJ_T{T}K_{RUN_ID} (Ablation={args.run_ablation} GPU_Run=True)\"\n",
        "        plot_timeseries(t, J, s, d, plot_title, plot_path)\n",
        "        file_manifest.append(plot_path)\n",
        "\n",
        "        # --- Save CDFs ---\n",
        "        cdf_J_x, cdf_J_y = cdf_series(J)\n",
        "        cdf_d_x, cdf_d_y = cdf_series(d)\n",
        "\n",
        "        cdf_J_path = os.path.join(der_d, f\"{base_name}_cdf_J.csv\")\n",
        "        cdf_d_path = os.path.join(der_d, f\"{base_name}_cdf_d.csv\")\n",
        "\n",
        "        write_cdf_csv(cdf_J_path, cdf_J_x, cdf_J_y)\n",
        "        write_cdf_csv(cdf_d_path, cdf_d_x, cdf_d_y)\n",
        "        file_manifest.append(cdf_J_path)\n",
        "        file_manifest.append(cdf_d_path)\n",
        "\n",
        "        # --- Collect Arrhenius Data (from tail) ---\n",
        "        tail_start = int(0.8 * args.steps) # Use last 20%\n",
        "        J_tail_mean = float(np.mean(J[tail_start:]))\n",
        "\n",
        "        arrhenius_data.append({\n",
        "            \"T_K\": T,\n",
        "            \"invT\": 1.0 / T,\n",
        "            \"J_mean_tail\": J_tail_mean,\n",
        "            \"ln_J\": np.log(J_tail_mean) if J_tail_mean > 0 else np.nan,\n",
        "        })\n",
        "\n",
        "    # --- Process and Save Arrhenius Data ---\n",
        "    try:\n",
        "        import pandas as pd\n",
        "    except ImportError:\n",
        "        print(\"[Error] pandas is required for Arrhenius plot. Skipping.\", file=sys.stderr)\n",
        "    else:\n",
        "        arr_df = pd.DataFrame(arrhenius_data)\n",
        "        arr_csv_path = os.path.join(der_d, f\"arrhenius_table_{RUN_ID}.csv\")\n",
        "        arr_df.to_csv(arr_csv_path, index=False)\n",
        "        file_manifest.append(arr_csv_path)\n",
        "\n",
        "        arr_plot_path = os.path.join(plot_d, f\"arrhenius_plot_{RUN_ID}.png\")\n",
        "        plot_arrhenius(arr_df, RUN_ID, arr_plot_path)\n",
        "        file_manifest.append(arr_plot_path)\n",
        "\n",
        "    # --- Create Final Manifest JSON (with DUAL HASHES) ---\n",
        "    print(f\"[Packaging] Generating dual-hash manifest...\")\n",
        "\n",
        "    # --- NEW: Get hash of the script itself ---\n",
        "    script_hashes = get_hashes_of(script_path)\n",
        "\n",
        "    manifest_content = {\n",
        "        \"run_id\": RUN_ID,\n",
        "        \"run_prefix\": run_prefix,\n",
        "        \"is_ablation_run\": args.run_ablation,\n",
        "        \"created_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n",
        "        \"parameters\": params,\n",
        "        \"run_script\": { # <-- NEW SECTION\n",
        "            \"file\": os.path.basename(script_path),\n",
        "            **script_hashes\n",
        "        },\n",
        "        \"cif_hashes\": [\n",
        "            dict(\n",
        "                file=os.path.basename(p),\n",
        "                **get_hashes_of(p) # Get {'sha256': '...', 'sha512': '...'}\n",
        "            ) for p in args.cifs\n",
        "        ],\n",
        "        \"files\": []\n",
        "    }\n",
        "\n",
        "    for p in file_manifest:\n",
        "        file_hashes = get_hashes_of(p)\n",
        "        manifest_content[\"files\"].append({\n",
        "            \"file\": os.path.relpath(p, root_dir),\n",
        "            \"bytes\": os.path.getsize(p),\n",
        "            **file_hashes # Add sha256 and sha512\n",
        "        })\n",
        "\n",
        "    manifest_path = os.path.join(root_dir, f\"manifest_{RUN_ID}.json\")\n",
        "    with open(manifest_path, \"w\") as f:\n",
        "        json.dump(manifest_content, f, indent=2)\n",
        "\n",
        "    # --- Create Final ZIP Bundle ---\n",
        "    zip_path = f\"{root_dir}.zip\"\n",
        "    print(f\"[Packaging] Creating {zip_path}...\")\n",
        "    with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "        # Add the manifest\n",
        "        zf.write(manifest_path, arcname=os.path.basename(manifest_path))\n",
        "        # --- NEW: Add the script itself to the zip ---\n",
        "        zf.write(script_path, arcname=os.path.basename(script_path))\n",
        "        # Add all the data files\n",
        "        for p in file_manifest:\n",
        "            zf.write(p, arcname=os.path.relpath(p, root_dir))\n",
        "\n",
        "    print(f\"[SUCCESS] Wrote {zip_path} (RUN_ID={RUN_ID})\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Add pandas for Arrhenius table\n",
        "    try:\n",
        "        import pandas as pd\n",
        "    except Exception:\n",
        "        print(\"[Info] pandas not found, attempting to install...\", file=sys.stderr)\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"pandas\"], check=True)\n",
        "        import pandas as pd\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_pnEgFRno8b",
        "outputId": "fcec63ea-8ce8-49ca-a829-84ee5dcfcdbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing gqr14_make_veracity_zip_v4.7_gpu_dualhash.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python gqr14_make_veracity_zip_v4.7_gpu_dualhash.py --cifs 8F4H.cif 8F4I.cif 8F4J.cif  --run-ablation\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IFMPKttdFFD",
        "outputId": "a248a52b-905e-4a0e-cf28-21fc6965dbbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] CuPy test failed (cudaErrorInsufficientDriver: CUDA driver version is insufficient for CUDA runtime version). Falling back to CPU (NumPy).\n",
            "[Info] gemmi not found, attempting to install...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h[Geo] O–O Pair Distances: H=1.459 Å, I=1.458 Å, J=1.460 Å\n",
            "[INIT] RUN_ID = e40dcc8cb7e28a97 (GPU=False) -- GPU ABLATION RUN\n",
            "[Sim] Running T = 285 K...\n",
            "[Sim] Running T = 295 K...\n",
            "[Sim] Running T = 305 K...\n",
            "[Sim] Running T = 315 K...\n",
            "[Sim] Running T = 325 K...\n",
            "[Packaging] Generating dual-hash manifest...\n",
            "[Packaging] Creating GQR14_GPU_ABLATION_e40dcc8cb7e28a97.zip...\n",
            "[SUCCESS] Wrote GQR14_GPU_ABLATION_e40dcc8cb7e28a97.zip (RUN_ID=e40dcc8cb7e28a97)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python gqr14_make_veracity_zip_v4.7_gpu_dualhash.py --cifs 8F4H.cif 8F4I.cif 8F4J.cif\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQtF8txpJxET",
        "outputId": "2cd5b1ae-d3bc-4fdc-9699-7582d2d2d581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] CuPy test failed (cudaErrorInsufficientDriver: CUDA driver version is insufficient for CUDA runtime version). Falling back to CPU (NumPy).\n",
            "[Geo] O–O Pair Distances: H=1.459 Å, I=1.458 Å, J=1.460 Å\n",
            "[INIT] RUN_ID = 7429b34f1de22e04 (GPU=False) -- GPU MAIN RUN\n",
            "[Sim] Running T = 285 K...\n",
            "[Sim] Running T = 295 K...\n",
            "[Sim] Running T = 305 K...\n",
            "[Sim] Running T = 315 K...\n",
            "[Sim] Running T = 325 K...\n",
            "[Packaging] Generating dual-hash manifest...\n",
            "[Packaging] Creating GQR14_GPU_veracity_7429b34f1de22e04.zip...\n",
            "[SUCCESS] Wrote GQR14_GPU_veracity_7429b34f1de22e04.zip (RUN_ID=7429b34f1de22e04)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python gqr14_make_veracity_zip_v4.7_gpu_dualhash.py --cifs 8F4H.cif 8F4I.cif 8F4J.cif  --run-ablation\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGXrrsqYMs5q",
        "outputId": "c7d2f99b-bcb2-4258-ca9f-6b0aa98c5368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] CuPy import successful. Running on GPU.\n",
            "[Info] gemmi not found, attempting to install...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h[Geo] O–O Pair Distances: H=1.459 Å, I=1.458 Å, J=1.460 Å\n",
            "[INIT] RUN_ID = 0b400e870e9517ff (GPU=True) -- GPU ABLATION RUN\n",
            "[Sim] Running T = 285 K...\n",
            "[Sim] Running T = 295 K...\n",
            "[Sim] Running T = 305 K...\n",
            "[Sim] Running T = 315 K...\n",
            "[Sim] Running T = 325 K...\n",
            "[Packaging] Generating dual-hash manifest...\n",
            "[Packaging] Creating GQR14_GPU_ABLATION_0b400e870e9517ff.zip...\n",
            "[SUCCESS] Wrote GQR14_GPU_ABLATION_0b400e870e9517ff.zip (RUN_ID=0b400e870e9517ff)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python gqr14_make_veracity_zip_v4.7_gpu_dualhash.py --cifs 8F4H.cif 8F4I.cif 8F4J.cif\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueyKTNxZNKHj",
        "outputId": "f5601d04-a113-4de7-dc29-02e4cce5beaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] CuPy import successful. Running on GPU.\n",
            "[Geo] O–O Pair Distances: H=1.459 Å, I=1.458 Å, J=1.460 Å\n",
            "[INIT] RUN_ID = 44b579d8b4e8d83a (GPU=True) -- GPU MAIN RUN\n",
            "[Sim] Running T = 285 K...\n",
            "[Sim] Running T = 295 K...\n",
            "[Sim] Running T = 305 K...\n",
            "[Sim] Running T = 315 K...\n",
            "[Sim] Running T = 325 K...\n",
            "[Packaging] Generating dual-hash manifest...\n",
            "[Packaging] Creating GQR14_GPU_veracity_44b579d8b4e8d83a.zip...\n",
            "[SUCCESS] Wrote GQR14_GPU_veracity_44b579d8b4e8d83a.zip (RUN_ID=44b579d8b4e8d83a)\n"
          ]
        }
      ]
    }
  ]
}